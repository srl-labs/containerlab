{"config":{"lang":["en"],"separator":"[\\s\\-\\_]","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>With the growing number of containerized Network Operating Systems grows the demand to easily run them in the user-defined, versatile lab topologies.</p> <p>Unfortunately, container orchestration tools like docker-compose are not a good fit for that purpose, as they do not allow a user to easily create connections between the containers which define a topology.</p> <p>Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them to create lab topologies of users choice and manages labs lifecycle.</p> <p>Containerlab focuses on the containerized Network Operating Systems which are typically used to test network features and designs, such as:</p> <ul> <li>Nokia SR Linux</li> <li>Arista cEOS</li> <li>Cisco XRd</li> <li>SONiC</li> <li>Juniper cRPD</li> <li>Cumulus VX</li> <li>Keysight IXIA-C</li> <li>RARE/freeRtr</li> <li>Ostinato</li> </ul> <p>In addition to native containerized NOSes, containerlab can launch traditional virtual machine based routers using vrnetlab or boxen integration:</p> <ul> <li>Nokia virtual SR OS (vSim/VSR)</li> <li>Juniper vMX</li> <li>Juniper vQFX</li> <li>Juniper vSRX</li> <li>Juniper vJunos-router</li> <li>Juniper vJunos-switch</li> <li>Juniper vJunos Evolved</li> <li>Cisco IOS XRv9k</li> <li>Cisco Catalyst 9000v</li> <li>Cisco Nexus 9000v</li> <li>Cisco c8000v</li> <li>Cisco CSR 1000v</li> <li>Cisco FTDv</li> <li>Dell FTOS10v</li> <li>Arista vEOS</li> <li>Palo Alto PAN</li> <li>IPInfusion OcNOS</li> <li>Check Point Cloudguard</li> <li>Fortinet Fortigate</li> <li>Aruba AOS-CX</li> <li>Huawei VRP</li> <li>OpenBSD</li> <li>FreeBSD</li> <li>SONiC</li> </ul> <p>And, of course, containerlab is perfectly capable of wiring up arbitrary linux containers which can host your network applications, virtual functions or simply be a test client. With all that, containerlab provides a single IaaC interface to manage labs which can span all the needed variants of nodes:</p> <p>This short clip briefly demonstrates containerlab features and explains its purpose:</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Lab as Code (IaC) approach     Declarative way of defining the labs by means of the topology definition <code>clab</code> files.</li> <li>Network Operating Systems centric     Focus on containerized Network Operating Systems. The sophisticated startup requirements of various NOS containers are abstracted with kinds which allows the user to focus on the use cases, rather than infrastructure hurdles.</li> <li>VM based nodes friendly     With the vrnetlab integration it is possible to get the best of two worlds - running virtualized and containerized nodes alike with the same IaaC approach and workflows.</li> <li>Multi-vendor and open     Although being kick-started by Nokia engineers, containerlab doesn't take sides and supports NOSes from other vendors and opensource projects.</li> <li>Lab orchestration     Starting the containers and interconnecting them alone is already good, but containerlab packages even more features like managing lab lifecycle: deploy, destroy, save, inspect, graph operations.</li> <li>Scaled labs generator     With <code>generate</code> capabilities of containerlab it possible to define/launch CLOS-based topologies of arbitrary scale. Just say how many tiers you need and how big each tier is, the rest will be done in a split second.</li> <li>Simplicity and convenience     Starting from frictionless installation and upgrade capabilities and ranging to the behind-the-scenes link wiring machinery, containerlab does its best for you to enjoy the tool.</li> <li>Fast     Blazing fast way to create container based labs on any Linux system with Docker.</li> <li>Automated TLS certificates provisioning     The nodes which require TLS certs will get them automatically on boot.</li> <li>Documentation is a first-class citizen     We do not let our users guess by making a complete, concise and clean documentation.</li> <li>Lab catalog    The \"most-wanted\" lab topologies are documented and included with containerlab installation. Based on this cherry-picked selection you can start crafting the labs answering your needs.</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<ul> <li>Labs and demos     Containerlab was meant to be a tool for provisioning networking labs built with containers. It is free, open and ubiquitous. No software apart from Docker is required!     As with any lab environment it allows the users to validate features, topologies, perform interop testing, datapath testing, etc.     It is also a perfect companion for your next demo. Deploy the lab fast, with all its configuration stored as a code -&gt; destroy when done.</li> <li>Testing and CI     Because of the containerlab's single-binary packaging and code-based lab definition files, it was never that easy to spin up a test bed for CI. Gitlab CI, Github Actions and virtually any CI system will be able to spin up containerlab topologies in a single simple command.</li> <li>Telemetry validation     Coupling modern telemetry stacks with containerlab labs make a perfect fit for Telemetry use cases validation. Spin up a lab with containerized network functions with a telemetry on the side, and run comprehensive telemetry use cases.</li> </ul>"},{"location":"#join-us","title":"Join us","text":"<p>Have questions, ideas, bug reports or just want to chat? Come join our discord server.</p>"},{"location":"community/","title":"Community","text":"<p>Containerlab openness and focus on multivendor labs were key to its success and adoption. With more than a dozen Network Operating Systems spread across several networking vendors and opensource teams, it is a tool that can answer the needs of a broad network engineers community.</p>"},{"location":"community/#chat-servers","title":"Chat servers","text":""},{"location":"community/#discord","title":"Discord","text":"<p>Growing the number of supported NOSes is a task that can't be done by a single person, and there the community role is adamant. To support and cherish the growing containerlab community and provide better feedback and discussions platform, we launched containerlab's official Discord server where maintainers and users hang out.</p> <p>Everybody is welcome to join and chat with our community members about all things containerlab!</p> <p> Join Containerlab Discord Server</p>"},{"location":"community/#irc","title":"IRC","text":"<p>For those who cherish IRC in their hearts, a community-led IRC channel <code>#containerlab</code> is available at IRC Libera server.</p>"},{"location":"community/#in-the-media","title":"In The Media","text":"<p>We are always happy to showcase containerlab and demonstrate its powers. Luckily, the network engineering community has lots of events worldwide, and we participated in some. Below you will find recordings of containerlab talks in different formats and on various venues listed in reverse chronological order<sup>1</sup>.</p>"},{"location":"community/#autocon-1","title":"Autocon 1","text":"<p> When to use and not to use Containerlab? \u00b7  2024-05-29</p> <p>As with every tool, it is important to use it in the right context. It is possible to drive a nail with a screwdriver, but a hammer would be more appropriate. In this talk, Roman Dodin explains the use cases where containerlab shines and where it might not be the best fit.</p> <p>Participants:</p> <ul> <li>  Roman Dodin</li> </ul>"},{"location":"community/#ripe-88","title":"RIPE 88","text":"<p> Simulating Networks at Scale with Clabernetes \u00b7  2024-05-20</p> <p>Carl Montanari and Simon Peccaud introduced Clabernetes to RIPE 88 attendees.</p> <p>OVHCloud was an early adopter of Clabernetes, which helped them to simulate the WAN network in a way horizontally scalable way that naturally plugged into the rest of their network automation and CI workflow.</p> <p>Participants:</p> <ul> <li>  Carl Montanari</li> <li>  Simon Peccaud</li> </ul>"},{"location":"community/#nfd-33","title":"NFD 33","text":"<p> Lab as Code with Containerlab \u00b7  2023-10-30</p> <p>Three years in the making with monthly releases, containerlab has become a de-facto standard for network engineers to build and run containerized network labs. In this talk, Roman explains the motivation behind containerlab, the benefits of defining labs as code, and shows containerlab workflow in action by spinning up SR Linux Telemetry Lab.</p> <p>Participants:</p> <ul> <li>  Roman Dodin</li> </ul>"},{"location":"community/#telco-podcast","title":"Telco podcast","text":"<p> Networking Topology As Code \u00b7  2022-04-21</p> <p>A year in the making; containerlab made lots of waves in the community threading its way to engineers hearts through simple and lightweight abstractions tailored to networking labs problem space. In this podcast Maciej and Anton from The Telco podcast interviewed Roman Dodin on containerlab' current state and many more.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#packet-pushers-tech-bytes","title":"Packet Pushers Tech Bytes","text":"<p> Containerlab Makes Container And VM Networking Labs Easy \u00b7  2021-11-15</p> <p>A short, 14 minutes long introductory talk about Containerlab. If you wanted to know what containerlab is, but all you have is 15 minutes break - go check it out.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#nanog-83","title":"NANOG 83","text":"<p> Containerlab - running networking labs with Docker UX \u00b7  2021-11-03</p> <p>Our very first NANOG appearance and we went full-steam there. This talk is the most comprehensive containerlab tutorial captured to that date. It starts with the basics and escalates to the advanced DC fabric deployment with HA telemetry cluster created. All driven by a single containerlab topology file.</p> <p>Participants:</p> <ul> <li>  Roman Dodin</li> <li> Karim Radhouani</li> </ul>"},{"location":"community/#open-networking-edge-summit-2021","title":"Open Networking &amp; Edge Summit 2021","text":"<p> Containerlab - a Modern way to Deploy Networking Topologies for Labs, CI, and Testing \u00b7  2021-10-11</p> <p>This 30mins screencast introduces containerlab by going through a multivendor lab example consisting of Nokia SR Linux, Arista cEOS, and GoBGP containers participating in a route reflection scenario.</p> <p>The talk starts with the reasoning as to why containerlab development was warranted and what features of container-based labs we wanted to have. Then we start building the lab step by step, explaining how the containerlab topology file is structured.</p> <p>Participants:   Roman Dodin</p>"},{"location":"community/#nlnog-2021","title":"NLNOG 2021","text":"<p> Running networking labs with Docker User Experience \u00b7  2021-09-05</p> <p>The first public talk around containerlab happened in the Netherlands at an in-person (sic!) networking event NLNOG 2021.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#modem-podcast-s01e10","title":"Modem Podcast s01e10","text":"<p> Containerlab: Declarative network labbing \u00b7  2021-06-06</p> <p>Building large-scale network labs can be tedious and error-prone \u2014 More importantly, they can be notoriously hard to spin up automatically. Containerlab is a new tool that promises to \u201credefine the way you run networking labs\u201d, and I really think it hits that target. On this episode of the Modulate Demodulate podcast, Nick and Chris C. are joined by Roman Dodin, one of the brains behind Containerlab.</p> <p>Participants:   Roman Dodin</p>"},{"location":"community/#blogs","title":"Blogs","text":"<p>The power of the community is in its members. We are delighted to have containerlab users who share their experience working with the tool, unveiling new use cases, and providing a personal touch to the workflows.</p> <p>This section logs the most notable blogs, streams, and and demos delivered by containerlab users worldwide.</p> <p>Want to share your experience?</p> <p>Do you have a blog around containerlab that you want to share with the community? Please let us know by mentioning it in the Comments section at the bottom of this page.</p>"},{"location":"community/#extending-the-kubernetes-host-network-with-evpn-tunnels","title":"Extending the Kubernetes (host) network with EVPN tunnels","text":"<p> Blog by Federico Paolinelli \u00b7  2024-06-14</p> <p>It is refreshing to see how people find new ways to use containerlab. In this blog post, Federico Paolinelli explains how he used containerlab to create a lab environment to test a solution to extend the Kubernetes host network with EVPN tunnels and FRR.</p> <p>Containerlab helped to build the lab environment where a KinD-based k8s cluster is connected to an EVPN emulated fabric. Thanks to the native KinD support in containerlab it was all a single lab package to deploy and test the solution.</p>"},{"location":"community/#using-containerlab-and-google-cloud-platform-to-lab-aruba-cx","title":"Using Containerlab and Google Cloud Platform to lab Aruba CX","text":"<p> Blog by Crispy \u00b7  2024-06-02</p> <p>Crispy shares his experience with switching from EVE-NG to Containerlab to lab Aruba CX switches. The blog post is a step-by-step guide on how to create a lab environment in GCP with containerlab and run Aruba CX switches in it.</p> <p>From Containerlab team</p> <p>Note, that while GCP is a nice way to run containerlab labs, it is not the only one. You may also try Codespaces as a \"free\" alternative.</p>"},{"location":"community/#building-a-home-lab-with-containerlab","title":"Building a home lab with containerlab","text":"<p> Blog by rarovar \u00b7  2024-04-18</p> <p>It is rare to see a blog post series that starts from, literally, bare metal and builds up to an in-depth explanation of how to use containerlab to build a network lab. In this blog post series, rarovar shares his Containerlab journey in 4 acts:</p> <ul> <li>Part 1: Preparing your Server</li> <li>Part 2: Creating your Containerlab VM</li> <li>Part 3: Installing Containerlab on your VM</li> <li>Part 4: Creating your first lab</li> </ul> <p>You will find links to all parts here.</p> <p>It was especially interesting to read how rarovar built the lab environment based on NUC hardware in Part1. But do check other parts as well!</p>"},{"location":"community/#containerlab-for-kubernetes-networking-experimentation","title":"Containerlab for Kubernetes Networking Experimentation","text":"<p> Blog by Dave Davis \u00b7  2024-03-25</p> <p>Dave' opening paragraph sets the stage for the blog post:</p> <p>Recently, my focus has shifted towards Kubernetes, where I've been assisting customers in designing resilient and scalable clusters. I'm on the lookout for a tool that can quickly set up test Kubernetes clusters with diverse network topologies. Ideally the solution would offer version control, rapid deployments, Kubernetes support, and ease of sharing.</p> <p>As you can see, containerlab fits this bill quite nicely. Dave goes on to explain how he used containerlab to create a lab environment to test Kubernetes networking solutions.</p>"},{"location":"community/#powering-up-your-netbox-labs-with-containerlab","title":"Powering up your Netbox labs with Containerlab","text":"<p> Blog by Rich Bibby \u00b7  2024-03-20</p> <p>Our friends at NetboxLabs have authored a nice blog post - Network Configuration Assurance With NetBox and Ansible - and they have used containerlab to create a lab environment to present their solution.</p> <p>It is always great to see containerlab being used to augment other tools and solutions, providing the glue between the applications and the network devices all within a tiny declarative packaging and a lightweight CLI to make it all work.</p>"},{"location":"community/#containerlab-creating-network-labs-cant-be-any-easier","title":"Containerlab - Creating Network Labs Can't Be Any Easier","text":"<p> Blog by Suresh Vina \u00b7  2024-03-15</p> <p>Suresh, a Networking enthusiast based in the lovely city of London, shares his experience with Containerlab after being married to traditional lab emulation tools for a long time. He explains Containerlab's core concepts using a lab with cEOS and Palo Alto nodes, showcasing the ease of use and flexibility of the tool in a multivendor setting.</p>"},{"location":"community/#rapid-deployment-of-cilium-bgp-environments-using-containerlab-kind","title":"Rapid deployment of Cilium BGP environments using Containerlab + Kind","text":"<p> Blog by SoByte \u00b7  2022-09-02</p> <p>A clever use of KinD and containerlab to deploy a k8s cluster with a network underlay to test Cilium's BGP capabilities.</p>"},{"location":"community/#cilium-bgp-tested-with-containerlab","title":"Cilium BGP tested with Containerlab","text":"<p> screencast by Nico Vibert \u00b7  2022-07-20</p> <p>In this video, Senior Technical Marketing Engineer Nico Vibert walks through BGP enhancements in Cilium 1.12, with the integration with GoBGP. Nico uses a spicey mix of kind and containerlab to stand up a lab that simulates a datacenter POD with Cilium-powered k8s nodes talking TOR switches and interconnected via the core layer.</p> <p></p> <p>Clever use of containerlab with a focus on the workload capabilities and integration with kind clusters.</p>"},{"location":"community/#containerlab-with-inmanta","title":"Containerlab with Inmanta","text":"<p> screencast by Inmanta \u00b7  2022-03-20</p> <p>Inmanta is a network orchestration and automation platform. In these YouTube series, Inmanta's engineer explains how this platform can help you manage network infrastructure provided by Containerlab. Make sure to check subsequent videos in their channel where multiple angles of containerlab are shown.</p>"},{"location":"community/#containerlab-based-ddos-testbed","title":"Containerlab-based DDOS testbed","text":"<p> Blog by Peter Phaal \u00b7  2022-03-16</p> <p>Real-time telemetry from a 5 stage Clos fabric describes lightweight emulation of realistic data center switch topologies using Containerlab. This article extends the testbed to experiment with distributed denial of service (DDoS) detection and mitigation techniques described in Real-time DDoS mitigation using BGP RTBH and FlowSpec.</p>"},{"location":"community/#multi-vendor-evpn-vxlan-setup-with-containerlab","title":"Multi-vendor EVPN VXLAN setup with Containerlab","text":"<p> Blog by @aninchat \u00b7  2022-03-12</p> <p>In this post, we deploy a multivendor EVPN L2 overlay fabric, with BGP in the underlay as well. The entire fabric deployment is automated with Ansible, and Containerlab is used to define and deploy the actual topology.</p>"},{"location":"community/#network-simulation-tools-and-containerlab","title":"Network Simulation Tools and Containerlab","text":"<p> Blog by @JulioPDX \u00b7  2022-02-13</p> <p>I\u2019ve been progressing through a series of technical books, some of which I\u2019ve shared on other blogs. A few of them focus on BGP. BGP being so broad, I decided to create a challenge lab. Creating the challenge/troubleshooting labs has really made more concepts stick. I\u2019m trying to use the principle of teaching someone to make the learning last. These have been incredibly fun to create and the community interaction has been amazing. One brave soul(Jeroen van Bemmel) shared his solution. I was fascinated on his solution and how he created his topology with Containerlab and net-sim tools.</p>"},{"location":"community/#juniper-vqfx-and-containerlab","title":"Juniper vQFX and containerlab","text":"<p> Blog by @aninchat \u00b7  2022-02-06</p> <p>In this post, we look at how Containerlab can be used to quickly spin up vQFX topologies for network validation and testing. We\u2019ll walk through the entire process - how to build docker images from vQFX images, what happens behind the scenes when bringing these containers up and how to build/verify your topology.</p>"},{"location":"community/#multipoint-redistribution-and-sr-linux","title":"Multipoint Redistribution and SR Linux","text":"<p> Blog by @JulioPDX \u00b7  2022-01-22</p> <p>I\u2019ve been working my way through Optimal Routing Design by Russ White, Don Slice, and Alvaro Retana. It has been a great read so far and I highly recommend it. When I work through a book I usually try and lab up any concept I can, this helps me make it stick as much as possible. Early on in the book the authors mention redistribution and possible issues that can come with doing it at more than one point in the network. So here we are reading this post. I hope you enjoy and possibly learn something along the way.</p> <p>Discussions: </p>"},{"location":"community/#real-time-telemetry-from-a-5-stage-clos-fabric","title":"Real-time telemetry from a 5 stage Clos fabric","text":"<p> Blog by Peter Phaal \u00b7  2022-02-21</p> <p>As Peter wrote, sFlow is a companion to the Streaming Telemetry applications. It's UDP based transport claims to be more suitable for situations where networks are congested. In this blog Peter explains how containerlab can deploy a Clos topolgy with sFlow collector and endpoints generating traffic to see the whole machinery in action.</p> <p>Discussions: </p>"},{"location":"community/#my-journey-and-experience-with-containerlab","title":"My Journey and Experience with Containerlab","text":"<p> Blog by @JulioPDX \u00b7  2021-12-10</p> <p>In this blog Julio took containerlab for a spin and shares his experience with it. His lab consists of a few Arista cEOS nodes which he then provisions with Nornir, using Ansible inventory generated by containerlab.</p> <p>Discussions:  \u00b7 </p>"},{"location":"community/#building-your-own-data-center-fabric-with-containerlab","title":"Building Your Own Data Center Fabric with Containerlab","text":"<p> Blog and a screencast by Alperen Akpinar \u00b7  2021-08-24</p> <p>Alperen did a great job explaining how to build a DC fabric topology using containerlab. This was the first post in the series, making it an excellent intro to containerlab, especially when you follow the screencast and watch the topology buildup live.</p> <p>In a subsequent post, Alperen explains how to configure the SR Linux fabric he just built.</p>"},{"location":"community/#how-to-consistently-run-a-temporary-vm-on-aws-to-run-containerlab","title":"How to consistently run a temporary VM on AWS to run Containerlab","text":"<p> Blog by @_nleiva \u00b7  2021-07-12</p> <p>Create and then destroy a cloud environment ready to Containerlab with pre-loaded topology files, which just work if you use FRR and SR Linux. If you need to run Arista's cEOS, see: Getting cEOS image.</p> <p>This post describes the benefits of running any temporary workload in the cloud to then focus on Containerlab, so skip to the \u201cNetwork testing challenges\u201d section if you want to get to the meat of it. More details on the labs included at Network Labs.</p> <ol> <li> <p>most recent talks appear first.\u00a0\u21a9</p> </li> </ol>"},{"location":"install/","title":"Installation","text":"<p>Containerlab is distributed as a Linux deb/rpm/apk package for amd64 and arm64 architectures and can be installed on any Debian- or RHEL-like distributive in a matter of a few seconds.</p>"},{"location":"install/#pre-requisites","title":"Pre-requisites","text":"<p>The following requirements must be satisfied to let containerlab tool run successfully:</p> <ul> <li>A user should have <code>sudo</code> privileges to run containerlab.</li> <li>A Linux server/VM<sup>1</sup> and Docker installed.</li> <li>Load container images (e.g. Nokia SR Linux, Arista cEOS) that are not downloadable from a container registry. Containerlab will try to pull images at runtime if they do not exist locally.</li> </ul>"},{"location":"install/#quick-setup","title":"Quick setup","text":"<p>The easiest way to get started with containerlab is to use the quick setup script that installs all of the following components in one go (or allows to install them separately):</p> <ul> <li>docker (docker-ce), docker compose</li> <li>Containerlab (using the package repository)</li> <li><code>gh</code> CLI tool</li> </ul> <p>The script has been tested on the following OSes:</p> <ul> <li>Ubuntu 20.04, 22.04, 23.10, 24.04</li> <li>Debian 11, 12</li> <li>Red Hat Enterprise Linux 9</li> <li>CentOS Stream 9</li> <li>Fedora Server 40 (should work on other variants of Fedora)</li> <li>Rocky Linux 9.3, 8.8 (should work on any 9.x and 8.x release)</li> </ul> <p>To install all components at once, run the following command on any of the supported OSes:</p> <pre><code>curl -sL https://containerlab.dev/setup | sudo -E bash -s \"all\"\n</code></pre> <p>By default, this will also configure sshd on the system to increase max auth tries so unknown keys don't lock ssh attempts. This behaviour can be turned off by setting the environment variable \"SETUP_SSHD\" to \"false\" before running the command shown above. The environment variable can be set and exported with the command shown below.</p> <pre><code>export SETUP_SSHD=\"false\"\n</code></pre> <p>To complete installation and enable sudo-less <code>docker</code> command execution, please run <code>newgrp docker</code> or logout and log back in.</p> <p>To install an individual component, specify the function name as an argument to the script. For example, to install only <code>docker</code>:</p> <pre><code>curl -sL https://containerlab.dev/setup | sudo -E bash -s \"install-docker\"\n</code></pre> <p>If you don't have your own shell configuration and want to have a slightly better bash PS1 prompt you can also run this script:</p> <pre><code>curl -sL https://containerlab.dev/setup | sudo -E bash -s \"setup-bash-prompt\"\n</code></pre> <p>Log out and log back in to see the new two-line prompt in action:</p> <pre><code>[*]\u2500[clab]\u2500[~]\n\u2514\u2500\u2500&gt;\n</code></pre>"},{"location":"install/#install-script","title":"Install script","text":"<p>Containerlab can be installed using the installation script that detects the operating system type and installs the relevant package:</p> <p>Note</p> <p>Containerlab is distributed via deb/rpm packages, thus only Debian- and RHEL-like distributives can leverage package installation. Other systems can follow the manual installation procedure.</p> Latest releaseSpecific versionwith <code>wget</code> <p>Download and install the latest release (may require <code>sudo</code>):</p> <pre><code>bash -c \"$(curl -sL https://get.containerlab.dev)\"\n</code></pre> <p>Download a specific version. Versions can be found on the Releases page.</p> <pre><code>bash -c \"$(curl -sL https://get.containerlab.dev)\" -- -v 0.10.3\n</code></pre> <pre><code># with wget\nbash -c \"$(wget -qO - https://get.containerlab.dev)\"\n</code></pre> <p>Since the installation script uses GitHub API, users may hit the rate limit imposed by GitHub. To avoid this, users can pass their personal GitHub token as an env var to the installation script:</p> <pre><code>GITHUB_TOKEN=&lt;your token&gt; bash -c \"$(curl -sL https://get.containerlab.dev)\"\n</code></pre>"},{"location":"install/#package-managers","title":"Package managers","text":"<p>It is possible to install official containerlab releases via public APT/YUM repository.</p> APTYUMDNF4DNF5APKAUR <pre><code>echo \"deb [trusted=yes] https://netdevops.fury.site/apt/ /\" | \\\nsudo tee -a /etc/apt/sources.list.d/netdevops.list\n\nsudo apt update &amp;&amp; sudo apt install containerlab\n</code></pre> <pre><code>sudo yum-config-manager --add-repo=https://netdevops.fury.site/yum/ &amp;&amp; \\\necho \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/netdevops.fury.site_yum_.repo\n\nsudo yum install containerlab\n</code></pre> <pre><code>sudo dnf config-manager -y --add-repo \"https://netdevops.fury.site/yum/\" &amp;&amp; \\\necho \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/netdevops.fury.site_yum_.repo\n\nsudo dnf install containerlab\n</code></pre> <pre><code>sudo dnf config-manager addrepo --set=baseurl=\"https://netdevops.fury.site/yum/\" &amp;&amp; \\\necho \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/netdevops.fury.site_yum_.repo\n\nsudo dnf install containerlab\n</code></pre> <p>Download <code>.apk</code> package from Github releases.</p> <p>Arch Linux users can download a package from this AUR repository.</p> Manual package installation <p>Alternatively, users can manually download the deb/rpm package from the Github releases page.</p> <p>example:</p> <pre><code># manually install latest release with package managers\nLATEST=$(curl -s https://github.com/srl-labs/containerlab/releases/latest | sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/')\n# with yum\nyum install \"https://github.com/srl-labs/containerlab/releases/download/v${LATEST}/containerlab_${LATEST}_linux_amd64.rpm\"\n# with dpkg\ncurl -sL -o /tmp/clab.deb \"https://github.com/srl-labs/containerlab/releases/download/v${LATEST}/containerlab_${LATEST}_linux_amd64.deb\" &amp;&amp; dpkg -i /tmp/clab.deb\n\n# install specific release with yum\nyum install https://github.com/srl-labs/containerlab/releases/download/v0.7.0/containerlab_0.7.0_linux_386.rpm\n</code></pre> <p>The package installer will put the <code>containerlab</code> binary in the <code>/usr/bin</code> directory as well as create the <code>/usr/bin/clab -&gt; /usr/bin/containerlab</code> symlink. The symlink allows the users to save on typing when they use containerlab: <code>clab &lt;command&gt;</code>.</p>"},{"location":"install/#windows","title":"Windows","text":"<p>Containerlab runs on Windows powered by Windows Subsystem Linux (aka WSL), where you can run Containerlab directly or in a Devcontainer. Open up Containerlab on Windows documentation for more details.</p>"},{"location":"install/#apple-macos","title":"Apple macOS","text":"<p>Running containerlab on macOS is possible both on ARM (M1/M2/M3/etc) and Intel chipsets. For a long time, we had many caveats around M-chipsets on Macs, but with the introduction of ARM64-native NOSes like Nokia SR Linux and Arista cEOS, powered by Rosetta emulation for x86_64-based NOSes, it is now possible to run containerlab on ARM-based Macs.</p> <p>Since we wanted to share our experience with running containerlab on macOS in details, we have created a separate - Containerlab on macOS - guide.</p>"},{"location":"install/#container","title":"Container","text":"<p>Containerlab is also available in a container packaging. The latest containerlab release can be pulled with:</p> <pre><code>docker pull ghcr.io/srl-labs/clab\n</code></pre> <p>To pick any of the released versions starting from release 0.19.0, use the version number as a tag, for example, <code>docker pull ghcr.io/srl-labs/clab:0.19.0</code></p> <p>Since containerlab itself deploys containers and creates veth pairs, its run instructions are a bit more complex, but still, it is a copy-paste-able command.</p> <p>For example, if your lab files are contained within the current working directory - <code>$(pwd)</code> - then you can launch containerlab container as follows:</p> <pre><code>docker run --rm -it --privileged \\\n    --network host \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v /var/run/netns:/var/run/netns \\\n    -v /etc/hosts:/etc/hosts \\\n    -v /var/lib/docker/containers:/var/lib/docker/containers \\\n    --pid=\"host\" \\\n    -v $(pwd):$(pwd) \\\n    -w $(pwd) \\\n    ghcr.io/srl-labs/clab bash\n</code></pre> <p>Within the started container you can use the same <code>containerlab deploy/destroy/inspect</code> commands to manage your labs.</p> <p>Note</p> <p>Containerlab' container command is itself <code>containerlab</code>, so you can deploy a lab without invoking a shell, for example:</p> <pre><code>docker run --rm -it --privileged \\\n# &lt;run options omitted&gt;\n-w $(pwd) \\\nghcr.io/srl-labs/clab deploy -t somelab.clab.yml\n</code></pre>"},{"location":"install/#manual-installation","title":"Manual installation","text":"<p>If the linux distributive can't install deb/rpm packages, containerlab can be installed from the archive:</p> <pre><code># get the latest available tag\nLATEST=$(curl -s https://github.com/srl-labs/containerlab/releases/latest | \\\n       sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/')\n\n# download tar.gz archive\ncurl -L -o /tmp/clab.tar.gz \"https://github.com/srl-labs/containerlab/releases/download/v${LATEST}/containerlab_${LATEST}_Linux_amd64.tar.gz\"\n\n# create containerlab directory\nmkdir -p /etc/containerlab\n\n# extract downloaded archive into the containerlab directory\ntar -zxvf /tmp/clab.tar.gz -C /etc/containerlab\n\n# (optional) move containerlab binary somewhere in the $PATH\nmv /etc/containerlab/containerlab /usr/bin &amp;&amp; chmod a+x /usr/bin/containerlab\n</code></pre>"},{"location":"install/#upgrade","title":"Upgrade","text":"<p>To upgrade <code>containerlab</code> to the latest available version issue the following command<sup>2</sup>:</p> <pre><code>sudo -E containerlab version upgrade\n</code></pre> <p>This command will fetch the installation script and will upgrade the tool to its most recent version. In case of GitHub rate limit, provide <code>GITHUB_TOKEN</code> env var with your personal GitHub token to the upgrade command.</p> <p>Or leverage <code>apt</code>/<code>yum</code> utilities if containerlab repo was added as explained in the Package managers section.</p>"},{"location":"install/#from-source","title":"From source","text":"<p>To build containerlab from source:</p> with <code>go build</code>with goreleaser <p>To build containerlab from source, clone the repository and issue <code>go build</code> at its root.</p> <p>When we release containerlab we use goreleaser project to build binaries for all supported platforms as well as the deb/rpm packages. Users can install <code>goreleaser</code> and do the same locally by issuing the following command:</p> <pre><code>goreleaser --snapshot --skip-publish --rm-dist\n</code></pre>"},{"location":"install/#uninstall","title":"Uninstall","text":"<p>To uninstall containerlab when it was installed via installation script or packages:</p> Debian-based systemRPM-based systemsManual removal <pre><code>apt remove containerlab\n</code></pre> <pre><code>yum remove containerlab\n</code></pre> <p>Containerlab binary is located at <code>/usr/bin/containerlab</code>. In addition to the binary, containerlab directory with static files may be found at <code>/etc/containerlab</code>.</p>"},{"location":"install/#selinux","title":"SELinux","text":"<p>When SELinux set to enforced mode containerlab binary might fail to execute with <code>Segmentation fault (core dumped)</code> error. This might be because containerlab binary is compressed with upx and selinux prevents it from being decompressed by default.</p> <p>To fix this:</p> <pre><code>sudo semanage fcontext -a -t textrel_shlib_t $(which containerlab)\nsudo restorecon $(which containerlab)\n</code></pre> <p>or more globally:</p> <pre><code>sudo setsebool -P selinuxuser_execmod 1\n</code></pre> <ol> <li> <p>Most containerized NOS will require &gt;1 vCPU. RAM size depends on the lab size. IPv6 should not be disabled in the kernel.\u00a0\u21a9</p> </li> <li> <p>only available if installed from packages\u00a0\u21a9</p> </li> </ol>"},{"location":"macos/","title":"Containerlab on macOS","text":"Summary for the impatient <ol> <li>Install OrbStack<sup>1</sup> on your macOS</li> <li>Create an arm64 Linux VM using OrbStack</li> <li>Install containerlab in the VM using the usual installation instructions</li> <li>Check what images can/should work on ARM64</li> <li>Deploy your lab. You can see the demo of this workflow in this YT video.</li> </ol> <p>Or use the Devcontainer if running another VM is not your thing.</p> <p>If you run an Intel mac, you still use OrbStack to deploy a VM, but you will not need to worry about the hard-to-find ARM64 images, as your processor runs x86_64 natively.</p> <p>For quite some time, we have been saying that containerlab and macOS is a challenging mix. This statement has been echoed through multiple workshops/demos and was based on the following reasons:</p> <ol> <li>ARM64 Network OS images: With the shift to ARM64 architecture made by Apple (and Microsoft<sup>2</sup>), we found ourselves in a situation where 99% of existing network OSes were not compiled for ARM64 architecture. This meant that containerlab users would have to rely on x86_64 emulation via Rosetta or QEMU, which imposes a significant performance penalty, often making the emulation unusable for practical purposes.</li> <li>Docker on macOS: Since containerlab is reliant on Docker for container orchestrations, it needs Docker to be natively installed on the host.     On macOS, Docker is always provided as a Linux/arm64 VM that runs the docker daemon, with docker-cli running on macOS natively. You can imagine, that dealing with a VM that runs a network topology poses some UX challenges, like getting access to the exposed ports or dealing with files on macOS that needs to be accessible to the Docker VM.  </li> <li>Linux on macOS? It is not only Docker that containerlab is based on. We leverage some Linux kernel APIs (like netlink) either directly or via Docker to be available to setup links, namespaces, bind-mounts, etc.     Naturally, Darwin (macOS kernel) is not Linux, and while it is POSIX compliant, it is not a drop-in replacement for Linux. This means that some of the Linux-specific features that containerlab relies on are simply not present on macOS.</li> </ol> <p>Looking at the above challenges one might think that containerlab on macOS is a lost cause. However, recently things have started to take a good course, and we are happy to say that for certain labs Containerlab on macOS might be even a better (!) choice overall.</p> <p>As a long time macOS user, Roman recorded an in-depth video demonstrating how to run containerlab topologies on macOS using the tools of his choice. You can watch the video below or jump to the text version of the guide below.</p>"},{"location":"macos/#network-os-images","title":"Network OS Images","text":"<p>The first thing one needs to understand that if you run macOS on ARM chipset (M1+), then you should use ARM64 network OS images whenever possible. This will give you the best performance and compatibility.</p> <p>With Rosetta virtualisation it is possible to run x86_64 images on ARM64, but this comes with the performance penalty that might even make nodes not work at all.</p> <p>VM-based images</p> <p>VM-based images built with hellt/vrnetlab require nested virtualization support, which is only available on M3+ chip with macOS version 15 and above.</p> <p>If you happen to satisfy these requirements, please let us know in the comments which images you were able to run on your M3+ Mac.</p>"},{"location":"macos/#native-arm64-network-os-and-application-images","title":"Native ARM64 Network OS and application images","text":"<p>Finally  some good news on this front, as vendors started to release or at least announce ARM64 versions of their network OSes. Nokia first released the preview version of their freely distributed SR Linux for ARM64, and Arista announced the imminent cEOS availability sometime in 2024.</p> <p>You can also get FRR container for ARM64 architecture from their container registry.</p> <p>And if all you have to play with is pure control plane, you can use Juniper cRPD, which is also available for ARM64.</p> <p>Yes, SR Linux, cEOS, FRR do not cover all the network OSes out there, but it is a good start, and we hope that more vendors will follow the trend.</p> <p>The good news is that almost all of the popular applications that we see being used in containerlabs are already built for ARM. Your streaming telemetry stack (gnmic, prometheus/influx, grafana), regular client-emulating endpoints such as Alpine or a collection of network related tools in the network-multitool image had already been supporting ARM architecture. You can leverage the sheer ecosystem multi-arch applications that are available in the public registries.</p>"},{"location":"macos/#running-under-rosetta","title":"Running under Rosetta","text":"<p>If the image you're looking for is not available in ARM64, you can still try running the AMD64 version of the image under Rosetta emulation. Rosetta is a macOS virtualisation layer that allows you running x86_64 code on ARM64 architecture.</p> <p>It has been known to work for the following images:</p> <ul> <li>Arista cEOS x64</li> <li>Cisco IOL</li> </ul>"},{"location":"macos/#docker-on-macos","title":"Docker on macOS","text":"<p>Ever since macOS switched to ARM architecture for their processors, people in a \"containers camp\" have been busy making sure that Docker works well on macOS's new architecture.</p>"},{"location":"macos/#how-docker-runs-on-macs","title":"How Docker runs on Macs","text":"<p>But before we start talking about Docker on ARM Macs, let's remind ourselves how Docker works on macOS with Intel processors.</p> Docker on Intel Macs <p>At the heart of any product or project that enables the Docker engine on Mac<sup>3</sup> is a Linux VM that hosts the Docker daemon, aka \"the engine\". This VM is created and managed by the application that sets up Docker on your desktop OS. The Linux VM is a mandatory piece because the whole container ecosystem is built around Linux kernel features and APIs. Therefore, running Docker on any host with an operating system other than Linux requires a Linux VM.</p> <p>As shown above, on Intel Macs, the macOS runs Darwin kernel on top of an AMD64 (aka x86_64) architecture, and consequently, the Docker VM runs the same architecture. The architecture of the Docker VM is the same as the host architecture allowing for a performant virtualization, since no processor emulation is needed.</p> <p>Now let's see how things change when we switch to ARM Macs:</p> Docker on ARM Macs <p>The diagram looks 99% the same as for the Intel Macs, the only difference being the architecture that macOS runs on and consequently the architecture of the Docker VM. Now we run ARM64 architecture on the host, and the Docker VM is also ARM64.</p> Native vs Emulation <p>If Docker runs exactly the same on ARM Macs as it does on Intel Macs, then why is it suddenly a problem to run containerlab on ARM Macs?</p> <p>Well, it all comes down to the requirement of having ARM64 network OS images that we discussed earlier. Now when your Docker VM runs Linux/ARM64, you can run natively only ARM64-native software in it, and we, as a network community, are not having a lot of ARM64-native network OSes. It is getting better, but we are not there yet to claim 100% parity with the x86_64 world.</p> <p>You should strive to run the native images as much as possible, as it gives you the best performance and compatibility. But how do you tell if the image is ARM64-native or not? A lot of applications that you might want to run in your containerlab topologies are already ARM64-native and often available as a multi-arch image.</p> <p>When running the following <code>docker image inspect</code> command, you can grep the <code>Architecture</code> field to see if the image is ARM64-native:</p> <pre><code>docker image inspect ghcr.io/nokia/srlinux:24.10.1 -f '{{.Architecture}}'\narm64\n</code></pre> <p>Running the same command for an image that is not ARM64-native will return <code>amd64</code>:</p> <pre><code>docker image inspect goatatwork/snmpwalk -f '{{.Architecture}}'\namd64\n</code></pre> <p>Still, it will be possible to run the <code>snmpwalk</code> container, thanks to Rosetta emulation.</p>"},{"location":"macos/#software","title":"Software","text":"<p>There are many software solutions that deliver Docker on macOS, both for Intel and ARM Macs.</p> <ul> <li> OrbStack - a great UX and performance. A choice of many and is recommended by Containerlab maintainer. Free for personal use.</li> <li>Docker Desktop - the original and the most popular Docker on macOS.</li> <li>Rancher Desktop - another popular software.</li> <li>Container Desktop - a cross-platform solution.</li> <li>CoLima - a lightweight, CLI-based VM solution.</li> </ul> <p>The way most users use Containerlab on macOS, though, not directly leveraging Docker that is provided by one of the above solutions. Instead, it might be easier to spin up a VM, powered by the above-mentioned software products, and install Containerlab natively inside this arm64/Linux VM. You can see this workflow demonstration in this YT video.</p>"},{"location":"macos/#devcontainer","title":"Devcontainer","text":"<p>Another convenient option to run containerlab on ARM/Intel Macs (and Windows) is to use the Devcontainer feature that works great with VS Code and many other IDE's.</p> <p>A development container (or devcontainer) allows you to use a container as a full-featured development environment. By creating the <code>devcontainer.json</code><sup>4</sup> file, you define the development environment for your project. Containerlab project maintains a set of pre-built multi-arch devcontainer images that you can use to run containerlabs. It was initially created to power containerlab in codespaces, but it is a perfect fit for running containerlab on a wide range of OSes such as macOS and Windows.</p> <p>Requirements</p> <ol> <li>Starting with Containerlab v0.60.0, you can use the devcontainer with ARM64 macOS to run containerlabs.</li> <li>VS Code Dev Containers extension needs to be installed to use this feature with VS Code.</li> </ol> <p>To start using the devcontainer, you have to create a <code>devcontainer.json</code> file in your project directory where you have your containerlab topology. If you're using Containerlab the right way, your labs are neatly stored in a git repo; in this case the <code>devcontainer.json</code> file will be part of the repo.</p> <p>If you prefer a video tutorial, we've got you covered, else continue reading.</p>"},{"location":"macos/#devcontainer-flavors","title":"Devcontainer flavors","text":"<p>Containerlab provides two types of devcontainer images:</p>  Docker In Docker (dind) <p>defined in the .devcontainer directory and tagged with</p> <ul> <li>regular image: <code>ghcr.io/srl-labs/containerlab/devcontainer-dind:&lt;version&gt;</code></li> <li>slim image: <code>ghcr.io/srl-labs/containerlab/devcontainer-dind-slim:&lt;version&gt;</code></li> </ul> <p>where <code>&lt;version&gt;</code> is the containerlab version without the <code>v</code> prefix.</p> <p>Docker In Docker variant provides an isolated docker environment inside the devcontainer. This means, that the docker daemon inside the devcontainer is not connected to the docker daemon on your host, you will not see the containers/images that you have on your host.</p> <p>This version is best to be used with Codespaces.</p>  Docker Outside Of Docker (dood) <p>defined in the .devcontainer directory and tagged with</p> <ul> <li>regular image: <code>ghcr.io/srl-labs/containerlab/devcontainer-dood:&lt;version&gt;</code></li> <li>slim image: <code>ghcr.io/srl-labs/containerlab/devcontainer-dood-slim:&lt;version&gt;</code></li> </ul> <p>where <code>&lt;version&gt;</code> is the containerlab version without the <code>v</code> prefix.</p> <p>Docker Outside Of Docker variant uses the docker daemon on your host, so inside your devcontainer image you will see the images and containers that you have on your host. This variant is likely best to be on your local machine running macOS or Windows.</p> <p>When running in this mode, VS Code will ask you to provide a path to the workspace first time you open the devcontainer. You should select the path to the repository on your host in the dialog.</p> <p>The labs that we publish with Codespaces support often already have the <code>devcontainer.json</code> files, in that case you don't even need to create them manually.</p>"},{"location":"macos/#docker-in-docker-dind","title":"Docker In Docker (dind)","text":"<p>If you intend to run a docker-in-docker version of the devcontainer, create the <code>.devcontainer/docker-in-docker/devcontainer.json</code> file at the root of your repo with the following content:</p> <code>./devcontainer/docker-in-docker/devcontainer.json</code><pre><code>{\n    \"image\": \"ghcr.io/srl-labs/containerlab/devcontainer-dind-slim:0.60.0\" //(1)!\n}\n</code></pre> <ol> <li>devcontainer versions match containerlab versions</li> </ol> <p>With the devcontainer file in place, when you open a repo in VS Code, you will be prompted to reopen the workspace in the devcontainer. Or you can press F1 and select <code>Dev Containers: Rebuild and Reopen in Container</code>.</p> <p></p> <p>Clicking on this button will open the workspace in the devcontainer; you will see the usual VS Code window, but now the workspace will have containerlab installed with a separate docker instance running inside the spawned container. This means that your devcontainer works in isolation with the rest of your system.</p> <p>Open a terminal in the VS Code and run the topology by typing the familiar <code>sudo clab dep</code> command to deploy the lab. That's it!</p>"},{"location":"macos/#docker-outside-of-docker-dood","title":"Docker Outside Of Docker (dood)","text":"<p>The docker-in-docker method of running a devcontainer is great for Codespaces, but when running on your local machine you might want to use the images that your have already pulled in your host's docker or see the containers that might be running on your host. In these cases, the isolation that docker-in-docker provides stands in the way, and it also have some performance implications.</p> <p>That's why we also have the docker-outside-of-docker (dood) variant of the devcontainer. To use this variant create the <code>.devcontainer/docker-outside-of-docker/devcontainer.json</code> file will have more meat in it, since we need to mount some host directories for containerlab to be able to do its magic:</p> <code>./devcontainer/docker-outside-of-docker/devcontainer.json</code><pre><code>{\n    \"image\": \"ghcr.io/srl-labs/containerlab/devcontainer-dood-slim:0.60.0\", //(1)!\n    \"runArgs\": [\n        \"--network=host\",\n        \"--pid=host\",\n        \"--privileged\"\n    ],\n    \"mounts\": [\n        \"type=bind,src=/run/docker/netns,dst=/run/docker/netns\",\n        \"type=bind,src=/var/lib/docker,dst=/var/lib/docker\",\n        \"type=bind,src=/lib/modules,dst=/lib/modules\"\n    ],\n    \"workspaceFolder\": \"${localWorkspaceFolder}\",\n    \"workspaceMount\": \"source=${localWorkspaceFolder},target=${localWorkspaceFolder},type=bind\"\n}\n</code></pre> <ol> <li>The devcontainer version matches the containerlab version. The rest of the file does not change and can be copy pasted.</li> </ol> <p>You can have both docker-in-docker and docker-outside-of-docker variants of the devcontainer file in your repo, and your IDE will be able to switch between them.</p>"},{"location":"macos/#devpod","title":"DevPod","text":"<p>DevPod is an open-source project by loft.sh that makes using devcontainers easier and more portable.</p> <p>When compared to Devcontainers-way explained in the previous section, DevPod has the following advantages:</p> <ul> <li>improved User Experience by offering launching workspaces directly from the browser</li> <li>support for multiple IDEs and multiple target providers (locally with docker, or any cloud, or even on top of K8s)</li> </ul> <p>A short demo is worth a thousand words:</p> <p>We are still polishing the DevPod integration, especially the integration with WSL. Let us know if you have any questions or suggestions.</p> <ol> <li> <p>Or any other application that enables Docker on macOS. OrbStack is just a great choice that is used by many.\u00a0\u21a9</p> </li> <li> <p>With Microsoft Surface laptop released with ARM64 architecture\u00a0\u21a9</p> </li> <li> <p>The same principles apply to Docker Desktop on Windows\u00a0\u21a9</p> </li> <li> <p>Follows the devcontainer specification \u21a9</p> </li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<p>Getting containerlab is as easy as it gets. Thanks to the quick setup script you can get up and running in a matter of seconds on any RHEL or Debian based OS<sup>1</sup>.</p> <pre><code>curl -sL https://containerlab.dev/setup | sudo -E bash -s \"all\"\n</code></pre> <p>By default, this will also configure sshd on the system to increase max auth tries so unknown keys don't lock ssh attempts. This behaviour can be turned off by setting the environment variable \"SETUP_SSHD\" to \"false\" before running the command shown above. The environment variable can be set and exported with the command shown below.</p> <pre><code>export SETUP_SSHD=\"false\"\n</code></pre> <p>To complete installation and enable sudo-less <code>docker</code> command execution, please run <code>newgrp docker</code> or logout and log back in.</p>"},{"location":"quickstart/#topology-definition-file","title":"Topology definition file","text":"<p>Once installed, containerlab manages the labs defined in the so-called topology definition, <code>clab</code> files. A user can write a topology definition file from scratch, or look at various lab examples we provided within the containerlab package or explore dozens of labs our community has shared.</p> <p>In this quickstart we will be using one of the provided labs which consists of Nokia SR Linux and Arista cEOS nodes connected one to another.</p> <p>The lab topology is defined in the srlceos01.clab.yml file. To make use of this lab example, we need to fetch the clab file:</p> <pre><code>mkdir ~/clab-quickstart #(1)!\ncd ~/clab-quickstart\n\ncurl -LO \\\nhttps://raw.githubusercontent.com/srl-labs/containerlab/main/lab-examples/srlceos01/srlceos01.clab.yml #(2)!\n</code></pre> <ol> <li>Create a directory to store the lab definition file.</li> <li>Download the lab definition file.</li> </ol> <p>Let's have a look at how this lab's topology is defined:</p> <pre><code># topology documentation: http://containerlab.dev/lab-examples/srl-ceos/\nname: srlceos01\n\ntopology:\n  nodes:\n    srl:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux:24.3.3\n    ceos:\n      kind: arista_ceos\n      image: ceos:4.32.0F\n\n  links:\n    - endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n</code></pre> <p>A topology definition deep-dive document provides a complete reference of the topology definition syntax. In this quickstart we keep it short, glancing over the key components of the file:</p> <ul> <li>Each lab has a <code>name</code>.</li> <li>The lab topology is defined under the <code>topology</code> element.</li> <li>Topology is a set of <code>nodes</code> and <code>links</code> between them.</li> <li>The nodes are always of a certain <code>kind</code>. The <code>kind</code> defines the node configuration and behavior.</li> <li>Containerlab supports a fixed number of <code>kinds</code>. In the example above, the <code>srl</code> and <code>ceos</code> are one of the supported kinds.</li> <li>The actual nodes of the topology are defined in the <code>nodes</code> section which holds a map of node names. In the example above, nodes with names <code>srl</code> and <code>ceos</code> are defined.</li> <li>Node elements must have a <code>kind</code> parameter to indicate which kind this node belongs to. Under the nodes section you typically provide node-specific parameters. This lab uses a node-specific parameters - <code>image</code>.  </li> <li><code>nodes</code> are interconnected with <code>links</code>. Each <code>link</code> is defined by a set of <code>endpoints</code>.</li> </ul>"},{"location":"quickstart/#container-image","title":"Container image","text":"<p>One of node's most important properties is the container <code>image</code> that the nodes are defined with. The image name follows the same rules as the images you use with, for example, Docker CLI or Docker Compose.</p> Image name formats and fully qualified names <p>There are several forms of how one can write an image name, but each name essentially maps to a fully qualified image name that consists of:</p> <ol> <li>registry - the registry where the image is stored.</li> <li>organisation - the organisation name in that registry</li> <li>repository - the repository name</li> <li>tag - the image tag</li> </ol> <p>Below you will find different image names you can come across when working with containerlab and the corresponding FQDN names they map to:</p> <ol> <li><code>ghcr.io/nokia/srlinux</code><ul> <li>registry: <code>ghcr.io</code></li> <li>organisation: <code>nokia</code></li> <li>repository: <code>srlinux</code></li> <li>tag: when no explicit tag is set, the implicit <code>latest</code> is used</li> </ul> </li> <li><code>ceos:4.32.0F</code><ul> <li>registry: when registry is not set, implicit <code>docker.io</code> registry is assumed</li> <li>organisation: when none is set, implicit <code>library</code> organisation is assumed</li> <li>repository: <code>ceos</code></li> <li>tag: <code>4.32.0F</code></li> </ul> </li> <li><code>prom/prometheus:v2.47</code><ul> <li>registry: when registry is not set, implicit <code>docker.io</code> registry is assumed</li> <li>organisation: <code>prom</code></li> <li>repository: <code>prometheus</code></li> <li>tag: <code>v2.47</code></li> </ul> </li> </ol> <p>Our topology file defines two nodes, where each node is defined with its own container image:</p> <ul> <li><code>ghcr.io/nokia/srlinux:24.3.3</code> - for Nokia SR Linux node</li> <li><code>ceos:4.32.0F</code> - for Arista cEOS node</li> </ul> <p>When containerlab starts to deploy the lab, it will first check if the images are available locally. Local images can be listed with <code>docker images</code> command.</p> <p>Containerlab will compare the image names from the topology file with the local images and if the images are not available locally, it will try to pull them from the remote registry.</p> <p>Images availability</p> <p>Quickstart lab includes Nokia SR Linux and Arista cEOS images. While Nokia SR Linux is a publicly available image and can be pulled by anyone, its counterpart, Arista cEOS image, is not available in a public registry.</p> <p>Arista requires its users to register with arista.com before downloading any images. Once you created an account and logged in, go to the software downloads section and download ceos64 tar archive for a given release.</p> <p>Once downloaded, import the archive with docker:</p> <pre><code># import container image and save it under ceos:4.32.0F name\ndocker import cEOS64-lab-4.32.0F.tar.xz ceos:4.32.0F\n</code></pre>"},{"location":"quickstart/#deploying-a-lab","title":"Deploying a lab","text":"<p>Now when we know what a basic topology file consists of, refreshed our knowledge on what container image name is and imported cEOS image, we can proceed with deploying this lab. To keep things easy and guessable, the command to deploy a lab is called <code>deploy</code>.</p> <p>Doesn't hurt to verify that we have cEOS image imported before we hit the deploy command:</p> <pre><code>docker images | grep ceos\nREPOSITORY                        TAG        IMAGE ID       CREATED         SIZE\nceos                              4.32.0F    40d39e1a92c2   24 hours ago    2.4GB\n</code></pre> <p>Remote topology files</p> <p>Containerlab allows to deploy labs from files located in remote Git repositories and/or HTTP URLs. Check out deploy command documentation for more details.</p> <p>While you can pre-pull the Nokia SR Linux image, containerlab will do it for you if it's not available locally, handy! So we are ready to deploy:</p> <pre><code>sudo containerlab deploy # (1)!\n</code></pre> <ol> <li><code>deploy</code> command will automatically lookup a file matching the <code>*.clab.y*ml</code> patter to select it.   If you have several files and want to pick a specific one, use <code>--topo &lt;path&gt;</code> flag.</li> </ol> <p>In no time you will see the summary table with the deployed lab nodes. The table will show the node name (which equals to container name), node kind, image name and a bunch of other useful information. You can always list the nodes of the lab with <code>containerlab inspect</code> command.</p> <pre><code>+---+---------------------+--------------+-----------------------+---------------+---------+-----------------+----------------------+\n| # |        Name         | Container ID |         Image         |     Kind      |  State  |  IPv4 Address   |     IPv6 Address     |\n+---+---------------------+--------------+-----------------------+---------------+---------+-----------------+----------------------+\n| 1 | clab-srlceos01-ceos | 6ec1b1367a77 | ceos:4.32.0F          | arista_ceos   | running | 172.20.20.11/24 | 3fff:172:20:20::b/64 |\n| 2 | clab-srlceos01-srl  | 6af1e33f4573 | ghcr.io/nokia/srlinux | nokia_srlinux | running | 172.20.20.10/24 | 3fff:172:20:20::a/64 |\n+---+---------------------+--------------+-----------------------+---------------+---------+-----------------+----------------------+\n</code></pre>"},{"location":"quickstart/#connecting-to-the-nodes","title":"Connecting to the nodes","text":"<p>We know you want to get your hands dirty with the nodes, so let's connect to them. The common way netengs use to interact with network devices is via CLI. With Network OSes you can use SSH to connect to the CLI by either using the management address assigned to the container or a node name:</p> <pre><code>\u276f ssh admin@clab-srlceos01-srl\n\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl#\n</code></pre> <p>Note</p> <p>For each supported kind we document the management interfaces and the ways to leverage them. For example, <code>srl</code> kind documentation provides the commands to leverage SSH and gNMI interfaces. <code>ceos</code> kind has its own instructions.</p> <p>The following tab view aggregates the ways to get CLI access per the lab node:</p> <p>Since the topology nodes are regular containers, you can connect to them just like to any other container.</p> Nokia SR LinuxArista cEOS <pre><code># access CLI\ndocker exec -it clab-srlceos01-srl sr_cli\n# access bash\ndocker exec -it clab-srlceos01-srl bash\n</code></pre> <pre><code># access CLI\ndocker exec -it clab-srlceos01-ceos Cli\n# access bash\ndocker exec -it clab-srlceos01-ceos bash\n</code></pre> <p>Feel free to explore the nodes, configure them, and run your favorite network protocols. If you break something, you can always destroy the lab and start over. Speaking of which...</p>"},{"location":"quickstart/#destroying-a-lab","title":"Destroying a lab","text":"<p>To remove the lab, use the <code>destroy</code> command that takes a topology file as an argument:</p> <pre><code>sudo containerlab destroy\n</code></pre>"},{"location":"quickstart/#what-next","title":"What next?","text":"<p>To get a broader view on the containerlab features and components, refer to the User manual section.</p> <p>Do not forget to check out the Lab examples section where we provide complete and ready-to-run topology definition files. This is a great starting point to explore containerlab by doing.</p> <ol> <li> <p>For other installation options such as package managers, manual binary downloads or instructions to get containerlab for non-RHEL/Debian distros, refer to the installation guide.\u00a0\u21a9</p> </li> </ol>"},{"location":"windows/","title":"Containerlab on Windows","text":"<p>By leveraging the Windows Subsystem Linux (aka WSL) you can run Containerlabs on Windows almost like on Linux. WSL allows Windows users to run a lightweight Linux VM inside Windows, and we can leverage this to run containerlab.</p> <p>There are two primary ways of running containerlab on Windows:</p> <ol> <li>Running containerlab directly in the WSL VM.</li> <li>Running containerlab inside a Devcontainer inside WSL.</li> </ol> <p>We will cover both of these ways in this document, but first let's quickly go over the WSL setup.</p>"},{"location":"windows/#setting-up-wsl","title":"Setting up WSL","text":"<p>WSL-Containerlab, WHAT?</p> <p>Hey, before you dive into the WSL details, you might want check out a project that the relentless team of @kaelemc, @FloSch62, and @hyposcaler-bot worked on over 900 Discord messages.</p> <p>It is coined as WSL-Containerlab and can be the best Containerlab-on-WSL experience if you can install WSL 2.4.4+ version. Read more at project's README.</p> <p>WSL takes the central role in running containerlabs on Windows. Luckily, setting up WSL is very easy, and there are plenty of resources online from blogs to YT videos explaining the bits and pieces. Here we will provide some CLI-centric<sup>1</sup> instructions that were executed on Windows 11.</p> <p>Windows and WSL version</p> <p>The following instructions were tested on Windows 11 and WSL2. On Windows 10 some commands may be different, but the general idea should be the same.</p> <p>First things first, open up a terminal and list the running WSL virtual machines and their versions:</p> <pre><code>wsl -l -v\n</code></pre> <pre><code>  NAME      STATE           VERSION\n* Ubuntu    Running         2\n</code></pre> <p>On this system we already have a WSL VM with Ubuntu OS running, which was created when we installed WSL on Windows. If instead of a list of WSL VMs you get an error, you need to install WSL first:</p> Installing WSL on Windows 11<pre><code>wsl --install -d Debian #(1)!\n</code></pre> <ol> <li>Installing a new WSL system will prompt you to choose a username and password.</li> </ol> <p>If you performed a default WSL installation before, you are likely running an Ubuntu, and while it is perfectly fine to use it, we prefer Debian, so let's remove Ubuntu and install Debian instead:</p> Removing Ubuntu and installing Debian<pre><code>wsl --unregister Ubuntu #(1)!\nwsl --install -d Debian\n</code></pre> <ol> <li>Unregistering a WSL VM will remove the VM. You should reference a WSL instance by the name you saw in the <code>wsl -l -v</code> command.</li> </ol> <p>Once the installation is complete, you will enter the WSL shell, which is a regular Linux shell<sup>2</sup>.</p> <p>It is recommended to reboot your Windows system after installing the WSL. When the reboot is done you have a working WSL system that can run containerlab, congratulations!</p>"},{"location":"windows/#installing-docker-and-containerlab-on-wsl","title":"Installing docker and containerlab on WSL","text":"<p>Now that we have a working WSL system, we can install docker and containerlab on it like we would on any Linux system.</p> <p>WSL2 and Docker Desktop</p> <p>If you have Docker Desktop<sup>3</sup> installed on your Windows system, you need to ensure that it is not enabled for the WSL VM that we intend to use for containerlabs. To check that your WSL system is \"free\" from Docker Desktop integration, run <code>sudo docker version</code> command and ensure that you have an error message saying that <code>docker</code> command is not found.</p> <p>Check Docker Desktop settings to see how to disable Docker Desktop integration with WSL2 if the above command does not return an expected error.</p> <p>We are going to use the quick setup script to install docker and containerlab, but since this script uses <code>curl</code>, we need to install it first:</p> <pre><code>sudo apt update &amp;&amp; sudo apt -y install curl\n</code></pre> <p>and then run the quick setup script:</p> <pre><code>curl -sL https://containerlab.dev/setup | sudo -E bash -s \"all\"\n</code></pre> <p>By default, this will also configure sshd on the system to increase max auth tries so unknown keys don't lock ssh attempts. This behaviour can be turned off by setting the environment variable \"SETUP_SSHD\" to \"false\" before running the command shown above. The environment variable can be set and exported with the command shown below.</p> <pre><code>export SETUP_SSHD=\"false\"\n</code></pre> <p>To complete installation and enable sudo-less <code>docker</code> command execution, please run <code>newgrp docker</code> or logout and log back in.</p> <p>Now you should be able to run the <code>docker version</code> command and see the version of docker installed. That was easy, wasn't it?</p> <p>The installation script also installs containerlab, so you can run <code>clab version</code> to see the version of containerlab installed. This means that containerlab is installed in the WSL VM and you can run containerlabs in a normal way, like you would on Linux.</p> Running VM-based routers inside WSL? <p>In Windows 11 with WSL2 it is now possible to enable KVM support. Let us know if that worked for you in our Discord.</p>"},{"location":"windows/#devcontainer","title":"Devcontainer","text":"<p>Another convenient option to run containerlab on Windows (and macOS) is to use the Devcontainer feature that works great with VS Code and many other IDE's.</p> <p>A development container (or devcontainer) allows you to use a container as a full-featured development environment. By creating the <code>devcontainer.json</code><sup>4</sup> file, you define the development environment for your project. Containerlab project maintains a set of pre-built multi-arch devcontainer images that you can use to run containerlabs. It was initially created to power containerlab in codespaces, but it is a perfect fit for running containerlab on a wide range of OSes such as macOS and Windows.</p> <p>Since the devcontainer works exactly the same way on Windows and macOS, please refer to the macOS section for the detailed documentation and a video walkthrough.</p> <p>A few things to keep in mind when using devcontainers on windows:</p> <ol> <li> <p>If using VS Code, you will need to install the server component in your WSL instance, this will require you to install <code>wget</code>, as VS Code installer requires it.</p> <pre><code>sudo apt -y install wget\n</code></pre> <p>Then you will be able to type <code>code .</code> in the cloned repository to open the project in VS Code.</p> </li> <li> <p>As with macOS, you will likely wish to use a Docker-outside-of-Docker method, where the devcontainer will have access to the images and containers from the WSL VM.</p> </li> </ol>"},{"location":"windows/#devpod","title":"DevPod","text":"<p>DevPod delivers a stellar User Experience on macOS<sup>5</sup>, but on Windows, it requires a bit more setup. We tried to make it as easy as possible by providing you with the WSL distribution with the necessary tools pre-installed.</p> <p>When you install Containerlab WSL VM and DevPod, you will have to create the SSH provider in DevPod with the following settings:</p> <p></p> <p>And that's it! You should now be able to use DevPod to run containerlabs on Windows.</p> <ol> <li> <p>If you don't have a decent terminal emulator on Windows, install \"Windows Terminal\" from the Microsoft Store.\u00a0\u21a9</p> </li> <li> <p>The kernel and distribution parameters can be checked as follows:</p> <pre><code>roman@Win11:~$ uname -a\nLinux LAPTOP-H6R3238F 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 GNU/Linux\n</code></pre> <p><pre><code>roman@Win11:~$ cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\n</code></pre> \u21a9</p> </li> <li> <p>Or any other desktop docker solution like Rancher Desktop, Podman Desktop, etc.\u00a0\u21a9</p> </li> <li> <p>Follows the devcontainer specification \u21a9</p> </li> <li> <p>Almost a one-click solution \u21a9</p> </li> </ol>"},{"location":"cmd/completion/","title":"shell completions","text":""},{"location":"cmd/completion/#description","title":"Description","text":"<p>The <code>completion</code> command generates shell completions for bash/zsh/fish shells.</p>"},{"location":"cmd/completion/#usage","title":"Usage","text":"<p><code>containerlab completion [arg]</code></p>"},{"location":"cmd/completion/#bash-completions","title":"Bash completions","text":"<p>Ensure that bash-completion is installed on your system.</p> <p>To load completions for the current session:</p> <pre><code>source &lt;(containerlab completion bash)\n</code></pre> <p>To load completions for each session:</p> LinuxmacOS <pre><code>containerlab completion bash &gt; /etc/bash_completion.d/containerlab\n</code></pre> <pre><code>containerlab completion bash &gt; /usr/local/etc/bash_completion.d/containerlab\n</code></pre> <p>To also autocomplete for <code>clab</code> command alias, add the following to your <code>.bashrc</code> or <code>.bash_profile</code>:</p> <pre><code>complete -o default -F __start_containerlab clab\n</code></pre>"},{"location":"cmd/completion/#zsh-completions","title":"ZSH completions","text":"<p>If shell completion is not already enabled in your environment you have to enable it by ensuring zsh completions are loaded. The following can be added to your zshrc:</p> <pre><code>autoload -U compinit; compinit\n</code></pre> <p>To load completions for each session generate the completion script and store it somewhere in your <code>$fpath</code>:</p> <pre><code>clab completion zsh | \\\nsed '1,2c\\#compdef containerlab clab\\ncompdef _containerlab containerlab clab' &gt; \\\n~/.oh-my-zsh/custom/completions/_containerlab\n</code></pre> <p>Completion script location</p> <p><code>echo $fpath</code> will show the directories zsh reads files from. You can either use one of the available completions directories from this list or add a new directory to the list by adding this in your .zshrc file:</p> <pre><code>fpath=(~/.oh-my-zsh/custom/completions $fpath)\n</code></pre> <p>And then using <code>~/.oh-my-zsh/custom/completions</code> for your completions.</p> <p>Start a new shell for this setup to take effect.</p>"},{"location":"cmd/completion/#fish-completions","title":"Fish completions","text":"<pre><code>containerlab completion fish | source\n</code></pre> <p>To load completions for each session, execute once:</p> <pre><code>containerlab completion fish &gt; ~/.config/fish/completions/containerlab.fish\n</code></pre>"},{"location":"cmd/deploy/","title":"deploy command","text":""},{"location":"cmd/deploy/#description","title":"Description","text":"<p>The <code>deploy</code> command spins up a lab using the topology expressed via topology definition file.</p>"},{"location":"cmd/deploy/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] deploy [local-flags]</code></p> <p>aliases: <code>dep</code></p>"},{"location":"cmd/deploy/#flags","title":"Flags","text":""},{"location":"cmd/deploy/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used to spin up a lab.</p> <p>When the topology path refers to a directory, containerlab will look for a file with <code>.clab.yml</code> or <code>.clab.yaml</code> extension in that directory and use it as a topology definition file.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory.</p> <p>If more than one file is found for directory-based path or when the flag is omitted entirely, containerlab will fail with an error.</p> <p>It is possible to read the topology file from stdin by passing <code>-</code> as a value to the <code>--topo</code> flag. See examples for more details.</p>"},{"location":"cmd/deploy/#remote-topology-files","title":"Remote topology files","text":""},{"location":"cmd/deploy/#git","title":"Git","text":"<p>To simplify the deployment of labs that are stored in remote version control systems, containerlab supports the use of remote topology files for GitHub.com and GitLab.com hosted projects.</p> <p>By specifying a URL to a repository or a <code>.clab.yml</code> file in a repository, containerlab will automatically clone<sup>1</sup> the repository in your current directory and deploy it. If the URL points to a <code>.clab.yml</code> file, containerlab will clone the repository and deploy the lab defined in the file.</p> <p>The following URL formats are supported:</p> Type Example Which topology file is used Link to github repository https://github.com/hellt/clab-test-repo/ An auto-find procedure will find a <code>clab.yml</code> in the repository root and deploy it Link to a file in a github repository https://github.com/hellt/clab-test-repo/blob/main/lab1.clab.yml A file specified in the URL will be deployed Link to a repo's branch https://github.com/hellt/clab-test-repo/tree/branch1 A branch of a repo is cloned and auto-find procedure kicks in Link to a file in a branch of a repo https://github.com/hellt/clab-test-repo/blob/branch1/lab2.clab.yml A branch is cloned and a file specified in the URL is used for deployment Link to a file in a subdir of a repo https://github.com/hellt/clab-test-repo/blob/main/dir/lab3.clab.yml A file specified in the subdir of the branch will be deployed Shortcut of a github project hellt/clab-test-repo An auto-find procedure will find a <code>clab.yml</code> in the repository root and deploy it <p>When the lab is deployed using the URL, the repository is cloned in the current working directory. If the repository is already cloned it will be used and not cloned again; containerlab will try to fetch the latest changes from the remote repository.</p> <p>Subsequent lab operations (such as destroy) must use the filesystem path to the topology file and not the URL.</p> Remote labs workflow in action <p> </p>"},{"location":"cmd/deploy/#https","title":"HTTP(S)","text":"<p>Labs can be deployed from remote HTTP(S) URLs as well. These labs should be self-contained and not reference any external resources, like startup-config files, licenses, binds, etc.</p> <p>The following URL formats are supported:</p> Type Example Description Link to raw github gist https://gist.githubusercontent.com/hellt/abc/raw/def/linux.clab.yml A file is downloaded to a temp directory and launched Link to a short schemaless URL srlinux.dev/clab-srl A file is downloaded to a temp directory and launched <p>Containerlab distinct HTTP URLs from GitHub/GitLab by checking if github.com or gitlab.com is present in the URL. If not, it will treat the URL as a plain HTTP(S) URL.</p>"},{"location":"cmd/deploy/#name","title":"name","text":"<p>With the global <code>--name | -n</code> flag a user sets a lab name. This value will override the lab name value passed in the topology definition file.</p>"},{"location":"cmd/deploy/#vars","title":"vars","text":"<p>Global <code>--vars</code> option for using specified json or yaml file to load template variables from for generating topology file.</p> <p>Default is to lookup files with \"_vars\" suffix and common json/yaml file extensions next to topology file. For example, for <code>mylab.clab.gotmpl</code> template of topology definition file, variables from <code>mylab.clab_vars.yaml</code> file will be used by default, if it exists, or one with <code>.json</code> or <code>.yml</code> extension.</p> <p>See documentation on Generated topologies for more information and examples on how to use these variables.</p>"},{"location":"cmd/deploy/#reconfigure","title":"reconfigure","text":"<p>The local <code>--reconfigure | -c</code> flag instructs containerlab to first destroy the lab and all its directories and then start the deployment process. That will result in a clean (re)deployment where every configuration artefact will be generated (TLS, node config) from scratch.</p> <p>Without this flag present, containerlab will reuse the available configuration artifacts found in the lab directory.</p> <p>Refer to the configuration artifacts page to get more information on the lab directory contents.</p>"},{"location":"cmd/deploy/#max-workers","title":"max-workers","text":"<p>With <code>--max-workers</code> flag, it is possible to limit the number of concurrent workers that create containers or wire virtual links. By default, the number of workers equals the number of nodes/links to create.</p>"},{"location":"cmd/deploy/#runtime","title":"runtime","text":"<p>Containerlab nodes can be started by different runtimes, with <code>docker</code> being the default one. Besides that, containerlab has experimental support for <code>podman</code>, and <code>ignite</code> runtimes.</p> <p>A global runtime can be selected with a global <code>--runtime | -r</code> flag that will select a runtime to use. The possible value are:</p> <ul> <li><code>docker</code> - default</li> <li><code>podman</code> - experimental support</li> <li><code>ignite</code></li> </ul>"},{"location":"cmd/deploy/#timeout","title":"timeout","text":"<p>A global <code>--timeout</code> flag drives the timeout of API requests that containerlab send toward external resources. Currently the only external resource is the container runtime (i.e. docker).</p> <p>In a busy compute the runtime may respond longer than anticipated, in that case increasing the timeout may help.</p> <p>The default timeout is set to 2 minutes and can be changed to values like <code>30s, 10m</code>.</p>"},{"location":"cmd/deploy/#export-template","title":"export-template","text":"<p>The local <code>--export-template</code> flag allows a user to specify a custom Go template that will be used for exporting topology data into <code>topology-data.json</code> file under the lab directory. If not set, the default template is used.</p> <p>To export the full topology data instead of a subset of fields exported by default, use <code>--export-template __full</code> which is a special value that instructs containerlab to use the full.tmpl template file. Note, some fields exported via <code>full.tmpl</code> might contain sensitive information like TLS private keys. To customize export data, it is recommended to start with a copy of <code>auto.tmpl</code> and change it according to your needs.</p>"},{"location":"cmd/deploy/#log-level","title":"log-level","text":"<p>Global <code>--log-level</code> parameter can be used to configure logging verbosity of all containerlab operations. <code>--debug | -d</code> option is a shorthand for <code>--log-level debug</code> and takes priority over <code>--log-level</code> if specified.</p> <p>Following values are accepted, ordered from most verbose to least: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>fatal</code>. Default level is <code>info</code>.</p> <p>It should be useful to enable more verbose logging when something doesn't work as expected, to better understand what's going on, and to provide more useful output logs when reporting containerlab issues, while making it more terse in production environments.</p>"},{"location":"cmd/deploy/#node-filter","title":"node-filter","text":"<p>The local <code>--node-filter</code> flag allows users to specify a subset of topology nodes targeted by <code>deploy</code> command. The value of this flag is a comma-separated list of node names as they appear in the topology.</p> <p>When a subset of nodes is specified, containerlab will only deploy those nodes and links belonging to all selected nodes and ignore the rest. This can be useful e.g. in CI/CD test case scenarios, where resource constraints may prohibit the deployment of a full topology.</p> <p>Read more about node filtering in the documentation.</p>"},{"location":"cmd/deploy/#skip-post-deploy","title":"skip-post-deploy","text":"<p>The <code>--skip-post-deploy</code> flag can be used to skip the post-deploy phase of the lab deployment. This is a global flag that affects all nodes in the lab.</p>"},{"location":"cmd/deploy/#skip-labdir-acl","title":"skip-labdir-acl","text":"<p>The <code>--skip-labdir-acl</code> flag can be used to skip the lab directory access control list (ACL) provisioning.</p> <p>The extended File ACLs are provisioned for the lab directory by default, unless this flag is set. Extended File ACLs allow a sudo user to access the files in the lab directory that might be created by the <code>root</code> user from within the container node.</p> <p>While this is useful in most cases, sometimes extended File ACLs might prevent your lab from working, especially when your lab directory end up being mounted from the network filesystem (NFS, CIFS, etc.). In such cases, you can use this flag to skip the ACL provisioning.</p>"},{"location":"cmd/deploy/#environment-variables","title":"Environment variables","text":""},{"location":"cmd/deploy/#clab_runtime","title":"<code>CLAB_RUNTIME</code>","text":"<p>Default value of \"runtime\" key for nodes, same as global <code>--runtime | -r</code> flag described above. Affects all containerlab commands in the same way, not just <code>deploy</code>.</p> <p>Intended to be set in environments where non-default container runtime should be used, to avoid needing to specify it for every command invocation or in every configuration file.</p> <p>Example command-line usage: <code>CLAB_RUNTIME=podman containerlab deploy</code></p>"},{"location":"cmd/deploy/#clab_version_check","title":"<code>CLAB_VERSION_CHECK</code>","text":"<p>Can be set to \"disable\" value to prevent deploy command making a network request to check new version to report if one is available.</p> <p>Useful when running in an automated environments with restricted network access.</p> <p>Example command-line usage: <code>CLAB_VERSION_CHECK=disable containerlab deploy</code></p>"},{"location":"cmd/deploy/#clab_labdir_base","title":"<code>CLAB_LABDIR_BASE</code>","text":"<p>To change the lab directory location, set <code>CLAB_LABDIR_BASE</code> environment variable accordingly. It denotes the base directory in which the lab directory will be created.</p> <p>The default behavior is to create the lab directory in the same directory as the topology file (<code>clab.yml</code> file).</p>"},{"location":"cmd/deploy/#examples","title":"Examples","text":""},{"location":"cmd/deploy/#deploy-a-lab-using-the-given-topology-file","title":"Deploy a lab using the given topology file","text":"<pre><code>containerlab deploy -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/deploy/#deploy-a-lab-and-regenerate-all-configuration-artifacts","title":"Deploy a lab and regenerate all configuration artifacts","text":"<pre><code>containerlab deploy -t mylab.clab.yml --reconfigure\n</code></pre>"},{"location":"cmd/deploy/#deploy-a-lab-without-specifying-topology-file","title":"Deploy a lab without specifying topology file","text":"<p>Given that a single topology file is present in the current directory.</p> <pre><code>containerlab deploy\n</code></pre>"},{"location":"cmd/deploy/#deploy-a-lab-using-short-flag-names","title":"Deploy a lab using short flag names","text":"<pre><code>clab dep -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/deploy/#deploy-a-lab-from-a-remote-url-with-curl","title":"Deploy a lab from a remote URL with curl","text":"<pre><code>curl -s https://gist.githubusercontent.com/hellt/9baa28d7e3cb8290ade1e1be38a8d12b/raw/03067e242d44c9bbe38afa81131e46bab1fa0c42/test.clab.yml | \\\n    sudo containerlab deploy -t -\n</code></pre> <ol> <li> <p>The repository is cloned with <code>--depth 1</code> parameter.\u00a0\u21a9</p> </li> </ol>"},{"location":"cmd/destroy/","title":"destroy command","text":""},{"location":"cmd/destroy/#description","title":"Description","text":"<p>The <code>destroy</code> command destroys a lab referenced by its topology definition file.</p>"},{"location":"cmd/destroy/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] destroy [local-flags]</code></p> <p>aliases: <code>des</code></p>"},{"location":"cmd/destroy/#flags","title":"Flags","text":""},{"location":"cmd/destroy/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used to identify the lab to destroy.</p> <p>When the topology path refers to a directory, containerlab will look for a file with <code>.clab.yml</code> extension in that directory and use it as a topology definition file.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory.</p> <p>If more than one file is found for directory-based path or when the flag is omitted entirely, containerlab will fail with an error.</p>"},{"location":"cmd/destroy/#cleanup","title":"cleanup","text":"<p>The local <code>--cleanup | -c</code> flag instructs containerlab to remove the lab directory and all its content.</p> <p>Without this flag present, containerlab will keep the lab directory and all files inside of it.</p> <p>Refer to the configuration artifacts page to get more information on the lab directory contents.</p>"},{"location":"cmd/destroy/#graceful","title":"graceful","text":"<p>To make containerlab attempt a graceful shutdown of the running containers, add the <code>--graceful</code> flag to destroy cmd. Without it, containers will be removed forcefully without even attempting to stop them.</p>"},{"location":"cmd/destroy/#keep-mgmt-net","title":"keep-mgmt-net","text":"<p>Do not try to remove the management network. Usually the management docker network (in case of docker) and the underlying bridge are being removed. If you have attached additional resources outside of containerlab and you want the bridge to remain intact just add the <code>--keep-mgmt-net</code> flag.</p>"},{"location":"cmd/destroy/#all","title":"all","text":"<p>Destroy command provided with <code>--all | -a</code> flag will perform the deletion of all the labs running on the container host. It will not touch containers launched manually.</p>"},{"location":"cmd/destroy/#node-filter","title":"node-filter","text":"<p>The local <code>--node-filter</code> flag allows users to specify a subset of topology nodes targeted by <code>destroy</code> command. The value of this flag is a comma-separated list of node names as they appear in the topology.</p> <p>When a subset of nodes is specified, containerlab will only destroy those nodes and their links and leave the rest of the topology intact. As such, users can destroy a subset of nodes and links in a lab without destroying the entire topology.</p> <p>Read more about node filtering in the documentation.</p>"},{"location":"cmd/destroy/#examples","title":"Examples","text":""},{"location":"cmd/destroy/#destroy-a-lab-described-in-the-given-topology-file","title":"Destroy a lab described in the given topology file","text":"<pre><code>containerlab destroy -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/destroy/#destroy-a-lab-and-remove-the-lab-directory","title":"Destroy a lab and remove the Lab directory","text":"<pre><code>containerlab destroy -t mylab.clab.yml --cleanup\n</code></pre>"},{"location":"cmd/destroy/#destroy-a-lab-without-specifying-topology-file","title":"Destroy a lab without specifying topology file","text":"<p>Given that a single topology file is present in the current directory.</p> <pre><code>containerlab destroy\n</code></pre>"},{"location":"cmd/destroy/#destroy-all-labs-on-the-container-host","title":"Destroy all labs on the container host","text":"<pre><code>containerlab destroy -a\n</code></pre>"},{"location":"cmd/destroy/#destroy-a-lab-using-short-flag-names","title":"Destroy a lab using short flag names","text":"<pre><code>clab des\n</code></pre>"},{"location":"cmd/exec/","title":"exec command","text":""},{"location":"cmd/exec/#description","title":"Description","text":"<p>The <code>exec</code> command allows to execute a command inside the nodes (containers).</p> <p>This command is similar to <code>docker exec</code>, but it allows to run the same command across multiple lab nodes using filters. Users can provide a path to the topology file and <code>--label</code> filter to narrow down the list of nodes to execute the command on.</p>"},{"location":"cmd/exec/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] exec [local-flags]</code></p>"},{"location":"cmd/exec/#flags","title":"Flags","text":""},{"location":"cmd/exec/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user can set a path to the topology file that will be used to filter the nodes targeted for execution of the command. The nodes can further be filtered with the <code>--label</code> flag.</p> <p>Note, that with the nodes of <code>ext-container</code> type, the topology must not be provided.</p>"},{"location":"cmd/exec/#cmd","title":"cmd","text":"<p>The command to be executed on the nodes is provided with <code>--cmd</code> flag. The command is provided as a string, thus it needs to be quoted to accommodate for spaces or special characters.</p>"},{"location":"cmd/exec/#format","title":"format","text":"<p>The <code>--format | -f</code> flag allows selecting between plain text format output or a json variant. Consult with the examples below to see the differences between these two formatting options.</p> <p>Defaults to <code>plain</code> output format.</p>"},{"location":"cmd/exec/#label","title":"label","text":"<p>Using <code>--label</code> it is possible to filter the nodes to execute the command on using labels attached to the nodes. The label is provided as a string in the form of <code>key=value</code>. The <code>key</code> is the label name, and the <code>value</code> is the label value.</p> <p>Exec command should either be provided with a topology file, or labels, or both.</p> <p>Recall that you can check the labels attached to the nodes with <code>docker inspect -f '{{.Config.Labels | json}}' &lt;container-name&gt;</code> command.</p>"},{"location":"cmd/exec/#examples","title":"Examples","text":""},{"location":"cmd/exec/#execute-a-command-on-all-nodes-of-the-lab","title":"Execute a command on all nodes of the lab","text":"<p>Show ipv4 information from all the nodes of the lab defined in <code>srl02.clab.yml</code> with a plain text output</p> <pre><code>\u276f containerlab exec -t srl02.clab.yml --cmd 'ip -4 a show dummy-mgmt0'\nINFO[0000] clab-srl02-srl1: stdout:\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    inet 172.20.20.3/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever\nINFO[0000] clab-srl02-srl2: stdout:\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    inet 172.20.20.2/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"cmd/exec/#execute-a-command-on-a-node-referenced-by-its-name","title":"Execute a command on a node referenced by its name","text":"<p>Show ipv4 information from a specific node of the lab with a plain text output</p> <pre><code>\u276f containerlab exec -t srl02.clab.yml --label clab-node-name=srl2 --cmd 'ip -4 a show dummy-mgmt0'\nINFO[0000] Parsing &amp; checking topology file: srl02.yml  \nINFO[0000] Executed command 'ip -4 a show dummy-mgmt0' on clab-srl02-srl2. stdout:\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    inet 172.20.20.5/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever \n</code></pre>"},{"location":"cmd/exec/#execute-a-command-on-multiple-nodes-referenced-by-a-filter-and-no-topology-file","title":"Execute a command on multiple nodes referenced by a filter and no topology file","text":"<p>Since containerlab injects default labels to the nodes, it is possible to leverage <code>clab-node-kind</code> label that is attached to all the nodes. This label contains the node kind (type) information. In this example we will execute a command on all the nodes of the lab that are of <code>nokia_srlinux</code> kind.</p> <pre><code>\u276f sudo clab exec --label clab-node-kind=nokia_srlinux --cmd \"ip -4 addr show dummy-mgmt0\"\nINFO[0000] Executed command \"ip -4 addr show dummy-mgmt0\" on the node \"greeter-srl\". stdout:\n2: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    inet 172.20.20.2/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever \nINFO[0000] Executed command \"ip -4 addr show dummy-mgmt0\" on the node \"srl\". stdout:\n2: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    inet 172.20.20.3/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever \n</code></pre>"},{"location":"cmd/exec/#execute-a-cli-command","title":"Execute a CLI Command","text":"<pre><code>\u276f containerlab exec -t srl02.yml --cmd 'sr_cli  \"show version\"'\nINFO[0001] clab-srl02-srl1: stdout:\n----------------------------------------------------\nHostname          : srl1\nChassis Type      : 7250 IXR-6\nPart Number       : Sim Part No.\nSerial Number     : Sim Serial No.\nSystem MAC Address: 02:00:6B:FF:00:00\nSoftware Version  : v20.6.3\nBuild Number      : 145-g93496a3f8c\nArchitecture      : x86_64\nLast Booted       : 2021-06-24T10:25:26.722Z\nTotal Memory      : 24052875 kB\nFree Memory       : 21911906 kB\n----------------------------------------------------\nINFO[0003] clab-srl02-srl2: stdout:\n----------------------------------------------------\nHostname          : srl2\nChassis Type      : 7250 IXR-6\nPart Number       : Sim Part No.\nSerial Number     : Sim Serial No.\nSystem MAC Address: 02:D8:A9:FF:00:00\nSoftware Version  : v20.6.3\nBuild Number      : 145-g93496a3f8c\nArchitecture      : x86_64\nLast Booted       : 2021-06-24T10:25:26.904Z\nTotal Memory      : 24052875 kB\nFree Memory       : 21911914 kB\n----------------------------------------------------\n</code></pre>"},{"location":"cmd/exec/#execute-a-command-with-json-formatted-output","title":"Execute a Command with json formatted output","text":"<pre><code>\u276f containerlab exec -t srl02.yml --cmd 'sr_cli  \"show version | as json\"' -f json | jq\n</code></pre> <pre><code>{\n  \"clab-srl02-srl1\": {\n    \"stderr\": \"\",\n    \"stdout\": {\n      \"basic system info\": {\n        \"Architecture\": \"x86_64\",\n        \"Build Number\": \"145-g93496a3f8c\",\n        \"Chassis Type\": \"7250 IXR-6\",\n        \"Free Memory\": \"21911367 kB\",\n        \"Hostname\": \"srl1\",\n        \"Last Booted\": \"2021-06-24T10:25:26.722Z\",\n        \"Part Number\": \"Sim Part No.\",\n        \"Serial Number\": \"Sim Serial No.\",\n        \"Software Version\": \"v20.6.3\",\n        \"System MAC Address\": \"02:00:6B:FF:00:00\",\n        \"Total Memory\": \"24052875 kB\"\n      }\n    }\n  },\n  \"clab-srl02-srl2\": {\n    \"stderr\": \"\",\n    \"stdout\": {\n      \"basic system info\": {\n        \"Architecture\": \"x86_64\",\n        \"Build Number\": \"145-g93496a3f8c\",\n        \"Chassis Type\": \"7250 IXR-6\",\n        \"Free Memory\": \"21911367 kB\",\n        \"Hostname\": \"srl2\",\n        \"Last Booted\": \"2021-06-24T10:25:26.904Z\",\n        \"Part Number\": \"Sim Part No.\",\n        \"Serial Number\": \"Sim Serial No.\",\n        \"Software Version\": \"v20.6.3\",\n        \"System MAC Address\": \"02:D8:A9:FF:00:00\",\n        \"Total Memory\": \"24052875 kB\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"cmd/generate/","title":"generate command","text":""},{"location":"cmd/generate/#description","title":"Description","text":"<p>The <code>generate</code> command generates the topology definition file based on the user input provided via CLI flags.</p> <p>With this command it is possible to generate definition file for a CLOS fabric by just providing the number of nodes on each tier. The generated topology can be saved in a file or immediately scheduled for deployment.</p> <p>It is assumed, that the interconnection between the tiers is done in a full-mesh fashion. Such as tier1 nodes are fully meshed with tier2, tier2 is meshed with tier3 and so on.</p>"},{"location":"cmd/generate/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] generate [local-flags]</code></p> <p>aliases: <code>gen</code></p>"},{"location":"cmd/generate/#flags","title":"Flags","text":""},{"location":"cmd/generate/#name","title":"name","text":"<p>With the global <code>--name | -n</code> flag a user sets the name of the lab that will be generated.</p>"},{"location":"cmd/generate/#nodes","title":"nodes","text":"<p>The user configures the CLOS fabric topology by using the <code>--nodes</code> flag. The flag value is a comma separated list of CLOS tiers where each tier is defined by the number of nodes, its kind and type. Multiple <code>--node</code> flags can be specified.</p> <p>For example, the following flag value will define a 2-tier CLOS fabric with tier1 (leafs) consists of 4x SR Linux containers of IXR-D3 type and the 2x Arista cEOS spines: <pre><code>4:srl:ixrd3,2:ceos\n</code></pre></p> <p>Note, that the default kind is <code>srl</code>, so you can omit the kind for SR Linux node. The same nodes value can be expressed like that: <code>4:ixrd3,2:ceos</code></p>"},{"location":"cmd/generate/#kind","title":"kind","text":"<p>With <code>--kind</code> flag it is possible to set the default kind that will be set for the nodes which do not have a kind specified in the <code>--nodes</code> flag.</p> <p>For example the following value will generate a 3-tier CLOS fabric of cEOS nodes:</p> <pre><code># cEOS fabric\ncontainerlab gen -n 3tier --kind ceos --nodes 4,2,1\n\n# since SR Linux kind is assumed by default\n# SRL fabric command is even shorter\ncontainerlab gen -n 3tier --nodes 4,2,1\n</code></pre>"},{"location":"cmd/generate/#image","title":"image","text":"<p>Use <code>--image</code> flag to specify the container image that should be used by a given kind.</p> <p>The value of this flag follows the <code>kind=image</code> pattern. For example, to set the container image <code>ceos:4.32.0F</code> for the <code>ceos</code> kind the flag will be: <code>--image ceos=ceos:4.32.0F</code>.</p> <p>To set images for multiple kinds repeat the flag: <code>--image srl=srlinux:latest --image ceos=ceos:4.32.0F</code> or use the comma separated form: <code>--image srl=srlinux:latest,ceos=ceos:latest</code></p> <p>If the kind information is not provided in the <code>image</code> flag, the kind value will be taken from the <code>--kind</code> flag.</p>"},{"location":"cmd/generate/#license","title":"license","text":"<p>With <code>--license</code> flag it is possible to set the license path that should be used by a given kind.</p> <p>The value of this flag follows the <code>kind=path</code> pattern. For example, to set the license path for the <code>srl</code> kind: <code>--license srl=/tmp/license.key</code>.</p> <p>To set license for multiple kinds repeat the flag: <code>--license &lt;kind1&gt;=/path1 --image &lt;kind2&gt;=/path2</code> or use the comma separated form: <code>--license &lt;kind1&gt;=/path1,&lt;kind2&gt;=/path2</code></p>"},{"location":"cmd/generate/#deploy","title":"deploy","text":"<p>When <code>--deploy</code> flag is present, the lab deployment process starts using the generated topology definition file.</p> <p>The generated definition file is first saved by the path set with <code>--file</code> or, if file path is not set, by the default path of <code>&lt;lab-name&gt;.clab.yml</code>. Then the equivalent of the <code>deploy -t &lt;file&gt; --reconfigure</code> command is executed.</p>"},{"location":"cmd/generate/#max-workers","title":"max-workers","text":"<p>With <code>--max-workers</code> flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create.</p> <p>If during the deployment of a large scaled lab you see errors about max number of opened files reached, limit the max workers with this flag.</p>"},{"location":"cmd/generate/#file","title":"file","text":"<p>With <code>--file</code> flag it's possible to save the generated topology definition in a file by a given path.</p>"},{"location":"cmd/generate/#node-prefix","title":"node-prefix","text":"<p>With <code>--node-prefix</code> flag a user sets the name prefix of every node in a lab.</p> <p>Nodes will be named by the following template: <code>&lt;node-prefix&gt;-&lt;tier&gt;-&lt;node-number&gt;</code>. So a node named <code>node1-3</code> means this is the third node in a first tier of a topology.</p> <p>Default prefix: <code>node</code>.</p>"},{"location":"cmd/generate/#group-prefix","title":"group-prefix","text":"<p>With <code>--group-prefix</code> it is possible to change the Group value of a node. Group information is used in the topology graph rendering.</p>"},{"location":"cmd/generate/#network","title":"network","text":"<p>With <code>--network</code> flag a user sets the name of the management network that will be created by container orchestration system such as docker.</p> <p>Default: <code>clab</code>.</p>"},{"location":"cmd/generate/#ipv4-subnet-ipv6-subnet","title":"ipv4-subnet | ipv6-subnet","text":"<p>With <code>--ipv4-subnet</code> and <code>ipv6-subnet</code> it's possible to change the address ranges of the management network. Nodes will receive IP addresses from these ranges if they are configured with DHCP.</p>"},{"location":"cmd/generate/#examples","title":"Examples","text":""},{"location":"cmd/generate/#generate-topology-for-a-3-tier-clos-network","title":"Generate topology for a 3-tier CLOS network","text":"<p>Generate and deploy a lab topology for 3-tier CLOS network with 8 leafs, 4 spines and 2 superspines. All using Nokia SR Linux nodes with license and image provided.</p> <p>Note</p> <p>The <code>srl</code> kind in the image and license flags can be omitted, as it is implied by default</p> <pre><code>containerlab generate --name 3tier --image srl=srlinux:latest \\\n                      --license srl=license.key \\\n                      --nodes 8,4,2 --deploy\n</code></pre>"},{"location":"cmd/graph/","title":"graph command","text":""},{"location":"cmd/graph/#description","title":"Description","text":"<p>The <code>graph</code> command generates a graphical representations of a topology.</p> <p>The following graphing options are available:</p> <ol> <li>an HTML page served by <code>containerlab</code> web-server based on a user-provided HTML template and static files.</li> <li>Drawio (diagrams.net) diagram</li> <li>Mermaid.js graph description file that can be rendered in Markdown</li> <li>a graph description file in dot format that can be rendered using Graphviz or viewed online.<sup>1</sup></li> </ol>"},{"location":"cmd/graph/#html","title":"HTML","text":"<p>The HTML-based graph representation is the default graphing option. The topology will be graphed and served online using the embedded web server.</p> <p>The default graph template is based on the NeXt UI framework<sup>2</sup>.</p> <p></p> <p>To render a topology using this default graph engine:</p> <pre><code>containerlab graph -t &lt;path/to/topo.clab.yml&gt;\n</code></pre>"},{"location":"cmd/graph/#layout-and-sorting","title":"Layout and sorting","text":"<p>Topology graph created with NeXt UI has some control elements that allow you to choose the color theme of the web view, scaling and panning. Besides these generic controls it is possible to enable auto-layout of the components using buttons at the top of the screen.</p> <p>The graph engine can automatically pan and sort elements in your topology based on their role. We encode the role via <code>group</code> property of a node.</p> <p>Today we have the following sort orders available to users:</p> <pre><code>sortOrder: ['10', '9', 'superspine', '8', 'dc-gw', '7', '6', 'spine', '5', '4', 'leaf', 'border-leaf', '3', 'server', '2', '1'],\n</code></pre> <p>The values are sorted so that <code>10</code> is placed higher in the hierarchy than <code>9</code> and so on.</p> <p>Consider the following snippet:</p> <pre><code>topology:\n  nodes:\n    ### SPINES ###\n    spine1:\n      group: spine\n\n    ### LEAFS ###\n    leaf1:\n      group: leaf\n\n    ### CLIENTS ###\n    client1:\n      kind: linux\n      group: server\n</code></pre> <p>The <code>group</code> property set to the predefined value will automatically auto-align the elements based on their role.</p>"},{"location":"cmd/graph/#drawio","title":"Drawio","text":"<p>When <code>graph</code> command is called with the <code>--drawio</code> flag, containerlab will leverage the <code>clab-io-draw</code> project to generate the drawio file that represents the topology in a graphical form and can be imported into draw.io.</p> <p>You can pass additional arguments to the <code>clab-io-draw</code> tool using the <code>--drawio-args=</code> flag. For example:</p> <pre><code>containerlab graph --drawio --drawio-args=\"--theme nokia_dark --layout horizontal\" -t topo.yaml\n</code></pre>"},{"location":"cmd/graph/#mermaid","title":"Mermaid","text":"<p>When <code>graph</code> is called with the <code>--mermaid</code> flag containerlab generates a graph description file in Mermaid graph format. Several Markdown renders such as Github, Gitlab, and Notion support rendering embeded mermaid graphs in code blocks. If the results of the render are not satisfying the result can be imported into draw.io and further edited.</p>"},{"location":"cmd/graph/#graphviz","title":"Graphviz","text":"<p>When <code>graph</code> command is called with the <code>--dot</code> flag, containerlab will generate a graph description file in dot format.</p> <p>The dot file can be used to view the graphical representation of the topology either by rendering the dot file into a PNG file or using online dot viewer.</p>"},{"location":"cmd/graph/#online-vs-offline-graphing","title":"Online vs offline graphing","text":"<p>If the lab is running containerlab will try to build the graph by inspecting the running containers which are part of the lab. This method provides additional details (like IP addresses). It is possible to opt out of this behavior by using the --offline flag.</p> <p>If --offline flag was not provided and no containers were found matching the lab name, containerlab will use the topo file only (as if offline mode was set).</p>"},{"location":"cmd/graph/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] graph [local-flags]</code></p>"},{"location":"cmd/graph/#flags","title":"Flags","text":""},{"location":"cmd/graph/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used to spin up a lab.</p> <p>When the topology path refers to a directory, containerlab will look for a file with <code>.clab.yml</code> extension in that directory and use it as a topology definition file.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory.</p> <p>If more than one file is found for directory-based path or when the flag is omitted entirely, containerlab will fail with an error.</p>"},{"location":"cmd/graph/#srv","title":"srv","text":"<p>The <code>--srv</code> flag allows a user to customize the HTTP address and port for the web server. Default value is <code>:50080</code>.</p> <p>A single path <code>/</code> is served, where the graph is generated based on either a default template or on the template supplied using <code>--template</code>.</p>"},{"location":"cmd/graph/#template","title":"template","text":"<p>The <code>--template</code> flag allows to customize the HTML-based graph by supplying a user-defined template that will be rendered and exposed on the address specified by the <code>--srv</code>.</p>"},{"location":"cmd/graph/#static-dir","title":"static-dir","text":"<p>The <code>--static-dir</code> flag enables the embedded HTML web-server to serve static files from the specified directory. Must be used together with the <code>--template</code> flag.</p> <p>With this flag, it is possible to link to local files (JS, CSS, fonts, etc.) from the custom HTML template.</p>"},{"location":"cmd/graph/#drawio_1","title":"drawio","text":"<p>With <code>--drawio</code> flag set, containerlab will generate the drawio file for the topology file found in the current working directory.</p>"},{"location":"cmd/graph/#drawio-version","title":"drawio-version","text":"<p>To change the version of the clab-io-draw container used to generate the drawio file, use the <code>--drawio-version</code> flag. The default value is <code>latest</code>.</p>"},{"location":"cmd/graph/#dot","title":"dot","text":"<p>With <code>--dot</code> flag provided containerlab will generate the <code>dot</code> file instead of serving the topology with embedded HTTP server.</p>"},{"location":"cmd/graph/#mermaid_1","title":"mermaid","text":"<p>With <code>--mermaid</code> flag provided containerlab will generate the <code>mermaid</code> file instead of serving the topology with embedded HTTP server.</p>"},{"location":"cmd/graph/#mermaid-direction","title":"mermaid-direction","text":"<p>With <code>--mermaid-direction</code> flag provided with <code>--mermaid</code> flag, containerlab adjusts direction of the generated graph. Accepted values are TB, TD, BT, RL, and LR.</p>"},{"location":"cmd/graph/#node-filter","title":"node-filter","text":"<p>The local <code>--node-filter</code> flag allows users to specify a subset of topology nodes targeted by <code>graph</code> command. The value of this flag is a comma-separated list of node names as they appear in the topology.</p> <p>When a subset of nodes is specified, containerlab will only graph selected nodes and their links.</p>"},{"location":"cmd/graph/#examples","title":"Examples","text":""},{"location":"cmd/graph/#render-graph-of-topology-on-html-server","title":"Render graph of topology on HTML server","text":"<p>This will render the running lab if the lab is running and the topology file if it isn't. Default options will be used (HTML server running on port <code>50080</code>).</p> <pre><code>containerlab graph -t /path/to/topo1.clab.yml\n</code></pre>"},{"location":"cmd/graph/#render-graph-on-specified-http-server-port","title":"Render graph on specified http server port","text":"<pre><code>containerlab graph --topo /path/to/topo1.clab.yml --srv \":3002\"\n</code></pre>"},{"location":"cmd/graph/#render-graph-using-a-custom-html-template","title":"Render graph using a custom html template","text":"<pre><code>containerlab graph --topo /path/to/topo1.clab.yml --template my_template.html\n</code></pre>"},{"location":"cmd/graph/#render-graph-using-a-custom-template-that-links-to-local-files","title":"Render graph using a custom template that links to local files","text":"<p>The HTML server will use a custom template that links to local files located at /path/to/static_files directory</p> <pre><code>containerlab graph --topo /path/to/topo1.clab.yml --template my_template.html --static-dir /path/to/static_files\n</code></pre>"},{"location":"cmd/graph/#generate-a-drawio-file","title":"Generate a drawio file","text":"<p>Execute the following command in the directory where a <code>*.clab.yml</code> file is located:</p> <pre><code>containerlab graph --drawio\n</code></pre> <ol> <li> <p>This method is prone to errors when node names contain dashes and special symbols. Use with caution, and prefer the HTML server alternative.\u00a0\u21a9</p> </li> <li> <p>NeXt UI css/js files can be found at <code>/etc/containerlab/templates/graph/nextui</code> directory\u00a0\u21a9</p> </li> </ol>"},{"location":"cmd/inspect/","title":"inspect command","text":""},{"location":"cmd/inspect/#description","title":"Description","text":"<p>The <code>inspect</code> command provides the information about the deployed labs.</p>"},{"location":"cmd/inspect/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] inspect [local-flags]</code></p>"},{"location":"cmd/inspect/#flags","title":"Flags","text":""},{"location":"cmd/inspect/#all","title":"all","text":"<p>With the local <code>--all</code> flag it's possible to list all deployed labs in a single table. The output will also show the relative path to the topology file that was used to spawn this lab.</p> <p>The lab name and path values will be set for the first node of such lab, to reduce the clutter. Refer to the examples section for more details.</p>"},{"location":"cmd/inspect/#topology-name","title":"topology | name","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used to spin up a lab.</p> <p>When the topology path refers to a directory, containerlab will look for a file with <code>.clab.yml</code> extension in that directory and use it as a topology definition file.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory.</p> <p>If more than one file is found for directory-based path or when the flag is omitted entirely, containerlab will fail with an error.</p>"},{"location":"cmd/inspect/#format","title":"format","text":"<p>The local <code>--format</code> flag enables different output stylings. By default the table view will be used.</p> <p>Currently, the only other format option is <code>json</code> that will produce the output in the JSON format.</p>"},{"location":"cmd/inspect/#details","title":"details","text":"<p>The <code>inspect</code> command produces a brief summary about the running lab components. It is also possible to get a full view on the running containers by adding <code>--details</code> flag.</p> <p>With this flag inspect command will output every bit of information about the running containers. This is what <code>docker inspect</code> command provides.</p>"},{"location":"cmd/inspect/#wide","title":"wide","text":"<p>The local <code>-w | --wide</code> flag adds all available columns to the <code>inspect</code> output table.</p>"},{"location":"cmd/inspect/#examples","title":"Examples","text":""},{"location":"cmd/inspect/#list-all-running-labs-on-the-host","title":"List all running labs on the host","text":"<pre><code>\u276f containerlab inspect --all\n+---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+\n| # | Topo Path  | Lab Name |      Name       | Container ID |       Image        | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+\n| 1 | newlab.yml | newlab   | clab-newlab-n1  | 3c8262034088 | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.4/24 | 3fff:172:20:20::4/80 |\n| 2 |            |          | clab-newlab-n2  | 79c562b71997 | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.5/24 | 3fff:172:20:20::5/80 |\n| 3 | srl02.yml  | srl01    | clab-srl01-srl  | 13c9e7543771 | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.2/24 | 3fff:172:20:20::2/80 |\n| 4 |            |          | clab-srl01-srl2 | 8cfca93b7b6f | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.3/24 | 3fff:172:20:20::3/80 |\n+---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+\n</code></pre>"},{"location":"cmd/inspect/#provide-information-about-a-specific-running-lab-by-its-name","title":"Provide information about a specific running lab by its name","text":"<p>Provide information about the running lab named <code>srlceos01</code></p> <pre><code>\u276f containerlab inspect --name srlceos01\n+---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| # |        Name         | Container ID |  Image  | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srlceos01-ceos | 90bebb1e2c5f | ceos    | ceos |       | running | 172.20.20.4/24 | 3fff:172:20:20::4/80 |\n| 2 | clab-srlceos01-srl  | 82e9aa3c7e6b | srlinux | srl  |       | running | 172.20.20.3/24 | 3fff:172:20:20::3/80 |\n+---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+\n</code></pre>"},{"location":"cmd/inspect/#provide-information-about-a-specific-running-lab-by-its-topology-file","title":"Provide information about a specific running lab by its topology file","text":"<pre><code>\u276f clab inspect -t srl02.clab.yml \nINFO[0000] Parsing &amp; checking topology file: srl02.clab.yml \n+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+\n| # |      Name       | Container ID |         Image         | Kind |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+\n| 1 | clab-srl02-srl1 | 7a7c101be7d8 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.4/24 | 3fff:172:20:20::4/64 |\n| 2 | clab-srl02-srl2 | 5e3737621753 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.5/24 | 3fff:172:20:20::5/64 |\n+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+\n</code></pre>"},{"location":"cmd/inspect/#provide-owner-information-of-running-labs","title":"Provide owner information of running labs","text":"<p>An owner is a linux user that started the lab. When <code>sudo</code> is used, the original user is displayed as the owner.</p> <pre><code>clab inspect --all --wide\n+---+-----------------------------------+----------+-------+-----------------+--------------+-----------------------+---------------+---------+----------------+----------------------+\n| # |             Topo Path             | Lab Name | Owner |      Name       | Container ID |         Image         |     Kind      |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------------------------+----------+-------+-----------------+--------------+-----------------------+---------------+---------+----------------+----------------------+\n| 1 | lab-examples/srl01/srl01.clab.yml | srl01    | user1 | clab-srl01-srl  | ea86f40b412a | ghcr.io/nokia/srlinux | nokia_srlinux | running | 172.20.20.2/24 | 3fff:172:20:20::2/64 |\n| 2 | lab-examples/srl02/srl02.clab.yml | srl02    | user2 | clab-srl02-srl1 | ba7e807235b6 | ghcr.io/nokia/srlinux | nokia_srlinux | running | 172.20.20.4/24 | 3fff:172:20:20::4/64 |\n| 3 |                                   |          |       | clab-srl02-srl2 | 71006155b70a | ghcr.io/nokia/srlinux | nokia_srlinux | running | 172.20.20.3/24 | 3fff:172:20:20::3/64 |\n+---+-----------------------------------+----------+-------+-----------------+--------------+-----------------------+---------------+---------+----------------+----------------------+\n</code></pre>"},{"location":"cmd/inspect/#provide-information-about-a-specific-running-lab-in-json-format","title":"Provide information about a specific running lab in json format","text":"<pre><code>\u276f containerlab inspect --name srlceos01 -f json\n[\n  {\n    \"lab_name\": \"srlceos01\",\n    \"name\": \"clab-srlceos01-srl\",\n    \"container_id\": \"82e9aa3c7e6b\",\n    \"image\": \"srlinux\",\n    \"kind\": \"srl\",\n    \"state\": \"running\",\n    \"ipv4_address\": \"172.20.20.3/24\",\n    \"ipv6_address\": \"3fff:172:20:20::3/80\"\n  },\n  {\n    \"lab_name\": \"srlceos01\",\n    \"name\": \"clab-srlceos01-ceos\",\n    \"container_id\": \"90bebb1e2c5f\",\n    \"image\": \"ceos\",\n    \"kind\": \"ceos\",\n    \"state\": \"running\",\n    \"ipv4_address\": \"172.20.20.4/24\",\n    \"ipv6_address\": \"3fff:172:20:20::4/80\"\n  }\n]\n</code></pre>"},{"location":"cmd/redeploy/","title":"redeploy command","text":""},{"location":"cmd/redeploy/#description","title":"Description","text":"<p>The <code>redeploy</code> command redeploys a lab referenced by a provided topology definition file. It effectively combines the <code>destroy</code> and <code>deploy</code> commands into a single operation.</p> <p>The two most common applications of this command are:</p> <ol> <li> <p>Redeploying a lab while keeping the lab directory intact:</p> <pre><code>sudo containerlab redeploy -t mylab.clab.yml\n</code></pre> <p>This command will destroy the lab and redeploy it using the same topology file and the same lab directory. This should keep intact any saved configurations for the nodes.</p> </li> <li> <p>Redeploying a lab while removing the lab directory at the destroy stage:</p> <pre><code>sudo containerlab redeploy --cleanup -t mylab.clab.yml\n</code></pre> <p>or using the shorthands:</p> <pre><code>sudo clab rdep -c -t mylab.clab.yml\n</code></pre> <p>This command will destroy the lab and remove the lab directory before deploying the lab again. This ensures a clean redeployment as if you were deploying a lab for the first time discarding any previous lab state.</p> </li> </ol>"},{"location":"cmd/redeploy/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] redeploy [local-flags]</code></p> <p>aliases: <code>rdep</code></p>"},{"location":"cmd/redeploy/#flags","title":"Flags","text":""},{"location":"cmd/redeploy/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used to redeploy a lab.</p> <p>When the topology path refers to a directory, containerlab will look for a file with <code>.clab.yml</code> or <code>.clab.yaml</code> extension in that directory and use it as a topology definition file.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory.</p> <p>If more than one file is found for directory-based path or when the flag is omitted entirely, containerlab will fail with an error.</p>"},{"location":"cmd/redeploy/#cleanup","title":"cleanup","text":"<p>The local <code>--cleanup | -c</code> flag instructs containerlab to remove the lab directory and all its content during the destroy phase.</p> <p>Without this flag present, containerlab will keep the lab directory and all files inside of it.</p>"},{"location":"cmd/redeploy/#graceful","title":"graceful","text":"<p>To make containerlab attempt a graceful shutdown of the running containers during destroy phase, add the <code>--graceful</code> flag. Without it, containers will be removed forcefully without attempting to stop them.</p>"},{"location":"cmd/redeploy/#graph","title":"graph","text":"<p>The local <code>--graph | -g</code> flag instructs containerlab to generate a topology graph after deploying the lab.</p>"},{"location":"cmd/redeploy/#network","title":"network","text":"<p>With <code>--network</code> flag users can specify a custom name for the management network that containerlab creates for the lab.</p>"},{"location":"cmd/redeploy/#ipv4-subnet","title":"ipv4-subnet","text":"<p>Using <code>--ipv4-subnet | -4</code> flag users can define a custom IPv4 subnet that containerlab will use to assign management IPv4 addresses.</p>"},{"location":"cmd/redeploy/#ipv6-subnet","title":"ipv6-subnet","text":"<p>Using <code>--ipv6-subnet | -6</code> flag users can define a custom IPv6 subnet that containerlab will use to assign management IPv6 addresses.</p>"},{"location":"cmd/redeploy/#max-workers","title":"max-workers","text":"<p>With <code>--max-workers</code> flag, it is possible to limit the number of concurrent workers that create/delete containers or wire virtual links. By default, the number of workers equals the number of nodes/links to process.</p>"},{"location":"cmd/redeploy/#keep-mgmt-net","title":"keep-mgmt-net","text":"<p>Do not try to remove the management network during destroy phase. Usually the management docker network (in case of docker) and the underlying bridge are being removed. If you have attached additional resources outside of containerlab and you want the bridge to remain intact just add the <code>--keep-mgmt-net</code> flag.</p>"},{"location":"cmd/redeploy/#skip-post-deploy","title":"skip-post-deploy","text":"<p>The <code>--skip-post-deploy</code> flag can be used to skip the post-deploy phase of the lab deployment. This is a global flag that affects all nodes in the lab.</p>"},{"location":"cmd/redeploy/#export-template","title":"export-template","text":"<p>The local <code>--export-template</code> flag allows a user to specify a custom Go template that will be used for exporting topology data into <code>topology-data.json</code> file under the lab directory.</p>"},{"location":"cmd/redeploy/#node-filter","title":"node-filter","text":"<p>The local <code>--node-filter</code> flag allows users to specify a subset of topology nodes targeted by <code>redeploy</code> command. The value of this flag is a comma-separated list of node names as they appear in the topology.</p> <p>When a subset of nodes is specified, containerlab will only redeploy those nodes and their links and ignore the rest.</p>"},{"location":"cmd/redeploy/#skip-labdir-acl","title":"skip-labdir-acl","text":"<p>The <code>--skip-labdir-acl</code> flag can be used to skip the lab directory access control list (ACL) provisioning during the deploy phase.</p>"},{"location":"cmd/redeploy/#examples","title":"Examples","text":""},{"location":"cmd/redeploy/#redeploy-a-lab-using-the-given-topology-file","title":"Redeploy a lab using the given topology file","text":"<pre><code>containerlab redeploy -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/redeploy/#redeploy-a-lab-with-removing-the-lab-directory-at-destroy-stage","title":"Redeploy a lab with removing the Lab directory at destroy stage","text":"<pre><code>containerlab redeploy --cleanup -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/redeploy/#redeploy-a-lab-without-specifying-topology-file","title":"Redeploy a lab without specifying topology file","text":"<p>Given that a single topology file is present in the current directory.</p> <pre><code>containerlab redeploy\n</code></pre>"},{"location":"cmd/redeploy/#redeploy-a-lab-using-short-flag-names","title":"Redeploy a lab using short flag names","text":"<pre><code>clab rdep -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/redeploy/#redeploy-specific-nodes-in-a-lab","title":"Redeploy specific nodes in a lab","text":"<pre><code>containerlab redeploy -t mylab.clab.yml --node-filter \"node1,node2\"\n</code></pre>"},{"location":"cmd/save/","title":"save command","text":""},{"location":"cmd/save/#description","title":"Description","text":"<p>The <code>save</code> command perform configuration save for all the containers running in a lab.</p> <p>The exact command that performs configuration save depends on a given kind. The below table explains the method used for each kind:</p> Kind Command Notes Nokia SR Linux <code>sr_cli -d tools system configuration save</code> Nokia SR OS delivered via netconf RPC <code>copy-config running startup</code> Arista cEOS <code>Cli -p 15 -c wr</code> Cisco IOL <code>write memory</code>"},{"location":"cmd/save/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] save [local-flags]</code></p>"},{"location":"cmd/save/#flags","title":"Flags","text":""},{"location":"cmd/save/#topology-name","title":"topology | name","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used to spin up a lab.</p> <p>When the topology path refers to a directory, containerlab will look for a file with <code>.clab.yml</code> extension in that directory and use it as a topology definition file.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory.</p> <p>If more than one file is found for directory-based path or when the flag is omitted entirely, containerlab will fail with an error.</p>"},{"location":"cmd/save/#node-filter","title":"node-filter","text":"<p>The local <code>--node-filter</code> flag allows users to specify a subset of topology nodes targeted by <code>save</code> command. The value of this flag is a comma-separated list of node names as they appear in the topology.</p> <p>When a subset of nodes is specified, containerlab will only attempt to save configuration on the selected nodes.</p>"},{"location":"cmd/save/#examples","title":"Examples","text":""},{"location":"cmd/save/#save-the-configuration-of-the-containers-in-a-specific-lab","title":"Save the configuration of the containers in a specific lab","text":"<p>Save the configuration of the containers running in lab named srl02</p> <pre><code>\u276f containerlab save -n srl02\nINFO[0001] clab-srl02-srl1: stdout: /system:\n    Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:54.998Z' and comment ''\n\nINFO[0002] clab-srl02-srl2: stdout: /system:\n    Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:56.444Z' and comment ''\n</code></pre>"},{"location":"cmd/tools/disable-tx-offload/","title":"disable-tx-offload command","text":""},{"location":"cmd/tools/disable-tx-offload/#description","title":"Description","text":"<p>The <code>disable-tx-offload</code> command under the <code>tools</code> command disables tx checksum offload for <code>eth0</code> interface of a container referenced by its name.</p> <p>The need for <code>disable-tx-offload</code> might arise when you launch a container outside of containerlab or restart a container. Some nodes, like SR Linux, will require correct checksums in TCP packets; thus, it is needed to disable checksum offload on those containers to do checksum calculations instead of offloading it.</p>"},{"location":"cmd/tools/disable-tx-offload/#usage","title":"Usage","text":"<p><code>containerlab tools disable-tx-offload [local-flags]</code></p>"},{"location":"cmd/tools/disable-tx-offload/#flags","title":"Flags","text":""},{"location":"cmd/tools/disable-tx-offload/#container","title":"container","text":"<p>With the local mandatory <code>--container | -c</code> flag, a user specifies which container to remove tx offload.</p>"},{"location":"cmd/tools/disable-tx-offload/#examples","title":"Examples","text":"<pre><code># disable tx checksum offload on gnmic container\n\u276f clab tools disable-tx-offload -c clab-st-gnmic\nINFO[0000] getting container 'clab-st-gnmic' information \nINFO[0000] Tx checksum offload disabled for eth0 interface of clab-st-gnmic container \n</code></pre>"},{"location":"cmd/tools/cert/sign/","title":"Cert sign","text":""},{"location":"cmd/tools/cert/sign/#description","title":"Description","text":"<p>The <code>sign</code> sub-command under the <code>tools cert</code> command creates a private key and a certificate and signs the created certificate with a given Certificate Authority.</p>"},{"location":"cmd/tools/cert/sign/#usage","title":"Usage","text":"<p><code>containerlab tools cert sign [local-flags]</code></p>"},{"location":"cmd/tools/cert/sign/#flags","title":"Flags","text":""},{"location":"cmd/tools/cert/sign/#name","title":"Name","text":"<p>To set a name under which the certificate and key files will be saved, use the <code>--name | -n</code> flag. A name set to <code>mynode</code> will create files <code>mynode.pem</code>, <code>mynode.key</code> and <code>mynode.csr</code>. The default value is <code>cert</code>.</p>"},{"location":"cmd/tools/cert/sign/#path","title":"Path","text":"<p>A directory path under which the generated files will be placed is set with <code>--path | -p</code> flag. Defaults to acurrent working directory.</p>"},{"location":"cmd/tools/cert/sign/#ca-cert-and-ca-key","title":"CA Cert and CA Key","text":"<p>To indicate which CA should sign the certificate request, the provide a path to the CA certificate and key files.</p> <p><code>--ca-cert</code> flag sets the path to the CA certificate file. <code>--ca-key</code> flag sets the path to the CA private key file.</p>"},{"location":"cmd/tools/cert/sign/#common-name","title":"Common Name","text":"<p>Certificate Common Name (CN) field is set with <code>--cn</code> flag. Defaults to <code>containerlab.dev</code>.</p>"},{"location":"cmd/tools/cert/sign/#hosts","title":"Hosts","text":"<p>To add Subject Alternative Names (SAN) use the <code>--hosts</code> flag that takes a comma separate list of SAN values. Users can provide both DNS names and IP address, and the values will be placed into the DSN SAN and IP SAN automatically.</p>"},{"location":"cmd/tools/cert/sign/#country","title":"Country","text":"<p>Certificate Country (C) field is set with <code>--country | -c</code> flag. Defaults to <code>Internet</code>.</p>"},{"location":"cmd/tools/cert/sign/#locality","title":"Locality","text":"<p>Certificate Locality (L) field is set with <code>--locality | -l</code> flag. Defaults to <code>Server</code>.</p>"},{"location":"cmd/tools/cert/sign/#organization","title":"Organization","text":"<p>Certificate Organization (O) field is set with <code>--organization | -o</code> flag. Defaults to <code>Containerlab</code>.</p>"},{"location":"cmd/tools/cert/sign/#organization-unit","title":"Organization Unit","text":"<p>Certificate Organization Unit (OU) field is set with <code>--ou</code> flag. Defaults to <code>Containerlab Tools</code>.</p>"},{"location":"cmd/tools/cert/sign/#key-size","title":"Key size","text":"<p>To set the key size, use the <code>--key-size</code> flag. Defaults to <code>2048</code>.</p>"},{"location":"cmd/tools/cert/sign/#examples","title":"Examples","text":"<pre><code># create a private key and certificate and sign the latter\n# with the Hosts list of [node.io, 192.168.0.1]\n# saving both files under the default name `cert` in the PWD\n# and signed by the CA identified by cert ca.pem and key ca-key.pem\ncontainerlab tools cert sign --ca-cert /tmp/ca.pem \\\n             --ca-key /tmp/ca.key \\\n             --hosts node.io,192.168.0.1\n</code></pre> <p>Generated certificate can be verified/viewed with openssl tool:</p> <pre><code>openssl x509 -in ca.pem -text\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65\n&lt;SNIP&gt;\n</code></pre>"},{"location":"cmd/tools/cert/ca/create/","title":"CA Create","text":""},{"location":"cmd/tools/cert/ca/create/#description","title":"Description","text":"<p>The <code>create</code> sub-command under the <code>tools cert ca</code> command creates a Certificate Authority (CA) certificate and its private key.</p>"},{"location":"cmd/tools/cert/ca/create/#usage","title":"Usage","text":"<p><code>containerlab tools cert ca create [local-flags]</code></p>"},{"location":"cmd/tools/cert/ca/create/#flags","title":"Flags","text":""},{"location":"cmd/tools/cert/ca/create/#name","title":"Name","text":"<p>To set a name under which the certificate and key files will be saved, use the <code>--name | -n</code> flag. A name set to <code>myname</code> will create files <code>myname.pem</code>, <code>mynamey.key</code> and <code>myname.csr</code>. The default value is <code>ca</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#path","title":"Path","text":"<p>A directory path under which the generated files will be placed is set with <code>--path | -p</code> flag. Defaults to current working directory.</p>"},{"location":"cmd/tools/cert/ca/create/#expiry","title":"Expiry","text":"<p>Certificate validity period is set as a duration interval with <code>--expiry | -e</code> flag. Defaults to <code>87600h</code>, which is 10 years.</p>"},{"location":"cmd/tools/cert/ca/create/#common-name","title":"Common Name","text":"<p>Certificate Common Name (CN) field is set with <code>--cn</code> flag. Defaults to <code>containerlab.dev</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#country","title":"Country","text":"<p>Certificate Country (C) field is set with <code>--country | -c</code> flag. Defaults to <code>Internet</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#locality","title":"Locality","text":"<p>Certificate Locality (L) field is set with <code>--locality | -l</code> flag. Defaults to <code>Server</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#organization","title":"Organization","text":"<p>Certificate Organization (O) field is set with <code>--organization | -o</code> flag. Defaults to <code>Containerlab</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#organization-unit","title":"Organization Unit","text":"<p>Certificate Organization Unit (OU) field is set with <code>--ou</code> flag. Defaults to <code>Containerlab Tools</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#examples","title":"Examples","text":"<pre><code># create CA cert and key in the current dir.\n# uses default values for all certificate attributes\n# as a result, ca.pem and ca-cert.pem files will be written to the\n# current working directory\ncontainerlab tools cert ca create\n\n\n# create CA cert and key by the specified path with a filename root-ca\n# and a validity period of 1 minute\ncontainerlab tools cert ca create --path /tmp/certs/myca --name root-ca \\\n             --expiry 1m\n\nopenssl x509 -in /tmp/certs/myca/root-ca.pem -text | grep -A 2 Validity\n        Validity\n            Not Before: Mar 25 15:28:00 2021 GMT\n            Not After : Mar 25 15:29:00 2021 GMT\n</code></pre> <p>Generated certificate can be verified/viewed with openssl tool:</p> <pre><code>openssl x509 -in ca.pem -text\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65\n&lt;SNIP&gt;\n</code></pre>"},{"location":"cmd/tools/netem/set/","title":"Setting link impairments","text":"<p>With the <code>containerlab tools netem set</code> command users can set link impairments on a specific interface of a container. The following list of link impairments is supported:</p> <ul> <li>delay &amp; jitter</li> <li>packet loss</li> <li>rate limiting</li> <li>packet corruption</li> </ul> Considerations <p>Note, that <code>netem</code> is a Linux kernel module and it might not be available in particular kernel configurations.</p> <p>For example, on some RHEL 8 systems the following commands might be needed to run to add the support for netem:</p> <pre><code>dnf install kernel-debug-modules-extra\ndnf install kernel-modules-extra\nsystemctl reboot now \n</code></pre> <p>Note, that setting link impairments with <code>netem set</code> command is implemented in a way that all impairments are applied to the interface at once. This means that if an interface had a packet loss of 10% and you execute <code>netem set</code> command with a delay of 100ms, the packet loss will be reset to 0% and the delay will be set to 100ms.</p> <p>Once the impairments are set, they act for as long as the underlying node/container is running. To clear the impairments, set them to the default values.</p>"},{"location":"cmd/tools/netem/set/#usage","title":"Usage","text":"<pre><code>containerlab tools netem set [local-flags]\n</code></pre>"},{"location":"cmd/tools/netem/set/#flags","title":"Flags","text":""},{"location":"cmd/tools/netem/set/#node","title":"node","text":"<p>With the mandatory <code>--node | -n</code> flag a user specifies the name of the containerlab node to set link impairments on.</p>"},{"location":"cmd/tools/netem/set/#interface","title":"interface","text":"<p>With the mandatory <code>--interface | -i</code> flag a user specifies the name of the interface to set link impairments on. This can also be the interface alias, if one is used.</p>"},{"location":"cmd/tools/netem/set/#delay","title":"delay","text":"<p>With the <code>--delay</code> flag a user specifies the delay to set on the interface. The delay is specified in duration format. Example: <code>50ms</code>, <code>3s</code>.</p> <p>Default value is <code>0s</code>.</p>"},{"location":"cmd/tools/netem/set/#jitter","title":"jitter","text":"<p>Delay variation, aka jitter, is specified with the <code>--jitter</code> flag. The jitter is specified in duration format and can only be used if <code>--delay</code> is specified. Example: <code>5ms</code>.</p> <p>Default value is <code>0s</code>.</p>"},{"location":"cmd/tools/netem/set/#loss","title":"loss","text":"<p>Packet loss is specified with the <code>--loss</code> flag. The loss is specified in percentage format. Example: <code>10</code>.</p>"},{"location":"cmd/tools/netem/set/#rate","title":"rate","text":"<p>Egress rate limiting is specified with the <code>--rate</code> flag. The rate is specified in kbit per second format. Example: value <code>100</code> means rate of 100kbit/s.</p>"},{"location":"cmd/tools/netem/set/#corruption","title":"corruption","text":"<p>Packet corruption percentage is specified with the <code>--corruption</code> flag. Corruption modifies the contents of the packet at a random position based on percentage set.</p> <p>Example: corruption of 10 means 10% corruption probability for a traffic passing the interface.</p>"},{"location":"cmd/tools/netem/set/#examples","title":"Examples","text":""},{"location":"cmd/tools/netem/set/#setting-delay-and-jitter","title":"Setting delay and jitter","text":"<p>For <code>clab-netem-r1</code> node and its <code>eth1</code> interface set delay of 5ms and jitter of 1ms:</p> <pre><code>containerlab tools netem set -n clab-netem-r1 -i eth1 --delay 5ms --jitter 1ms\n</code></pre>"},{"location":"cmd/tools/netem/set/#setting-packet-loss","title":"Setting packet loss","text":"setting packet loss at 10% rate<pre><code>containerlab tools netem set -n clab-netem-r1 -i eth1 --loss 10\n</code></pre>"},{"location":"cmd/tools/netem/set/#clear-any-existing-impairments","title":"Clear any existing impairments","text":"<pre><code>containerlab tools netem set -n clab-netem-r1 -i eth1\n+-----------+-------+--------+-------------+-------------+\n| Interface | Delay | Jitter | Packet Loss | Rate (kbit) |\n+-----------+-------+--------+-------------+-------------+\n| eth1      | 0s    | 0s     | 0.00%       |           0 |\n+-----------+-------+--------+-------------+-------------+\n</code></pre> <p>The above command will use default values for all supported link impairments, which is <code>0s</code> for delay and jitter, <code>0</code> for loss and <code>0</code> for rate.</p>"},{"location":"cmd/tools/netem/show/","title":"Showing link impairments","text":"<p>With the <code>containerlab tools netem show</code> command users can list all link impairments for a given containerlab node.</p> <p>For links with no associated qdisc the output will contain <code>N/A</code> values.</p>"},{"location":"cmd/tools/netem/show/#usage","title":"Usage","text":"<pre><code>containerlab tools netem show [local-flags]\n</code></pre>"},{"location":"cmd/tools/netem/show/#flags","title":"Flags","text":""},{"location":"cmd/tools/netem/show/#node","title":"node","text":"<p>With the mandatory <code>--node | -n</code> flag a user specifies the name of the containerlab node to show link impairments on.</p>"},{"location":"cmd/tools/netem/show/#examples","title":"Examples","text":""},{"location":"cmd/tools/netem/show/#showing-link-impairments-for-a-node","title":"Showing link impairments for a node","text":"<pre><code>containerlab tools netem show -n clab-netem-r1\n+-----------+-------+--------+-------------+-------------+\n| Interface | Delay | Jitter | Packet Loss | Rate (kbit) |\n+-----------+-------+--------+-------------+-------------+\n| lo        | N/A   | N/A    | N/A         | N/A         |\n| eth0      | N/A   | N/A    | N/A         | N/A         |\n| eth1      | 15ms  | 2ms    |        0.00 |           0 |\n+-----------+-------+--------+-------------+-------------+\n</code></pre>"},{"location":"cmd/tools/veth/create/","title":"vEth create","text":""},{"location":"cmd/tools/veth/create/#description","title":"Description","text":"<p>The <code>create</code> sub-command under the <code>tools veth</code> command creates a vEth interface between the following combination of nodes:</p> <ol> <li>container &lt;-&gt; container</li> <li>container &lt;-&gt; linux bridge</li> <li>container &lt;-&gt; ovs bridge</li> <li>container &lt;-&gt; host</li> </ol> <p>To specify the both endpoints of the veth interface pair the following two notations are used:</p> <ol> <li>two elements notation: <code>&lt;node-name&gt;:&lt;interface-name&gt;</code>     this notation is used for <code>container &lt;-&gt; container</code> or <code>container &lt;-&gt; host</code> attachments.</li> <li>three elements notation: <code>&lt;kind&gt;:&lt;node-name&gt;:&lt;interface-name&gt;</code>     this notation is used for <code>container &lt;-&gt; bridge</code> and <code>container &lt;-&gt; ovs-bridge</code> attachments</li> </ol> <p>Check out examples to see how these notations are used.</p>"},{"location":"cmd/tools/veth/create/#usage","title":"Usage","text":"<p><code>containerlab tools veth create [local-flags]</code></p>"},{"location":"cmd/tools/veth/create/#flags","title":"Flags","text":""},{"location":"cmd/tools/veth/create/#a-endpoint","title":"a-endpoint","text":"<p>vEth interface endpoint A is set with <code>--a-endpoint | -a</code> flag.</p>"},{"location":"cmd/tools/veth/create/#b-endpoint","title":"b-endpoint","text":"<p>vEth interface endpoint B is set with <code>--b-endpoint | -b</code> flag.</p>"},{"location":"cmd/tools/veth/create/#mtu","title":"mtu","text":"<p>vEth interface MTU is set to <code>9500</code> by default, and can be changed with <code>--mtu | -m</code> flag.</p>"},{"location":"cmd/tools/veth/create/#examples","title":"Examples","text":"<pre><code># create veth interface between containers clab-demo-node1 and clab-demo-node2\n# both ends of veth pair will be named `eth1`\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b clab-demo-node2:eth1\n\n# create veth interface between container clab-demo-node1 and linux bridge br-1\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b bridge:br-1:br-eth1\n\n# create veth interface between container clab-demo-node1 and OVS bridge ovsbr-1\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b ovs-bridge:ovsbr-1:br-eth1\n\n# create veth interface between container clab-demo-node1 and host\n# note that a special node-name `host` is reserved to indicate that attachment is destined for container host system\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b host:veth-eth1\n</code></pre>"},{"location":"cmd/tools/vxlan/create/","title":"vxlan create","text":""},{"location":"cmd/tools/vxlan/create/#description","title":"Description","text":"<p>The <code>create</code> sub-command under the <code>tools vxlan</code> command creates a VxLAN interface and sets <code>tc</code> rules to redirect traffic to/from a specified interface available in root namespace of a container host.</p> <p>This combination of a VxLAN interface and <code>tc</code> rules make possible to transparently connect lab nodes running on different VMs/hosts.</p> <p>VxLAN interface name will be a catenation of a prefix <code>vx-</code> and the interface name that is used to redirect traffic. If the existing interface is named <code>srl_e1-1</code>, then VxLAN interface created for this interface will be named <code>vx-srl_e1-1</code>.</p>"},{"location":"cmd/tools/vxlan/create/#usage","title":"Usage","text":"<p><code>containerlab tools vxlan create [local-flags]</code></p>"},{"location":"cmd/tools/vxlan/create/#flags","title":"Flags","text":""},{"location":"cmd/tools/vxlan/create/#remote","title":"remote","text":"<p>VxLAN tunnels set up with this command are unidirectional in nature. To set the remote endpoint address the <code>--remote</code> flag should be used.</p>"},{"location":"cmd/tools/vxlan/create/#port","title":"port","text":"<p>Port number that the VxLAN tunnel will use is set with <code>--port | -p</code> flag. Defaults to <code>14789</code><sup>1</sup>.</p>"},{"location":"cmd/tools/vxlan/create/#id","title":"id","text":"<p>VNI that the VxLAN tunnel will use is set with <code>--id | -i</code> flag. Defaults to <code>10</code>.</p>"},{"location":"cmd/tools/vxlan/create/#link","title":"link","text":"<p>As mentioned above, the tunnels are set up with a goal to transparently connect containers deployed on different hosts.</p> <p>To indicate which interface will be \"piped\" to a VxLAN tunnel the <code>--link | -l</code> flag should be used.</p>"},{"location":"cmd/tools/vxlan/create/#dev","title":"dev","text":"<p>With <code>--dev</code> flag users can set the linux device that should be used in setting up the tunnel.</p> <p>Normally this flag can be omitted, since containerlab will take the device name which is used to reach the remote address as seen by the kernel routing table.</p>"},{"location":"cmd/tools/vxlan/create/#mtu","title":"mtu","text":"<p>With <code>--mtu | -m</code> flag it is possible to set VxLAN MTU. Max MTU is automatically set, so this flag is only needed when MTU lower than max is needed to be provisioned.</p>"},{"location":"cmd/tools/vxlan/create/#examples","title":"Examples","text":"<pre><code># create vxlan tunnel and redirect traffic to/from existing interface srl_e1-1 to it\n# this effectively means anything that appears on srl_e1-1 interface will be piped to vxlan interface\n# and vice versa.\n\n# srl_e1-1 interface exists in root namespace\n\u276f ip l show srl_e1-1\n617: srl_e1-1@if618: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default \n    link/ether fa:4c:16:11:11:05 brd ff:ff:ff:ff:ff:ff link-netns clab-vx-srl1\n\n# create a vxlan tunnel to a remote vtep 10.0.0.20 with VNI 10 and redirect traffic to srl_e1-1 interface\n\u276f clab tools vxlan create --remote 10.0.0.20 -l srl_e1-1 --id 10\nINFO[0000] Adding VxLAN link vx-srl_e1-1 under ens3 to remote address 10.0.0.20 with VNI 10\nINFO[0000] configuring ingress mirroring with tc in the direction of vx-srl_e1-1 -&gt; srl_e1-1\nINFO[0000] configuring ingress mirroring with tc in the direction of srl_e1-1 -&gt; vx-srl_e1-1\n\n# check the created interface\n\u276f ip l show vx-srl_e1-1\n619: vx-srl_e1-1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/ether 7a:6e:ba:82:a4:6f brd ff:ff:ff:ff:ff:ff\n</code></pre> <ol> <li> <p>The reason we don't use default <code>4789</code> port number is because it is often blocked/filtered in (cloud) environments and we want to make sure that the VxLAN tunnels have higher chances to be established.\u00a0\u21a9</p> </li> </ol>"},{"location":"cmd/tools/vxlan/delete/","title":"vxlan delete","text":""},{"location":"cmd/tools/vxlan/delete/#description","title":"Description","text":"<p>The <code>delete</code> sub-command under the <code>tools vxlan</code> command deletes VxLAN interfaces which name matches a user specified prefix. The <code>delete</code> command is typically used to remove VxLAN interfaces created with <code>create</code> command.</p>"},{"location":"cmd/tools/vxlan/delete/#usage","title":"Usage","text":"<p><code>containerlab tools vxlan delete [local-flags]</code></p>"},{"location":"cmd/tools/vxlan/delete/#flags","title":"Flags","text":""},{"location":"cmd/tools/vxlan/delete/#prefix","title":"prefix","text":"<p>Set a prefix with <code>--prefix | -p</code> flag. The VxLAN interfaces which name is matched by the prefix will be deleted. Default prefix is <code>vx-</code> which is matched the default prefix used by <code>create</code> command.</p>"},{"location":"cmd/tools/vxlan/delete/#examples","title":"Examples","text":"<pre><code># delete all VxLAN interfaces created by containerlab\n\u276f clab tools vxlan create delete\nINFO[0000] Deleting VxLAN link vx-srl_e1-1\n</code></pre>"},{"location":"lab-examples/bgp-vpls-nok-jun/","title":"BGP VPLS between Nokia and Juniper","text":"Description BGP VPLS between Nokia SR OS and Juniper vMX Components Nokia SR OS, Juniper vMX Resource requirements<sup>1</sup>  2  7-10 GB Lab location hellt/bgp-vpls-lab Topology file vpls.clab.yml Version information<sup>2</sup> <code>containerlab:0.10.1</code>, <code>vr-sros:20.10.R1</code>, <code>vr-vmx:20.4R1.12</code>, <code>docker-ce:19.03.13</code>, <code>vrnetlab</code><sup>3</sup>"},{"location":"lab-examples/bgp-vpls-nok-jun/#description","title":"Description","text":"<p>This lab demonstrates how containerlab can be used in a classical networking labs where the prime focus is not on the containerized NOS, but on a classic VM-based routers.</p> <p>The topology created in this lab matches the network used in the BGP VPLS Deep Dive article:</p> <p></p> <p>It allows readers to follow through the article with the author and create BGP VPLS service between the Nokia and Juniper routers using configuration snippets provided within the lab repository.</p> <p>As the article was done before Nokia introduced MD-CLI, the configuration snippets for SR OS were translated to MD-CLI.</p>"},{"location":"lab-examples/bgp-vpls-nok-jun/#quickstart","title":"Quickstart","text":"<ol> <li>Ensure that your host supports virtualization and/or nested virtualization in case of a VM.</li> <li>Install<sup>4</sup> containerlab.</li> <li>Build if needed, vrnetlab container images for the routers used in the lab.</li> <li>Clone lab repository.</li> <li>Deploy the lab topology <code>clab dep -t vpls.clab.yml</code></li> </ol> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKSM might help with RAM consumption.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> <li> <p>Router images are built with vrnetlab aebe377. To reproduce the image, checkout to this commit and build the relevant images. Note, that you might need to use containerlab of the version that is stated in the description.\u00a0\u21a9</p> </li> <li> <p>If installing the latest containerlab, make sure to use the latest hellt/vrnetlab project as well, as there might have been changes with the integration. If unsure, install the containerlab version that is specified in the lab description.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/cvx01/","title":"Cumulus Linux and FRR","text":"Description Cumulus Linux connected back-to-back with FRR Components Cumulus Linux Resource requirements[^1]  1  1 GB Topology file topo.clab.yml Name cvx01 Version information[^2] <code>cvx:4.3.0</code> <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/cvx01/#description","title":"Description","text":"<p>The lab consists of Cumulus Linux and FRR nodes connected back-to-back over a point-to-point ethernet link. Both nodes are also connected to the clab docker network over their management <code>eth0</code> interfaces.</p>"},{"location":"lab-examples/cvx01/#configuration","title":"Configuration","text":"<p>Both nodes have been provided with a startup configuration and should come up with all their interfaces fully configured.</p> <p>Once the lab is started, the nodes will be able to ping each other:</p> <pre><code>$ ssh lab-cvx01-sw1\nWarning: Permanently added 'clab-cvx01-sw1,192.168.223.2' (ECDSA) to the list of known hosts.\nroot@clab-cvx01-sw1's password:\nLinux 1c3f259f31872500 4.19.0-cl-1-amd64 #1 SMP Cumulus 4.19.149-1+cl4.3u1 (2021-01-28) x86_64\nroot@1c3f259f31872500:mgmt:~# ping 12.12.12.2 -c 1\nvrf-wrapper.sh: switching to vrf \"default\"; use '--no-vrf-switch' to disable\nPING 12.12.12.2 (12.12.12.2) 56(84) bytes of data.\n64 bytes from 12.12.12.2: icmp_seq=1 ttl=64 time=0.400 ms\n\n--- 12.12.12.2 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.400/0.400/0.400/0.000 ms\n</code></pre>"},{"location":"lab-examples/cvx01/#use-cases","title":"Use cases","text":"<ul> <li>Demonstrate how a <code>cvx</code> node, running in its default <code>ignite</code> runtime, can connect to nodes running in other runtimes, e.g. <code>docker</code></li> <li>Demonstrate how to inject startup configuration into a <code>cvx</code> node.</li> <li>Verify basic control plane and data plane operations</li> </ul>"},{"location":"lab-examples/cvx02/","title":"Cumulus Linux (docker runtime) and Host","text":"Description Cumulus Linux in Docker runtime connected to a Host Components Cumulus Linux Resource requirements[^1]  1  1 GB Topology file topo.clab.yml Name cvx02 Version information[^2] <code>cvx:4.3.0</code> <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/cvx02/#description","title":"Description","text":"<p>The lab consists of Cumulus Linux running in its non-default, docker runtime, connected to a <code>linux</code> container playing the role of an end-host. Both nodes are also connected to the clab docker network over their management <code>eth0</code> interfaces.</p>"},{"location":"lab-examples/cvx02/#configuration","title":"Configuration","text":"<p>Both nodes have been provided with a startup configuration and should come up with all their interfaces fully configured.</p> <p>Once the lab is started, the nodes will be able to ping each other:</p> <pre><code>$  clab-cvx02-sw1\nWarning: Permanently added 'clab-cvx02-sw1,3fff:172:20:20::3' (ECDSA) to the list of known hosts.\nroot@clab-cvx02-sw1's password:\nLinux sw1 5.10.16.3-networkop+ #17 SMP Mon May 24 15:22:51 BST 2021 x86_64\nroot@sw1:mgmt:~# ping 12.12.12.2\nvrf-wrapper.sh: switching to vrf \"default\"; use '--no-vrf-switch' to disable\nPING 12.12.12.2 (12.12.12.2) 56(84) bytes of data.\n64 bytes from 12.12.12.2: icmp_seq=1 ttl=64 time=0.297 ms\n^C\n--- 12.12.12.2 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.297/0.297/0.297/0.000 ms\nroot@sw1:mgmt:~#\n</code></pre>"},{"location":"lab-examples/cvx02/#use-cases","title":"Use cases","text":"<ul> <li>Demonstrate how a <code>cvx</code> node can run in its non-default <code>docker</code> runtime</li> <li>Demonstrate how to inject startup configuration into a <code>cvx</code> node.</li> <li>Verify basic control plane and data plane operations</li> </ul>"},{"location":"lab-examples/ext-bridge/","title":"External bridge capability","text":"Description Connecting nodes via linux bridges Components Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file br01.clab.yml Name br01"},{"location":"lab-examples/ext-bridge/#description","title":"Description","text":"<p>This lab consists of three Nokia SR Linux nodes connected to a linux bridge.</p> <p></p> <p>Note</p> <p><code>containerlab</code> will not create/remove the bridge interface on your behalf.</p> <p>bridge element must be part of the lab nodes. Consult with the topology file to see how to reference a bridge.</p>"},{"location":"lab-examples/ext-bridge/#use-cases","title":"Use cases","text":"<p>By introducing a link of <code>bridge</code> type to the containerlab topology, we are opening ourselves to some additional scenarios:</p> <ul> <li>interconnect nodes via a broadcast domain</li> <li>connect multiple fabrics together</li> <li>connect containerlab nodes to the applications/nodes running outside of the lab host</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/freebsd01/","title":"Freebsd01","text":"Description A FreeBSD connected to two Alpine Linux Hosts Components FreeBSD, Multitool Alpine Linux Resource requirements<sup>1</sup>  1  512 MB Topology file freebsd01.clab.yml Name freebsd01 Version information<sup>2</sup> <code>freebsd-13.2-zfs-2023-04-21.qcow2</code>, <code>docker:24.0.6</code>"},{"location":"lab-examples/freebsd01/#description","title":"Description","text":"<p>This lab consists of one FreeBSD router connected to two Alpine Linux nodes.</p> <pre><code>client1&lt;----&gt;FreeBSD&lt;----&gt;client2\n</code></pre>"},{"location":"lab-examples/freebsd01/#configuration","title":"Configuration","text":"<p>The FreeBSD node takes about 1 minute to complete its start up. Check using \"docker container ls\" until the FreeBSD container shows up as \"healthy\".</p> <pre><code># docker container ls\nCONTAINER ID   IMAGE                                  COMMAND                  CREATED              STATUS                        PORTS                                       NAMES\n30c629f1a12e   vrnetlab/vr-freebsd:13.2               \"/launch.py --userna\u2026\"   23 hours ago   Up 23 hours (healthy)   22/tcp, 5000/tcp, 10000-10099/tcp           clab-freebsd01-fbsd1\n6b476bfa41b1   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   23 hours ago   Up 23 hours             80/tcp, 443/tcp, 1180/tcp, 11443/tcp        clab-freebsd01-client2\n21ab9e4857b3   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   23 hours ago   Up 23 hours             80/tcp, 443/tcp, 1180/tcp, 11443/tcp        clab-freebsd01-client1\n</code></pre>"},{"location":"lab-examples/freebsd01/#fbsd01","title":"fbsd01","text":"<p>Log into the FreeBSD node using SSH with <code>ssh admin@clab-freebsd01-fbsd1</code> and add the following configuration. Password is <code>admin</code>.</p> <pre><code>sudo sysctl net.inet.ip.forwarding=1\nsudo ifconfig vtnet1 192.168.1.1/30\nsudo ifconfig vtnet2 192.168.2.1/30\n</code></pre>"},{"location":"lab-examples/freebsd01/#client1","title":"client1","text":"<p>The two clients should be configured with the correct IP addresses and a route to the other client via the FreeBSD node. First attach to the container process <code>docker exec -it clab-freebsd01-client1 ash</code></p> <pre><code>docker exec -it clab-freebsd01-client1 ash\n\n# ip -br a show dev eth1\neth1@if71231     UP             192.168.1.2/30 fe80::a8c1:abff:fe17:b15f/64\n\n# ip r\ndefault via 172.20.20.1 dev eth0\n172.20.20.0/24 dev eth0 proto kernel scope link src 172.20.20.3\n192.168.1.0/30 dev eth1 proto kernel scope link src 192.168.1.2\n192.168.2.0/30 via 192.168.1.1 dev eth1\n</code></pre>"},{"location":"lab-examples/freebsd01/#verification","title":"Verification","text":"<p>Traceroute from client1 to client2 to verify the data-plane via the FreeBSD node.</p>"},{"location":"lab-examples/freebsd01/#client1_1","title":"client1","text":"<pre><code># traceroute 192.168.2.2\ntraceroute to 192.168.2.2 (192.168.2.2), 30 hops max, 46 byte packets\n 1  192.168.1.1 (192.168.1.1)  0.680 ms  0.676 ms  0.597 ms\n 2  192.168.2.2 (192.168.2.2)  1.105 ms  1.088 ms  0.869 ms\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/frr01/","title":"FRR","text":"Description A 3-node ring of FRR routers with OSPF IGP Components FRR Resource requirements<sup>1</sup>  2  2 GB Topology file frr01.clab.yml Name frr01 Version information<sup>2</sup> <code>containerlab:0.13.0</code>, <code>frrouting/frr:v7.5.1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/frr01/#description","title":"Description","text":"<p>This lab example consists of three FRR routers connected in a ring topology. Each router has one PC connected to it.</p> <p>This is also an example of how to pre-configure lab nodes of <code>linux</code> kind in containerlab.</p> <p>To start this lab, run the <code>run.sh</code> script, which will run the containerlab deploy commands, and then configure the PC interfaces.</p> <p>The lab configuration is documented in detail at: https://www.brianlinkletter.com/2021/05/use-containerlab-to-emulate-open-source-routers/</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/ftdv01/","title":"Cisco FTDv","text":"Description A Cisco FTDv connected to two Alpine Linux Hosts Components Cisco FTDV, Multitool Alpine Linux Resource requirements<sup>1</sup>  4  8 GB Topology file ftdv01.yml Name ftdv01 Version information<sup>2</sup> <code>Cisco_Secure_Firewall_Threat_Defense_Virtual-7.2.5-208.qcow2</code>, <code>docker:24.0.6</code>"},{"location":"lab-examples/ftdv01/#description","title":"Description","text":"<p>This lab consists of one Cisco FTDv firewall connected to two Alpine Linux nodes.</p> <pre><code>client1&lt;----&gt;FTDv&lt;----&gt;client2\n</code></pre>"},{"location":"lab-examples/ftdv01/#configuration","title":"Configuration","text":"<p>The FTDv node takes about 1-2 minutes to complete its start up. Check using \"docker container ls\" until the FTDv container shows up as \"healthy\".</p> <pre><code># docker container ls\nCONTAINER ID   IMAGE                                  COMMAND                  CREATED              STATUS                        PORTS                                       NAMES\n5682d73984d1   vrnetlab/vr-ftdv:7.2.5                 \"/launch.py --userna\u2026\"   34 minutes ago   Up 34 minutes (healthy)   22/tcp, 80/tcp, 443/tcp, 5000/tcp, 8305/tcp, 10000-10099/tcp   clab-ftdv01-ftdv1\n1ebe3dae6846   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   34 minutes ago   Up 34 minutes             80/tcp, 443/tcp, 1180/tcp, 11443/tcp                           clab-ftdv01-client1\n9726c9bb9e21   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   34 minutes ago   Up 34 minutes             80/tcp, 443/tcp, 1180/tcp, 11443/tcp                           clab-ftdv01-client2\n</code></pre>"},{"location":"lab-examples/ftdv01/#ftdv1","title":"ftdv1","text":"<p>Log into the FTDv node using the Web UI and add the following configuration. Password is <code>Admin@123</code>.</p> <ol> <li>Click \"Skip device setup\" on the initial screen.</li> <li>In the dialog window \"Are you sure you want to skip device setup?\" check the \"Start 90-day evaluation\" box, select the \"FTDv5 - Tiered\" performance tier, and click \"Confirm\".</li> <li>In the \"Interfaces\" menu configure GigabitEthernet0/0 with the <code>192.168.1.1/30</code> IP, and GigabitEthernet0/1 with the <code>192.168.2.1/30</code> IP.</li> <li>Go to the \"Policies\" menu and add a test \"allow all\" policy (all fields should be left empty, and the action should be \"allow\").</li> <li>Deploy pending changes.</li> </ol>"},{"location":"lab-examples/ftdv01/#client1","title":"client1","text":"<p>The two clients should be configured with the correct IP addresses and a route to the other client via the FTDv node. First attach to the container process <code>docker exec -it clab-ftdv01-client1 ash</code></p> <pre><code>docker exec -it clab-ftdv01-client1 ash\n\n# ip -br a show dev eth1\neth1@if3749      UP             192.168.1.2/30 fe80::a8c1:abff:feee:be5c/64\n\n# ip r\ndefault via 172.20.20.1 dev eth0\n172.20.20.0/24 dev eth0 proto kernel scope link src 172.20.20.4\n192.168.1.0/30 dev eth1 proto kernel scope link src 192.168.1.2\n192.168.2.0/30 via 192.168.1.1 dev eth1\n</code></pre>"},{"location":"lab-examples/ftdv01/#verification","title":"Verification","text":"<p>Traceroute from client1 to client2 to verify the data-plane via the FTDv node.</p>"},{"location":"lab-examples/ftdv01/#client1_1","title":"client1","text":"<pre><code># traceroute 192.168.2.2\ntraceroute to 192.168.2.2 (192.168.2.2), 30 hops max, 46 byte packets\n 1  192.168.2.2 (192.168.2.2)  1.372 ms  0.909 ms  0.403 ms\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/generic_vm01/","title":"Generic VM","text":"Description A generic Ubuntu VM interconnected with Nokia SR Linux Components Ubuntu VM, SR Linux Resource requirements<sup>1</sup>  2  4096 MB Topology file generic_vm01.yml Version information<sup>2</sup> <code>containerlab:0.55.0</code>, <code>ubuntu:22.04</code>, <code>docker:26.0.0</code>"},{"location":"lab-examples/generic_vm01/#description","title":"Description","text":"<p>This lab demonstrates how to use a Generic VM kind using Ubuntu 22.04 LTS by connecting it to the SR Linux switch and running a basic ping test.</p> <p>The topology is rather simple, with Ubuntu VM and SR Linux switch connected over a single interface.</p> <pre><code>ubuntu:eth1 &lt;----&gt; e1-1:SR Linux\n</code></pre>"},{"location":"lab-examples/generic_vm01/#deployment","title":"Deployment","text":"<p>Before deploying the lab, make sure you have built the container image for the Ubuntu VM using hellt/vrnetlab project. The topology file references the image as <code>vrnetlab/vr-ubuntu:jammy</code>, which is the default image name for Ubuntu 22.04 LTS.</p> <p>After building the image, deploy the lab using the following command:</p> <pre><code>sudo containerlab deploy -t generic_vm.clab.yml\n</code></pre>"},{"location":"lab-examples/generic_vm01/#configuration","title":"Configuration","text":"<p>The Ubuntu 22.04 VM takes about 1 minute to complete its start up and then extra 30 seconds to allow password-based authentication over SSH. Check the boot log using <code>docker logs -f clab-generic_vm-ubuntu</code>.</p>"},{"location":"lab-examples/generic_vm01/#sr-linux","title":"SR Linux","text":"<p>As seen in the topology file, the SR Linux node comes with its <code>ethernet-1/1</code> interface and subinterface preconfigured with <code>192.168.0.2/24</code> IP address. Thus no additional configuration is needed.</p>"},{"location":"lab-examples/generic_vm01/#ubuntu","title":"ubuntu","text":"<p>Log into the <code>ubuntu</code> node using SSH with <code>ssh clab-generic_vm-ubuntu</code> and add the IP configuration to the <code>ens2</code> interface that connects the VM with SR Linux switch. Password is <code>clab@123</code>.</p> <pre><code>sudo ip addr add dev ens2 192.168.0.1/24\nsudo ip link set dev ens2 up\n</code></pre>"},{"location":"lab-examples/generic_vm01/#verification","title":"Verification","text":"<p>With interface on Ubuntu side configured, ping from Ubuntu to SR Linux to verify the connectivity.</p> <pre><code>clab@ubuntu:~$ ping 192.168.0.2\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=14.2 ms\n64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=8.85 ms\n^C\n--- 192.168.0.2 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1002ms\nrtt min/avg/max/mdev = 8.849/11.508/14.168/2.659 ms\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/ixiacone-srl/","title":"Keysight IXIA-C and Nokia SR Linux","text":"Description Keysight Ixia-c-one node connected with Nokia SR Linux Components Keysight Ixia-c-one, Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file ixiacone-srl.clab.yaml Name ixiac01 Version information<sup>2</sup> <code>containerlab:0.46.2</code>, <code>ixia-c-one:1.19.0-5</code>, <code>srlinux:23.10.1</code>, <code>docker-ce:20.10.2</code>"},{"location":"lab-examples/ixiacone-srl/#description","title":"Description","text":"<p>This lab consists of a Keysight Ixia-c-one node with 2 ports connected to 2 ports on a Nokia SR Linux node via two point-to-point ethernet links. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p> <p>Keysight Ixia-c-one is a single-container distribution of Ixia-c, a software traffic generator and protocol emulator with Open Traffic Generator (OTG) API. This example will demonstrate how test case designers can leverage Go SDK client gosnappi to create an OTG configuration and execute a test verifying IPv4 forwarding.</p>"},{"location":"lab-examples/ixiacone-srl/#deployment","title":"Deployment","text":"<p>Change into the lab directory:</p> <pre><code>cd /etc/containerlab/lab-examples/ixiac01\n</code></pre> <p>Deploy the lab:</p> <pre><code>sudo containerlab deploy\n</code></pre>"},{"location":"lab-examples/ixiacone-srl/#use-cases","title":"Use cases","text":"<p>This lab allows users to validate an IPv4 traffic forwarding scenario between Keysight Ixia-c-one and Nokia SR Linux.</p>"},{"location":"lab-examples/ixiacone-srl/#ipv4-traffic-forwarding","title":"IPv4 Traffic forwarding","text":"<p>This lab demonstrates a simple IPv4 traffic forwarding scenario where</p> <ul> <li>Keysight Ixia-c-one with two test ports <code>eth1</code> and <code>eth2</code> connected to Nokia SR Linux with ports <code>e1-1</code> and <code>e1-2</code> respectively.</li> <li>An OTG configuration applied to Ixia-c-one that emulates a router behind each test port: <code>r1</code> with IP <code>1.1.1.1/24</code> behind <code>eth1</code>, and <code>r2</code> with IP <code>2.2.2.1/24</code> behind <code>eth2</code>.</li> <li>The test is configured to send 100 IPv4 packets with a rate 10pps from <code>10.10.10.1</code> behind <code>r1</code> to <code>10.20.20.x</code>, where <code>x</code> is changed from 1 to 5.</li> <li>SR Linux interfaces are configured with <code>1.1.1.2/24</code> and <code>2.2.2.2/24</code> IPv4 addresses.</li> <li>SR Linux is configured to forward the traffic destined for <code>20.20.20.0/24</code> to <code>2.2.2.1</code> using a static route in the default network instance.</li> </ul> <p>Logical IP topology of the lab is shown below:</p>"},{"location":"lab-examples/ixiacone-srl/#configuration","title":"Configuration","text":"<p>During the lab deployment and test execution the following configuration is applied to the lab nodes to forward and receive traffic.</p> <ul> <li> <p>SR Linux     SR Linux node comes up pre-configured with the commands listed in srl.cfg file which configure IPv4 addresses on both interfaces and install a static route to forward the traffic coming from ixia-c.</p> </li> <li> <p>Keysight ixia-c-one     IPv4 addresses for <code>ixia-c-one</code> node interfaces are configured via the OTG API as part of the <code>ipv4_forwarding.go</code> script.</p> </li> </ul>"},{"location":"lab-examples/ixiacone-srl/#execution","title":"Execution","text":"<p>The test case is written in Go language. To run it, Go &gt;= 1.21 needs to be installed first.</p> <p>Once installed, run the test:</p> <pre><code>go run ipv4_forwarding.go\n</code></pre> <p>Once 100 packets are sent, the test script checks that we received all the sent packets.</p> <p>During the test run you will see flow metrics reported each second with the current flow data such as:</p> <pre><code>2023/12/18 11:14:12 Metrics Response:\nchoice: flow_metrics\nflow_metrics:\n- bytes_rx: \"44032\"\n  bytes_tx: \"44032\"\n  frames_rx: \"86\"\n  frames_rx_rate: 9\n  frames_tx: \"86\"\n  frames_tx_rate: 9\n  name: r1.v4.r2\n  transmit: started\n</code></pre>"},{"location":"lab-examples/ixiacone-srl/#verification","title":"Verification","text":"<p>The test that we ran above will continuously keep checking flow metrics to ensure packet count received on rx port of ixia-c-one are as expected. If the condition is not met in 10 seconds, the test will timeout, hence indicating failure.</p> <p>Upon success, last flow metrics output will indicate the latest status with <code>transmit</code> set to <code>stopped</code>.</p> <pre><code>2023/12/18 11:14:13 Metrics Response:\nchoice: flow_metrics\nflow_metrics:\n- bytes_rx: \"51200\"\n  bytes_tx: \"51200\"\n  frames_rx: \"100\"\n  frames_rx_rate: 9\n  frames_tx: \"100\"\n  frames_tx_rate: 10\n  name: r1.v4.r2\n  transmit: stopped\n</code></pre>"},{"location":"lab-examples/ixiacone-srl/#cleanup","title":"Cleanup","text":"<p>To stop the lab, use:</p> <pre><code>sudo containerlab destroy --cleanup\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/lab-examples/","title":"Lab examples","text":"<p><code>containerlab</code> aims to provide a simple, intuitive and yet customizable way to run container based labs. To help our users get a glimpse on the features containerlab packages, we ship some essential lab topologies within the <code>containerlab</code> package.</p> <p>Note</p> <p>The lab examples featured on this site are very simple on purpose. They meant to show how to add different node kinds to the topology, but they were not designed to showcase a certain use case or lab scenario.</p> <p>For real-world lab use cases and scenarios, please refer to the <code>clab-topo</code> topic on GitHub, where the community shares their labs checked into GitHub repositories.</p> <p>We encourage the community to share their labs by adding the <code>clab-topo</code> topic to the repositories containing containerlab topology files; this way we can build a distributed, decentralized, and rich catalog of community-sourced labs for everyone's benefit!</p> <p>Some labs were contributed to clabs.netdevops.me web site along with the comprehensive description.</p> <p>These lab examples are meant to be used as-is or as a base layer to a more customized or elaborated lab scenarios. Once <code>containerlab</code> is installed, you will find the lab examples directories by the <code>/etc/containerlab/lab-examples</code> path.  Copy those directories over to your working directory to start using the provided labs.</p> <p>The source code of the lab examples is contained within the containerlab repo unless mentioned otherwise; any questions, issues or contributions related to the provided examples can be addressed via Github issues.</p> <p>Each lab comes with a definitive description that can be found in this documentation section.</p>"},{"location":"lab-examples/lab-examples/#how-to-deploy-a-lab-from-the-lab-catalog","title":"How to deploy a lab from the lab catalog?","text":"<p>Running the labs from the catalog is easy.</p>"},{"location":"lab-examples/lab-examples/#copy-lab-catalog","title":"Copy lab catalog","text":"<p>First, you need to copy the lab catalog to some place, for example to a current working directory. By copying labs from their original place we ensure that the changes we might make to the lab files will not be overwritten once we upgrade containerlab. To copy the entire catalog into your working directory:</p> <pre><code># copy over the srl02 lab files\ncp -a /etc/containerlab/lab-examples/* .\n</code></pre> <p>as a result of this command you will get several directories copied to the current working directory.</p> <p>Note</p> <p>Some big labs or community provided labs are typically stored in a separate git repository. To fetch those labs you will need to clone the lab' repo instead of copying the directories from <code>/etc/containerlab/lab-examples</code>.</p>"},{"location":"lab-examples/lab-examples/#get-the-lab-name","title":"Get the lab name","text":"<p>Every lab in the catalog has a unique short name. For example this lab states in the summary table that it's name is <code>srl02</code>. You will find a folder matching this name in your working directory, change into it:</p> <pre><code>cd srl02\n</code></pre>"},{"location":"lab-examples/lab-examples/#check-images-and-licenses","title":"Check images and licenses","text":"<p>Within the lab directory you will find the files that are used in the lab. Usually, only the topology definition file and, sometimes, config files are present in the lab directory.</p> <p>If you check the topology file you will see if any license files are required and what images are specified for each node/kind.</p> <p>Either change the topology file to point to the right image/license or change the image/license to match the topo definition file values.</p>"},{"location":"lab-examples/lab-examples/#deploy-the-lab","title":"Deploy the lab","text":"<p>You are ready to deploy!</p> <pre><code>containerlab deploy -t &lt;topo-file&gt;\n</code></pre>"},{"location":"lab-examples/lab-examples/#ssh-access","title":"SSH access","text":"<p>For nodes that come up with <code>ssh</code> enabled, the following lines can be added to the <code>~/.ssh/config</code> file on the containerlab host system to simplify access and prevent future ssh key warnings:</p> <pre><code>Host clab-*\n  User root\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n</code></pre>"},{"location":"lab-examples/min-5clos/","title":"5-stage Clos fabric","text":"Description A 5-stage CLOS topology based on Nokia SR Linux Components Nokia SR Linux Resource requirements<sup>1</sup>  4  8 GB Topology file clos02.clab.yml Name clos02"},{"location":"lab-examples/min-5clos/#description","title":"Description","text":"<p>This labs provides a lightweight folded 5-stage CLOS fabric with Super Spine level bridging two PODs.</p> <p>The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.</p>"},{"location":"lab-examples/min-5clos/#use-cases","title":"Use cases","text":"<p>With this lightweight CLOS topology a user can exhibit the following scenarios:</p> <ul> <li>perform configuration tasks applied to the 5-stage CLOS fabric</li> <li>demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves</li> </ul>"},{"location":"lab-examples/min-5clos/#configuration-setup","title":"Configuration setup","text":"<p>To help you get faster to the provisioning of the services on this mini fabric we added an auto-configuration script to this example.</p> <p>In order to make a fully deterministic lab setup we added another topology file called setup.clos02.clab.yml where the management interfaces of each network node and clients are statically addressed with <code>mgmt-ipv4/6</code> config option. Other than that, the topology files does not have any changes.</p>"},{"location":"lab-examples/min-5clos/#prerequisites","title":"Prerequisites","text":"<p>The configuration of the fabric elements is carried out with <code>gnmic</code> client, therefore it needs to be installed on the machine where you run the lab.</p>"},{"location":"lab-examples/min-5clos/#run-instructions","title":"Run instructions","text":"<p>First deploy this topology as per usual:</p> <pre><code>containerlab deploy -t setup.clos02.clab.yml\n</code></pre> <p>Once the lab is deployed, execute the configuration script:</p> <pre><code>bash setup.sh\n</code></pre>"},{"location":"lab-examples/min-5clos/#configuration-schema","title":"Configuration schema","text":"<p>The setup script will use the following IP addresses across the nodes of the lab:</p> <p>The script will configure the following:</p> <ol> <li>IP addresses for Management, System and Link interfaces of leaves and spines.</li> <li>IP addresses for Clients eth0 (Management) and eth1 interfaces.</li> <li>BGP, ISIS &amp; OSPF protocols.</li> </ol> <p>The following table outlines the addressing plan used in this lab:</p> Source Interface Towards IPv4 IPv6 leaf1 mgmt0.0 - <code>172.100.100.2/24</code> <code>3fff:172:100:100::2/64</code> system0.0 - <code>30.0.0.1/32</code> <code>3000:30:0:0::1/128</code> ethernet-1/1.0 spine1 <code>10.0.0.0/31</code> <code>1000:10:0:0::0/127</code> ethernet-1/2.0 spine2 <code>10.0.0.2/31</code> <code>1000:10:0:0::2/127</code> ethernet-1/3.0 client1 <code>10.0.0.24/31</code> <code>1000:10:0:0::24/127</code> leaf2 mgmt0.0 - <code>172.100.100.3/24</code> <code>3fff:172:100:100::3/64</code> system0.0 - <code>30.0.0.2/32</code> <code>3000:30:0:0::2/128</code> ethernet-1/1.0 spine1 <code>10.0.0.4/31</code> <code>1000:10:0:0::4/127</code> ethernet-1/2.0 spine2 <code>10.0.0.6/31</code> <code>1000:10:0:0::6/127</code> ethernet-1/3.0 client2 <code>10.0.0.26/31</code> <code>1000:10:0:0::26/127</code> leaf3 mgmt0.0 - <code>172.100.100.4/24</code> <code>3fff:172:100:100::4/64</code> system0.0 - <code>30.0.0.3/32</code> <code>3000:30:0:0::3/128</code> ethernet-1/1.0 spine3 <code>10.0.0.12/31</code> <code>1000:10:0:0::12/127</code> ethernet-1/2.0 spine4 <code>10.0.0.14/31</code> <code>1000:10:0:0::14/127</code> ethernet-1/3.0 client3 <code>10.0.0.28/31</code> <code>1000:10:0:0::28/127</code> leaf4 mgmt0.0 - <code>172.100.100.5/24</code> <code>3fff:172:100:100::5/64</code> system0.0 - <code>30.0.0.4/32</code> <code>3000:30:0:0::4/128</code> ethernet-1/1.0 spine3 <code>10.0.0.16/31</code> <code>1000:10:0:0::16/127</code> ethernet-1/2.0 spine4 <code>10.0.0.18/31</code> <code>1000:10:0:0::18/127</code> ethernet-1/3.0 client4 <code>10.0.0.30/31</code> <code>1000:10:0:0::30/127</code> spine1 mgmt0.0 - <code>172.100.100.6/24</code> <code>3fff:172:100:100::6/64</code> system0.0 - <code>30.0.0.5/32</code> <code>3000:30:0:0::5/128</code> ethernet-1/1.0 leaf1 <code>10.0.0.1/31</code> <code>1000:10:0:0::1/127</code> ethernet-1/2.0 leaf2 <code>10.0.0.5/31</code> <code>1000:10:0:0::5/127</code> ethernet-1/3.0 superspine1 <code>10.0.0.8/31</code> <code>1000:10:0:0::8/127</code> spine2 mgmt0.0 - <code>172.100.100.7/24</code> <code>3fff:172:100:100::7/64</code> system0.0 - <code>30.0.0.6/32</code> <code>3000:30:0:0::6/128</code> ethernet-1/1.0 leaf1 <code>10.0.0.3/31</code> <code>1000:10:0:0::3/127</code> ethernet-1/2.0 leaf2 <code>10.0.0.7/31</code> <code>1000:10:0:0::7/127</code> ethernet-1/3.0 superspine2 <code>10.0.0.10/31</code> <code>1000:10:0:0::10/127</code> spine3 mgmt0.0 - <code>172.100.100.8/24</code> <code>3fff:172:100:100::8/64</code> system0.0 - <code>30.0.0.7/32</code> <code>3000:30:0:0::7/128</code> ethernet-1/1.0 leaf3 <code>10.0.0.13/31</code> <code>1000:10:0:0::13/127</code> ethernet-1/2.0 leaf4 <code>10.0.0.17/31</code> <code>1000:10:0:0::17/127</code> ethernet-1/3.0 superspine1 <code>10.0.0.20/31</code> <code>1000:10:0:0::20/127</code> spine4 mgmt0.0 - <code>172.100.100.9/24</code> <code>3fff:172:100:100::9/64</code> system0.0 - <code>30.0.0.8/32</code> <code>3000:30:0:0::8/128</code> ethernet-1/1.0 leaf3 <code>10.0.0.15/31</code> <code>1000:10:0:0::15/127</code> ethernet-1/2.0 leaf4 <code>10.0.0.19/31</code> <code>1000:10:0:0::19/127</code> ethernet-1/3.0 superspine2 <code>10.0.0.22/31</code> <code>1000:10:0:0::22/127</code> superspine1 mgmt0.0 - <code>172.100.100.10/24</code> <code>3fff:172:100:100::10/64</code> system0.0 - <code>30.0.0.9/32</code> <code>3000:30:0:0::9/128</code> ethernet-1/1.0 spine1 <code>10.0.0.9/31</code> <code>1000:10:0:0::9/127</code> ethernet-1/2.0 spine3 <code>10.0.0.21/31</code> <code>1000:10:0:0::21/127</code> superspine2 mgmt0.0 - <code>172.100.100.11/24</code> <code>3fff:172:100:100::11/64</code> system0.0 - <code>30.0.0.10/32</code> <code>3000:30:0:0::10/128</code> ethernet-1/1.0 spine2 <code>10.0.0.11/31</code> <code>1000:10:0:0::11/127</code> ethernet-1/2.0 spine4 <code>10.0.0.23/31</code> <code>1000:10:0:0::23/127</code> client1 eth0 - <code>172.100.100.12/24</code> <code>3fff:172:100:100::12/64</code> eth1 leaf1 <code>10.0.0.25/31</code> <code>1000:10:0:0::25/127</code> client2 eth0 - <code>172.100.100.13/24</code> <code>3fff:172:100:100::13/64</code> eth1 leaf2 <code>10.0.0.27/31</code> <code>1000:10:0:0::27/127</code> client3 eth0 - <code>172.100.100.14/24</code> <code>3fff:172:100:100::14/64</code> eth1 leaf3 <code>10.0.0.29/31</code> <code>1000:10:0:0::29/127</code> client4 eth0 - <code>172.100.100.15/24</code> <code>3fff:172:100:100::15/64</code> eth1 leaf4 <code>10.0.0.31/31</code> <code>1000:10:0:0::31/127</code> <p>Configuration snippets that are used to provision the nodes are contained within the <code>configs</code> subdirectory.</p> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/min-clos/","title":"3-nodes Clos fabric","text":"Description A minimal CLOS topology with two leafs and a spine Components Nokia SR Linux Resource requirements<sup>1</sup>  2  3 GB Topology file clos01.clab.yml Name clos01"},{"location":"lab-examples/min-clos/#description","title":"Description","text":"<p>This labs provides a lightweight folded CLOS fabric topology using a minimal set of nodes: two leaves and a single spine.</p> <p></p> <p>The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.</p>"},{"location":"lab-examples/min-clos/#use-cases","title":"Use cases","text":"<p>With this lightweight CLOS topology a user can exhibit the following scenarios:</p> <ul> <li>perform configuration tasks applied to the 3-stage CLOS fabric</li> <li>demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/multinode/","title":"Multi-node labs","text":"Description A lab demonstrating multi-node (multi-vm) capabilities Components Nokia SR OS, Juniper vMX Resource requirements<sup>1</sup>  2  6 GB per node Topology file vxlan-vmx.clab.yml, vxlan-sros.clab.yml Name vxlan01 Version information<sup>2</sup> <code>containerlab:0.11.0</code>, <code>vr-sros:20.2.R1</code>, <code>vr-vmx:20.4R1.12</code>, <code>docker-ce:20.10.2</code>"},{"location":"lab-examples/multinode/#description","title":"Description","text":"<p>This lab demonstrates how containerlab can deploy labs on different machines and stitch the interfaces of the running nodes via VxLAN tunnels.</p> <p>With such approach users are allowed to spread the load between multiple VMs and still have the nodes connected via p2p links as if they were sitting on the same virtual machine.</p> <p>For the sake of the demonstration the topology used in this lab consists of just two virtualized routers packaged in a container format - Nokia SR OS and Juniper vMX. Although the routers are running on different VMs, they logically form a back-to-back connection over a pair of interfaces aggregated in a logical bundle.</p> <p>Upon succesful lab deployment and configuration, the routers will be able to exchange LACP frames, thus proving a transparent L2 connectivity and will be able to ping each other.</p>"},{"location":"lab-examples/multinode/#deployment","title":"Deployment","text":"<p>Since this lab is of a multi-node nature, a user needs to have two machines/VMs and perform lab deployment process on each of them. The lab directory has topology files named <code>vxlan-sros.clab.yml</code> and <code>vxlan-vmx.clab.yml</code> which are meant to be deployed on VM1 and VM2 accordingly.</p> <p>The following command will deploy a lab on a specified host:</p> VM1 (SROS)VM2 (VMX) <pre><code>clab dep -t vxlan-sros.clab.yml\n</code></pre> <pre><code>clab dep -t vxlan-vmx.clab.yml\n</code></pre>"},{"location":"lab-examples/multinode/#host-links","title":"host links","text":"<p>Both topology files leverage host link feature which allows a container to have its interface to be connected to a container host namespace. Once the topology is created you will have one side of the veth link visible in the root namespace by the names specified in topo file. For example, <code>vxlan-sros.clab.yml</code> file has the following <code>links</code> section:</p> <pre><code>  links:\n    # we expose two sros container interfaces\n    # to host namespace by using host interfaces style\n    # docs: https://containerlab.dev/manual/network/#host-links\n    - endpoints: [\"sros:eth1\", \"host:sr-eth1\"]\n    - endpoints: [\"sros:eth2\", \"host:sr-eth2\"]\n</code></pre> <p>This will effectively make two veth pairs. Let us consider the first veth pair where one end of a it will be placed inside the container' namespace and named <code>eth1</code>, the other end will stay in the container host root namespace and will be named <code>sros-eth1</code>.  </p> <p>Same picture will be on VM2 with vMX interfaces exposed to a container host.</p> verify host link VM1VM2 <pre><code>\u276f ip l | grep sros-eth\n622: sr-eth1@if623: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default \n624: sr-eth2@if625: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default \n</code></pre> <pre><code>\u276f ip l | grep vmx-eth\n1982: vmx-eth1@if1983: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n1984: vmx-eth2@if1985: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n</code></pre>"},{"location":"lab-examples/multinode/#vxlan-tunneling","title":"vxlan tunneling","text":"<p>At this moment there is no connectivity between the routers, as the datapath is not ready. What we need to add is the VxLAN tunnels that will stitch SR OS container with vMX.</p> <p>We do this by provisioning VxLAN tunnels that will stitch the interfaces of our routers.</p> <p>Logically we make our interface appear to be connected in a point-to-point fashion. To make these tunnels we leverage containerlab' <code>tools vxlan create</code> command, that will create the VxLAN tunnel and the necessary redirection rules to forward traffic back-and-forth to a relevant host interface.</p> <p>All we need is to provide the VMs address and choose VNI numbers. And do this on both hosts.</p> VM1VM2 <pre><code>\u276f clab tools vxlan create --remote 10.0.0.20 --id 10 --link sr-eth1\n\n\u276f clab tools vxlan create --remote 10.0.0.20 --id 20 --link sr-eth2\n</code></pre> <pre><code>\u276f clab tools vxlan create --remote 10.0.0.18 --id 10 --link vmx-eth1\n\n\u276f clab tools vxlan create --remote 10.0.0.18 --id 20 --link vmx-eth2\n</code></pre> <p>The above set of commands will create the necessary VxLAN tunnels and the datapath is ready.</p> <p>At this moment, the connectivity diagrams becomes complete and can be depicted as follows:</p>"},{"location":"lab-examples/multinode/#configuration","title":"Configuration","text":"<p>Once the datapath is in place, we proceed with the configuration of a simple LACP use case, where both SR OS and vMX routers have their pair of interfaces aggregated into a LAG and form an LACP neighborship.</p> SR OSvMX <pre><code>configure lag \"lag-aggr\" admin-state enable\nconfigure lag \"lag-aggr\" mode hybrid\nconfigure lag \"lag-aggr\" lacp mode active\nconfigure lag \"lag-aggr\" port 1/1/c1/1\nconfigure lag \"lag-aggr\" port 1/1/c2/1\n\nconfigure port 1/1/c1 admin-state enable\nconfigure port 1/1/c1 connector breakout c1-100g\nconfigure port 1/1/c1/1 admin-state enable\nconfigure port 1/1/c1/1 ethernet\nconfigure port 1/1/c1/1 ethernet mode hybrid\n\nconfigure port 1/1/c2 admin-state enable\nconfigure port 1/1/c2 connector breakout c1-100g\nconfigure port 1/1/c2/1 admin-state enable\nconfigure port 1/1/c2/1 ethernet mode hybrid\n\nconfigure router \"Base\" interface \"toVMX\" port lag-aggr:0\nconfigure router \"Base\" interface \"toVMX\" ipv4 primary address 192.168.1.1 prefix-length 24\n</code></pre> <pre><code>set interfaces ge-0/0/0 gigether-options 802.3ad ae0\nset interfaces ge-0/0/1 gigether-options 802.3ad ae0\nset interfaces ae0 aggregated-ether-options minimum-links 1\nset interfaces ae0 aggregated-ether-options link-speed 1g\nset interfaces ae0 aggregated-ether-options lacp active\nset interfaces ae0 unit 0 family inet address 192.168.1.2/24\n</code></pre>"},{"location":"lab-examples/multinode/#verification","title":"Verification","text":"<p>To verify that LACP protocol works the following commands can be issued on both routers to display information about the aggregated interface and LACP status:</p> SR OSvMX <pre><code># verifying operational status of LAG interface\nA:admin@sros# show lag \"lag-aggr\"\n\n===============================================================================\nLag Data\n===============================================================================\nLag-id         Adm     Opr     Weighted Threshold Up-Count MC Act/Stdby\n    name\n-------------------------------------------------------------------------------\n65             up      up      No       0         2        N/A\n    lag-aggr\n===============================================================================\n\n# show LACP statistics. Both incoming and trasmitted counters will increase\nA:admin@sros# show lag \"lag-aggr\" lacp-statistics\n\n===============================================================================\nLAG LACP Statistics\n===============================================================================\nLAG-id    Port-id        Tx         Rx         Rx Error   Rx Illegal\n                        (Pdus)     (Pdus)     (Pdus)     (Pdus)\n-------------------------------------------------------------------------------\n65        1/1/c1/1       78642      77394      0          0\n65        1/1/c2/1       78644      77396      0          0\n-------------------------------------------------------------------------------\nTotals                   157286     154790     0          0\n===============================================================================\n</code></pre> <pre><code>admin@vmx&gt; show interfaces ae0 brief\nPhysical interface: ae0, Enabled, Physical link is Up\nLink-level type: Ethernet, MTU: 1514, Speed: 2Gbps, Loopback: Disabled, Source filtering: Disabled, Flow control: Disabled\nDevice flags   : Present Running\nInterface flags: SNMP-Traps Internal: 0x4000\n\nLogical interface ae0.0\n    Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2\n    inet  192.168.1.2/24\n    multiservice\n\n\nadmin@vmx&gt; show lacp interfaces\nAggregated interface: ae0\n    LACP state:       Role   Exp   Def  Dist  Col  Syn  Aggr  Timeout  Activity\n    ge-0/0/0       Actor    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    ge-0/0/0     Partner    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    ge-0/0/1       Actor    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    ge-0/0/1     Partner    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    LACP protocol:        Receive State  Transmit State          Mux State\n    ge-0/0/0                  Current   Fast periodic Collecting distributing\n    ge-0/0/1                  Current   Fast periodic Collecting distributing\n\nadmin@vmx&gt; show lacp statistics interfaces ae0\nAggregated interface: ae0\n    LACP Statistics:       LACP Rx     LACP Tx   Unknown Rx   Illegal Rx\n    ge-0/0/0               78104       77469            0            0\n    ge-0/0/1               78106       77471            0            0\n</code></pre> <p>After the control plane verfification let's verify that the dataplane is working by pinging the IP address of the remote interface (issued from SR OS node in the example):</p> <pre><code>A:admin@sros# ping 192.168.1.2\nPING 192.168.1.2 56 data bytes\n64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=13.5ms.\n64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=2.61ms.\nping aborted by user\n\n---- 192.168.1.2 PING Statistics ----\n2 packets transmitted, 2 packets received, 0.00% packet loss\nround-trip min = 2.61ms, avg = 8.04ms, max = 13.5ms, stddev = 0.000ms\n</code></pre> <p>Great! Additionally users can capture the traffic from any of the interfaces involved in the datapath. To see the VxLAN encapsulation the VM's outgoing interfaces should be used.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/openbsd01/","title":"OpenBSD","text":"Description A OpenBSD connected to two Alpine Linux Hosts Components OpenBSD, Multitool Alpine Linux Resource requirements<sup>1</sup>  1  512 MB Topology file openbsd01.yml Name openbsd01 Version information<sup>2</sup> <code>containerlab:0.49.0</code>, <code>openbsd-7.3-2023-04-22.qcow2</code>, <code>docker:24.0.6</code>"},{"location":"lab-examples/openbsd01/#description","title":"Description","text":"<p>This lab consists of one OpenBSD router connected to two Alpine Linux nodes.</p> <pre><code>client1&lt;----&gt;OpenBSD&lt;----&gt;client2\n</code></pre>"},{"location":"lab-examples/openbsd01/#configuration","title":"Configuration","text":"<p>The OpenBSD node takes about 1 minute to complete its start up. Check using \"docker container ls\" until the OpenBSD container shows up as \"healthy\".</p> <pre><code># docker container ls\nCONTAINER ID   IMAGE                                  COMMAND                  CREATED              STATUS                        PORTS                                       NAMES\n6fee243af470   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   About a minute ago   Up About a minute             80/tcp, 443/tcp, 1180/tcp, 11443/tcp        clab-openbsd01-client1\n46800cd24290   vrnetlab/vr-openbsd:7.3                \"/launch.py --userna\u2026\"   About a minute ago   Up About a minute (healthy)   22/tcp, 5000/tcp, 10000-10099/tcp           clab-openbsd01-obsd1\nce53649d8741   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   About a minute ago   Up About a minute             80/tcp, 443/tcp, 1180/tcp, 11443/tcp        clab-openbsd01-client2\n</code></pre>"},{"location":"lab-examples/openbsd01/#obsd01","title":"obsd01","text":"<p>Log into the OpenBSD node using SSH with <code>ssh admin@clab-openbsd01-obsd1</code> and add the following configuration. Password is <code>admin</code>.</p> <pre><code>sudo sysctl net.inet.ip.forwarding=1\nsudo ifconfig vio1 192.168.1.1/30\nsudo ifconfig vio2 192.168.2.1/30\n</code></pre>"},{"location":"lab-examples/openbsd01/#client1","title":"client1","text":"<p>The two clients should be configured with the correct IP addresses and a route to the other client via the OpenBSD node. First attach to the container process <code>docker exec -it clab-openbsd01-client1 ash</code></p> <pre><code>docker exec -it clab-openbsd01-client1 ash\n\n# ip -br a show dev eth1\neth1@if3428      UP             192.168.1.2/30 fe80::a8c1:abff:fefd:9fcf/64\n\n# ip r\ndefault via 172.20.20.1 dev eth0\n172.20.20.0/24 dev eth0 proto kernel scope link src 172.20.20.3\n192.168.1.0/30 dev eth1 proto kernel scope link src 192.168.1.2\n192.168.2.0/30 via 192.168.1.1 dev eth1\n</code></pre>"},{"location":"lab-examples/openbsd01/#verification","title":"Verification","text":"<p>Traceroute from client1 to client2 to verify the data-plane via the OpenBSD node.</p>"},{"location":"lab-examples/openbsd01/#client1_1","title":"client1","text":"<pre><code># traceroute 192.168.2.2\ntraceroute to 192.168.2.2 (192.168.2.2), 30 hops max, 46 byte packets\n 1  192.168.1.1 (192.168.1.1)  0.874 ms  0.484 ms  0.151 ms\n 2  192.168.2.2 (192.168.2.2)  0.240 ms  0.182 ms  0.148 ms\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/ost-srl/","title":"Ostinato and Nokia SR Linux","text":"Description Ostinato traffic generator connected with Nokia SR Linux Components Ostinato, Nokia SR Linux Resource requirements<sup>1</sup>  2  4 GB Topology file ost-srl.clab.yaml Name ost-srl Version information<sup>2</sup> <code>containerlab:0.55.1</code>, <code>ostinato:v1.3.0-1</code>, <code>srlinux:24.3.2</code>, <code>docker-ce:26.0.0</code>"},{"location":"lab-examples/ost-srl/#description","title":"Description","text":"<p>This lab consists of a Ostinato node with 2 ports connected to 2 ports on a Nokia SR Linux node via two point-to-point ethernet links. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p> <p>Ostinato is a software based network packet traffic generator managed via a GUI or a Python script using the Ostinato API. This example will demonstrate how to use the Ostinato GUI included with the Ostinato for Containerlab image to verify IPv4 forwarding.</p>"},{"location":"lab-examples/ost-srl/#deployment","title":"Deployment","text":"<p>Change into the lab directory:</p> <pre><code>cd containerlab/lab-examples/ost-srl\n</code></pre> <p>Deploy the lab:</p> <pre><code>sudo containerlab deploy\n</code></pre>"},{"location":"lab-examples/ost-srl/#use-cases","title":"Use cases","text":"<p>This lab allows users to validate IPv4 forwarding on Nokia SR Linux (the DUT) using Ostinato as the traffic generator.</p>"},{"location":"lab-examples/ost-srl/#ipv4-traffic-forwarding","title":"IPv4 Traffic forwarding","text":"<p>This lab demonstrates a simple IPv4 traffic forwarding scenario where</p> <ul> <li>Ostinato with two test ports <code>eth1</code> and <code>eth2</code> connected to Nokia SR Linux ports <code>e1-1</code> and <code>e1-2</code> respectively.</li> <li>SR Linux is the DUT and its interfaces <code>e1-1</code> and <code>e1-2</code> are configured with IPv4 addresses <code>10.0.0.1/24</code> and <code>20.0.0.1/24</code> respectively.</li> <li>Ostinato will emulate host <code>10.0.0.100</code> on <code>eth1</code> and <code>20.0.0.100</code> on <code>eth2</code></li> <li>Ostinato will send bidirectional traffic at 100pps (<code>eth1</code> \u2192 <code>e1-1</code>) and 200pps (<code>eth2</code> \u2192 <code>e1-2</code>)</li> <li>The TX and RX traffic rates can be verified in the Port Stats window of the Ostinato GUI</li> </ul>"},{"location":"lab-examples/ost-srl/#configuration","title":"Configuration","text":"<p>During the lab deployment and test execution the following configuration is applied to the lab nodes to forward and receive traffic.</p> <ul> <li> <p>SR Linux     SR Linux node comes up pre-configured with the commands listed in srl.cfg file which configure IPv4 addresses on both interfaces.</p> </li> <li> <p>Ostinato     Ostinato configuration is saved in the ost-srl.ossn file which will configure the emulated hosts and traffic streams. The configuration file needs to be loaded manually as explained in the next section</p> </li> </ul> <p>Warning</p> <p>All Ostinato stream and session files are NOT in text (or human readable) format but a binary format read and written by Ostinato</p>"},{"location":"lab-examples/ost-srl/#execution","title":"Execution","text":"<ol> <li>Access the Ostinato GUI by connecting a VNC client to <code>&lt;host-ip&gt;:5900</code></li> <li>Once the GUI opens and the <code>eth1</code>, <code>eth2</code> ports are listed, go to <code>File | Open Session</code> and open <code>/root/shared/ost-srl.ossn</code></li> <li>In the Ostinato Port Stats window, select both <code>eth1</code> and <code>eth2</code> port columns and click on  (start transmit)</li> </ol>"},{"location":"lab-examples/ost-srl/#verification","title":"Verification","text":"<ol> <li>Verify the port stats and rates in the same window</li> <li>Keeping both <code>eth1</code> and <code>eth2</code> selected, click on  (stop transmit)</li> </ol> <p>Here's a short video showing the above steps -</p>"},{"location":"lab-examples/ost-srl/#next-steps","title":"Next Steps","text":"<p>You can edit the Ostinato emulated devices and streams to experiment further.</p> <p>Here are some suggestions -</p> <ul> <li>Override the IPv4 checksum to an invalid value and verify the DUT silently discards the packets (some DUTs may send an ICMP parameter problem)</li> <li>Try setting IPv4 TTL to 1 and verify<ul> <li>traffic is not forwarded by the DUT</li> <li>DUT sends back ICMP TTL Exceeded (you can capture and view captured packets)</li> </ul> </li> <li>Try IPv6 traffic streams</li> </ul> <p>Learn more about Ostinato traffic streams.</p>"},{"location":"lab-examples/ost-srl/#cleanup","title":"Cleanup","text":"<p>To stop the lab, use:</p> <pre><code>sudo containerlab destroy --cleanup\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/peering-lab/","title":"Peering lab","text":"<p>Internet eXchange Points are the glue that connects the Internet. They are the physical locations where ISPs, CDNs and all other ASN holders connect to exchange traffic. While traffic exchange might sound simple, it is a complex process with lots of moving parts:</p> <ul> <li>Peering routers configuration.</li> <li>Route Servers configuration.</li> <li>Route filtering.</li> <li>MANRS compliance.</li> <li>RPKI validation.</li> <li>IXP services enablement.</li> </ul> <p>Each of these topics is a whole body of knowledge on its own and various Internet exchange consortiums have published best practices and guidelines to help IXP operators and their members to configure their networks properly.</p> <p>The guidelines and current best practices are best to be reinforced in a lab environment. And with this thought in mind, we present containerlab users with this hands-on lab simulating an IXP with Route Servers and peering members.</p>"},{"location":"lab-examples/peering-lab/#lab-summary","title":"Lab summary","text":"Summary Lab name Peering Lab Lab components Nokia SR OS, FRRouting (FRR), OpenBGPd and BIRD route servers Resource requirements  2 vCPU  6 GB Lab hellt/sros-frr-ixp-lab Version information <code>containerlab:0.41.1</code>, <code>Nokia SR OS:23.3.R1</code>, <code>FRR:8.4.1</code>, <code>BIRD:2.13</code>, <code>openbgpd:7.9</code> Authors Roman Dodin"},{"location":"lab-examples/peering-lab/#prerequisites","title":"Prerequisites","text":"<p>Since containerlab uses containers as the nodes of a lab, the Docker engine has to be installed on the host system first.</p>"},{"location":"lab-examples/peering-lab/#lab-topology","title":"Lab topology","text":"<p>The lab topology used in this lab needs to be flexible enough to practice various IXP scenarios, yet simple enough to be easily understood and configured. The following topology was chosen for this lab:</p> <p></p> <p>On a physical level, the lab topology consists of two IXP members running Nokia SR OS and FRR network OSes accordingly. Each peer has a single interface connected to the IXP network (aka Peering LAN) to peer with other members and exchange traffic.</p> <p>Our IXP lab also features a redundant pair of Route Servers powered by two most commonly used open-source routing daemons: OpenBGPd and BIRD. The Route Servers are connected to the IXP network via a dedicated <code>eth1</code> interface.</p> <p>From the control-plane perspective, two peers establish eBGP sessions with the Route Servers and announce their networks. To keep things simple, each peer announces only one network, namely peer1 announces <code>10.0.0.1/32</code> and peer2 - <code>10.0.0.2/32</code>.</p> <p></p> <p>The Route Servers receive NLRIs from peers and pass them over to the other IXP members.</p>"},{"location":"lab-examples/peering-lab/#obtaining-container-images","title":"Obtaining container images","text":"<p>Every component of this lab is openly available and can be downloaded from public repositories, but Nokia SR OS, which has to be obtained from Nokia representatives. In this lab SR OS container image name is <code>sros:23.3.R1</code>.</p>"},{"location":"lab-examples/peering-lab/#topology-definition","title":"Topology definition","text":"<p>The topology definition file for this lab is available in the lab repository. The topology file - <code>ixp.clab.yml</code> - declaratively describes the lab topology and is used by containerlab to create the lab environment.</p>"},{"location":"lab-examples/peering-lab/#peers","title":"Peers","text":"<p>The two IXP members are defined as follows:</p> <pre><code>topology:\n  nodes:\n    peer1:\n      kind: vr-nokia_sros\n      image: sros:23.3.R1 #(1)!\n      license: license.key\n      startup-config: configs/sros.partial.cfg\n\n    peer2:\n      kind: linux\n      image: quay.io/frrouting/frr:8.4.1\n      binds:\n        - configs/frr.conf:/etc/frr/frr.conf\n        - configs/frr-daemons.cfg:/etc/frr/daemons\n</code></pre> <ol> <li>SR OS container has to be requested from Nokia or built manually from qcow2 disk image using <code>hellt/vrnetlab</code> project as explained here.</li> </ol> <p>Apart from typical containerlab node definitions statements like <code>kind</code> and <code>image</code>, for SR OS we leverage the <code>vr-nokia_sros</code> kind, <code>license</code> and <code>startup-config</code> keys to provide the SR OS container with the license key and the startup configuration file respectively. Check out Basic configuration section for more details on the contents of startup-configuration files for each topology member.</p> <p>For the FRR, which uses the public official container image, node we leverage the <code>binds</code> key to mount the FRR configuration file and the FRR daemon configuration file into the container. Again, the contents of these files are explained in the Basic configuration section.</p>"},{"location":"lab-examples/peering-lab/#route-servers","title":"Route Servers","text":"<p>Both route servers are based on the <code>linux</code> kind which represents a regular linux container:</p> <pre><code>rs1: # OpenBGPd route server\n  kind: linux\n  image: quay.io/openbgpd/openbgpd:7.9\n  binds:\n    - configs/openbgpd.conf:/etc/bgpd/bgpd.conf\n  exec:\n    - \"ip address add dev eth1 192.168.0.3/24\"\n\nrs2: # BIRD route server\n  kind: linux\n  image: ghcr.io/srl-labs/bird:2.13\n  binds:\n    - configs/bird.conf:/etc/bird.conf\n  exec:\n    - \"ip address add dev eth1 192.168.0.4/24\"\n</code></pre> <p>OpenBGPd server uses an official container image and mounts the OpenBGPd configuration file into the container.</p> <p>BIRD doesn't have an official container image, so we created a BIRD v2.13 container image published at ghcr.io/srl-labs/bird<sup>1</sup> and also mount the BIRD configuration file into the container.</p>"},{"location":"lab-examples/peering-lab/#peering-lan-and-links","title":"Peering LAN and links","text":"<p>At the geographical center of our topology lies the Peering LAN, which is represented by the <code>ixp-net</code> node that uses the <code>bridge</code> kind.</p> <pre><code>ixp-net:\n  kind: bridge\n</code></pre> <p>As per the kind's documentation, the <code>bridge</code> kind uses an underlying linux bridge and allows topology nodes to connect their interface to the bridge. Note, that the linux bridge with the matching name (which is <code>ixp-net</code> in our case) must be created on the host before containerlab starts the lab environment.</p> <p>Finally, we define the links between the nodes. As per our topology, each peer and each route server has a single interface connected to the Peering LAN. The links are defined as follows:</p> <pre><code>links:\n  - endpoints: [\"peer1:eth1\", \"ixp-net:port1\"]\n  - endpoints: [\"peer2:eth1\", \"ixp-net:port2\"]\n  - endpoints: [\"rs1:eth1\", \"ixp-net:port3\"]\n  - endpoints: [\"rs2:eth1\", \"ixp-net:port4\"]\n</code></pre> <p>With the links defined, our connectivity diagram looks like this:</p> <p></p>"},{"location":"lab-examples/peering-lab/#basic-configuration","title":"Basic configuration","text":"<p>Using <code>startup-config</code> and <code>binds</code> configuration setting every node in our lab is equipped with the startup configuration that makes it possible to deploy a functioning IXP environment.</p> <p>The basic configuration that is captured in the startup configuration files contains the following:</p> <ul> <li>basic interface configuration for each node</li> <li>basic BGP configuration for the peer nodes to enable peering with the route servers</li> <li>basic import/export policies on the peer's side</li> <li>basic Route Server configuration with no filtering or route validation</li> </ul>"},{"location":"lab-examples/peering-lab/#sr-os","title":"SR OS","text":"<p>Nokia SR OS startup configuration file is provided in the form of a CLI-styled configuration blob that is captured in the sros-partial.cfg. The statements in this config file are applied when the node is started and ready to accept CLI commands.</p> <p>Tip</p> <p>Throughout this lab we will introduce and explain different BGP features and provide the relevant configuration snippets. As it takes time to put in writing all the features, ref links, and stories around them, we created a configuration cheat sheet that contains a condensed version of the BGP peering configuration snippets for SR OS. The cheat sheet is available at sajusal/sros-peering repository and should help you to get started with the lab in a self-exploration mode.</p>"},{"location":"lab-examples/peering-lab/#frr","title":"FRR","text":"<p>FRR configuration is split between the two files:</p> <ul> <li>frr.conf - contains the basic FRR configuration, which includes the most simple BGP configuration to enable peering.</li> <li>daemons.cfg - contains the list of FRR daemons to be started</li> </ul>"},{"location":"lab-examples/peering-lab/#route-servers_1","title":"Route Servers","text":"<p>Router servers are no different and equipped with the basic route server configuration that enables unfiltered peering. The configuration files are bind mounted to each respective container as follows:</p> <ul> <li>OpenBGPd - bgpd.conf</li> <li>BIRD - bird.conf</li> </ul>"},{"location":"lab-examples/peering-lab/#lab-lifecycle","title":"Lab lifecycle","text":""},{"location":"lab-examples/peering-lab/#deploying-the-lab","title":"Deploying the lab","text":"<p>Now that we have the topology and the configuration files in place, we can deploy the lab environment. To do so, we use the <code>containerlab deploy</code> command:</p> <pre><code>containerlab deploy --topo ixp.clab.yaml\n</code></pre> <p></p> <p>Deployment time depends on the host machine specs but roughly may take 2 to 5 minutes. Of which 3 seconds is spent on actual deployment and the rest is spent on waiting for SR OS VM to boot up and accept SSH connections.</p> <p>Upon successful deployment, containerlab presents the lab summary table that contains the information about the deployed nodes:</p> <pre><code>+---+----------------+--------------+-------------------------------+---------------+---------+----------------+----------------------+\n| # |      Name      | Container ID |             Image             |     Kind      |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+----------------+--------------+-------------------------------+---------------+---------+----------------+----------------------+\n| 1 | clab-ixp-peer1 | c9f5301899fb | sros:23.3.R1                  | vr-nokia_sros | running | 172.20.20.5/24 | 3fff:172:20:20::5/64 |\n| 2 | clab-ixp-peer2 | 83da54ce9f7b | quay.io/frrouting/frr:8.4.1   | linux         | running | 172.20.20.3/24 | 3fff:172:20:20::3/64 |\n| 3 | clab-ixp-rs1   | 701ee906f03f | quay.io/openbgpd/openbgpd:7.9 | linux         | running | 172.20.20.4/24 | 3fff:172:20:20::4/64 |\n| 4 | clab-ixp-rs2   | 7de1a2f30d52 | ghcr.io/srl-labs/bird:2.13    | linux         | running | 172.20.20.2/24 | 3fff:172:20:20::2/64 |\n+---+----------------+--------------+-------------------------------+---------------+---------+----------------+----------------------+\n</code></pre> <p>This table contains vital information about the deployed nodes, such as the name, container ID, image, kind, state, IPv4 and IPv6 addresses. The names and IP addresses can be used to connect to the nodes via SSH if the node happens to run an SSH server.</p>"},{"location":"lab-examples/peering-lab/#inspecting-the-lab","title":"Inspecting the lab","text":"<p>At any point in time, containerlab users can refresh themselves on what is currently deployed in the lab environment by using the <code>containerlab inspect</code> command:</p> <pre><code>$ containerlab inspect --all\n+---+--------------+----------+----------------+--------------+-------------------------------+---------------+---------+----------------+----------------------+\n| # |  Topo Path   | Lab Name |      Name      | Container ID |             Image             |     Kind      |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+--------------+----------+----------------+--------------+-------------------------------+---------------+---------+----------------+----------------------+\n| 1 | ixp.clab.yml | ixp      | clab-ixp-peer1 | c9f5301899fb | sros:23.3.R1                  | vr-nokia_sros | running | 172.20.20.5/24 | 3fff:172:20:20::5/64 |\n| 2 |              |          | clab-ixp-peer2 | 83da54ce9f7b | quay.io/frrouting/frr:8.4.1   | linux         | running | 172.20.20.3/24 | 3fff:172:20:20::3/64 |\n| 3 |              |          | clab-ixp-rs1   | 701ee906f03f | quay.io/openbgpd/openbgpd:7.9 | linux         | running | 172.20.20.4/24 | 3fff:172:20:20::4/64 |\n| 4 |              |          | clab-ixp-rs2   | 7de1a2f30d52 | ghcr.io/srl-labs/bird:2.13    | linux         | running | 172.20.20.2/24 | 3fff:172:20:20::2/64 |\n+---+--------------+----------+----------------+--------------+-------------------------------+---------------+---------+----------------+----------------------+\n</code></pre>"},{"location":"lab-examples/peering-lab/#destroying-the-lab","title":"Destroying the lab","text":"<p>One of containerlab principles is treating labs in a cattle-not-pets manner. This means that labs are ephemeral and can be destroyed at any point in time. To do so, we use the <code>containerlab destroy</code> command:</p> <pre><code>containerlab destroy --cleanup -t ixp.clab.yml #(1)!\n</code></pre> <ol> <li>The <code>--cleanup</code> flag instructs containerlab to remove the lab directory that contains the lab configuration files.</li> </ol>"},{"location":"lab-examples/peering-lab/#accessing-the-lab-nodes","title":"Accessing the lab nodes","text":"<p>To connect to the nodes of a running lab users can either use <code>ssh</code> client and the node name or IP address that containerlab assigned, or use the <code>docker exec</code> command. Typically the latter is used to connect to the nodes that do not run an SSH server, such as nodes <code>peer2</code>, <code>rs1</code> and <code>rs2</code>.</p> <p>Nokia SR OS node runs an SSH server, therefore connecting to it is as simple as:</p> connecting to Nokia SR OS node<pre><code>$ ssh admin@clab-ixp-peer1 #(1)!\nadmin@clab-ixp-peer1's password: \n\nSR OS Software\nCopyright (c) Nokia 2023.  All Rights Reserved.\n\n[/]\nA:admin@peer1#\n</code></pre> <ol> <li>The <code>admin</code> user is the default user for the Nokia SR OS node. Password is <code>admin</code>. <code>clab-ixp-peer1</code> is the name of the node as defined in the lab topology file with a lab prefix prepended to it.</li> </ol> <p>For nodes without an SSH server running, we use the <code>docker exec</code> command to execute a process that runs in the context of the container and provides us with a shell:</p> connecting to FRRouting <code>vtysh</code><pre><code>\u276f docker exec -it clab-ixp-peer2 vtysh\n% Can't open configuration file /etc/frr/vtysh.conf due to 'No such file or directory'.\n\nHello, this is FRRouting (version 8.4.1_git).\nCopyright 1996-2005 Kunihiro Ishiguro, et al.\n\npeer2#\n</code></pre> <p>The same connection method is used to connect to BIRD's <code>birdc</code> shell and OpenBGPd's <code>ash</code> shell:</p> <ul> <li><code>docker exec -it clab-ixp-rs1 ash</code> - OpenBGPd doesn't have a shell, so we use ash to get into the container's shell from which we can execute <code>bgpctl</code> command line utility to drive OpenBGPd.</li> <li><code>docker exec -it clab-ixp-rs2 birdc</code> - BIRD employs <code>birdc</code> shell which allows us to interact with BIRD via a command line interface.</li> </ul>"},{"location":"lab-examples/peering-lab/#basic-service-verification","title":"Basic service verification","text":"<p>The basic configuration that our peers and route servers were configured with assumed a plain unfiltered exchange of BGP routes. No IRR filtering, RPKI validation or any other MANRS-recommended security measures were enabled. This means that we can verify the basic connectivity between the peers and route servers by simply checking the BGP session status:</p> Nokia SR OSFRRouting <p>With the <code>show router bgp summary</code> command we can obtain the summarised BGP session status, where at the end of the output we can see that the BGP session with the two route servers was established successfully: Nokia SR OS node<pre><code>[/]\nA:admin@peer1# show router bgp summary\n===============================================================================\nBGP Router ID:10.0.0.1         AS:64501       Local AS:64501\n===============================================================================\nBGP Admin State         : Up          BGP Oper State              : Up\nTotal Peer Groups       : 1           Total Peers                 : 2\nTotal VPN Peer Groups   : 0           Total VPN Peers             : 0\nCurrent Internal Groups : 1           Max Internal Groups         : 1\nTotal BGP Paths         : 9           Total Path Memory           : 3200\n\n# -- snip --\n\n===============================================================================\nBGP Summary\n===============================================================================\nLegend : D - Dynamic Neighbor\n===============================================================================\nNeighbor\nDescription\n                  AS PktRcvd InQ  Up/Down   State|Rcv/Act/Sent (Addr Family)\n                      PktSent OutQ\n-------------------------------------------------------------------------------\n192.168.0.3\n                64503    2933    0 01d00h24m 1/1/1 (IPv4)\n                         2934    0           \n192.168.0.4\n                64503    3346    0 01d00h24m 1/0/1 (IPv4)\n                         2934    0           \n-------------------------------------------------------------------------------\n</code></pre></p> <p>A single route has been sent to both peers (<code>10.0.0.1/32</code>) and a single route has been received from each peer. We can investigate which route has been received from the peers by using <code>show router bgp neighbor &lt;neighbor addr&gt; received-routes</code>:</p> <pre><code>[/]\nA:admin@peer1# show router bgp neighbor \"192.168.0.3\" received-routes \n===============================================================================\nBGP Router ID:10.0.0.1         AS:64501       Local AS:64501      \n===============================================================================\nLegend -\nStatus codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                l - leaked, x - stale, &gt; - best, b - backup, p - purge\nOrigin codes  : i - IGP, e - EGP, ? - incomplete\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     IGP Cost\n      As-Path                                                        Label\n-------------------------------------------------------------------------------\nu*&gt;i  10.0.0.2/32                                        n/a         0\n      192.168.0.2                                        None        0\n      64502                                                          -\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>As we see, the FRR's route <code>10.0.0.2/32</code> has been received from the peer, as expected.</p> <p>On FRR side we can use <code>show ip bgp summary</code> to get the summarised status: FRRouting node<pre><code>peer2# show ip bgp summary\n\nIPv4 Unicast Summary (VRF default):\nBGP router identifier 10.0.0.2, local AS number 64502 vrf-id 0\nBGP table version 2\nRIB entries 3, using 576 bytes of memory\nPeers 2, using 1434 KiB of memory\n\nNeighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc\n192.168.0.3     4      64503     29424     29426        0    0    0 1d00h31m            1        2 N/A\n192.168.0.4     4      64503     33613     29426        0    0    0 1d00h31m            1        2 N/A\n\nTotal number of neighbors 2\n</code></pre></p>"},{"location":"lab-examples/peering-lab/#use-cases","title":"Use cases","text":"<p>Now that the foundation is in place, users are encouraged to explore the various use cases that are typical for an IXP setup. The following use cases may be covered in the updates to this lab in the future:</p> <ul> <li>Peer-side advanced filtering and BGP configuration</li> <li>IRR Filtering</li> <li>RPKI validation</li> <li>Looking glass integration</li> <li>ARouteServer-based provisioning</li> <li>IXP-manager introduction</li> </ul>"},{"location":"lab-examples/peering-lab/#references","title":"References","text":"<p>The following resources were used to create this lab:</p> <ul> <li>AMS-IX Route Servers</li> <li>Implementation of RPKI and IRR filtering on the AMS-IX platform</li> <li>NL-IX Config Guide Peering</li> <li>Nokia SR OS 23.3.1 BGP Guide</li> <li>RFC 7948 &amp; RFC 7948</li> <li>Getting started with BIRD by Kintone</li> <li>HowTo BIRD by dn42</li> <li>IXP Lab by NSRC</li> <li>ARouteServer: repo, docs, tutorial</li> <li>bgpq4</li> <li>RPKI docs by NLNetLabs</li> </ul> <ol> <li> <p>Container image is based on the pierky/bird container image, but with iproute2 package installed.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/rare-freertr/","title":"RARE/freeRtr","text":"Description A 2-node network of RARE/freeRtr routers Components RARE/freeRtr Resource requirements<sup>1</sup>  1  1 GB Lab location rare-freertr/freeRtr-containerlab Version information<sup>2</sup> <code>containerlab:0.39.0</code>, <code>freertr-containerlab:latest</code>, <code>docker:23.0.1</code>"},{"location":"lab-examples/rare-freertr/#description","title":"Description","text":"<p>RARE stands for Router for Academia, Research &amp; Education. It is an open source routing platform, used to create a network operating system (NOS) on commodity hardware. This lab example comprises two RARE/freeRtr routers connected via their respective <code>eth1</code> port.</p> <p>Containerlab's support for RARE is detailed on the Rare's kind page.</p>"},{"location":"lab-examples/rare-freertr/#lab-instructions","title":"Lab instructions","text":"<p>The lab is documented in details in the rare-freertr/freeRtr-containerlab repo.</p> <p>There you'll find information such as:</p> <ul> <li>How to build <code>RARE/freeRtr containerlab</code> image</li> <li>How to launch <code>RARE/freeRtr Hello world</code> lab</li> <li>Lab configuration</li> <li>Lab verification</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with docs for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/single-srl/","title":"Single SR Linux node","text":"Description a single Nokia SR Linux node Components Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file srl01.clab.yml Name srl01"},{"location":"lab-examples/single-srl/#description","title":"Description","text":"<p>A lab consists of a single SR Linux container equipped with a single interface - its management interface. No other network/data interfaces are created.</p> <p></p> <p>The SR Linux's <code>mgmt</code> interface is connected to the <code>containerlab</code> docker network that is created as part of the lab deployment process. The <code>mgmt</code> interface of SRL will get IPv4/6 address information via DHCP service provided by docker daemon.</p>"},{"location":"lab-examples/single-srl/#use-cases","title":"Use cases","text":"<p>This lightweight lab enables the users to perform the following exercises:</p> <ul> <li>get familiar with SR Linux architecture</li> <li>explore SR Linux extensible CLI</li> <li>navigate the SR Linux YANG tree</li> <li>play with gNMI<sup>2</sup> and JSON-RPC programmable interfaces</li> <li>write/debug/manage custom apps built for SR Linux NDK</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> <li> <p>Check out gnmic gNMI client to interact with SR Linux gNMI server.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-ceos/","title":"Nokia SR Linux and Arista cEOS","text":"Description A Nokia SR Linux connected back-to-back with Arista cEOS Components Nokia SR Linux, Arista cEOS Resource requirements<sup>1</sup>  2  4 GB Topology file srlceos01.clab.yml Name srlceos01 Version information<sup>2</sup> <code>containerlab:0.56.0</code>, <code>srlinux:24.3.3</code>, <code>ceos:4.32.0F</code>, <code>docker-ce:26.0.0</code>"},{"location":"lab-examples/srl-ceos/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Arista cEOS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p>"},{"location":"lab-examples/srl-ceos/#deployment","title":"Deployment","text":"<p>The deployment process of this lab is explained in the quickstart.</p>"},{"location":"lab-examples/srl-ceos/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Arista cEOS operating systems.</p>"},{"location":"lab-examples/srl-ceos/#bgp","title":"BGP","text":"<p>This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and Arista cEOS. Both nodes exchange NLRI with their loopback prefix making it reachable.</p>"},{"location":"lab-examples/srl-ceos/#configuration","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes.</p> SR LinuxcEOS <p>Get into SR Linux CLI with <code>ssh clab-srlceos01-srl</code> and start configuration. You can configure the node by typing in commands using the snippet below, or copy it entirely and paste it into the CLI.</p> <pre><code># enter the candidate datastore\nenter candidate\n\n# configure physical interface\n/ interface ethernet-1/1 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            admin-state enable\n            address 192.168.1.1/24 {\n            }\n        }\n    }\n}\n\n# configure loopback interface\n/ interface lo0 {\n    subinterface 0 {\n        ipv4 {\n            admin-state enable\n            address 10.10.10.1/32 {\n            }\n        }\n    }\n}\n\n# configure routing policy to import/export routes via BGP\n/ routing-policy {\n    policy loopbacks-policy {\n        statement 1 {\n            match {\n                protocol local\n            }\n            action {\n                policy-result accept\n            }\n        }\n    }\n}\n\n/ network-instance default {\n    # add physical and logical interface to the network instance\n    interface ethernet-1/1.0 {\n    }\n    interface lo0.0 {\n    }\n    protocols {\n        bgp {\n            autonomous-system 65001\n            router-id 10.10.10.1\n            afi-safi ipv4-unicast {\n                admin-state enable\n            }\n            group ibgp {\n                export-policy loopbacks-policy\n                import-policy loopbacks-policy\n            }\n            neighbor 192.168.1.2 {\n                admin-state enable\n                peer-as 65001\n                peer-group ibgp\n            }\n        }\n    }\n}\n\ncommit now\n</code></pre> <p>Get into cEOS CLI with <code>ssh clab-srlceos01-ceos</code><sup>3</sup> and start configuration</p> <pre><code># enter configuration mode\nenable\nconfigure\nip routing\n\n# configure loopback and data interfaces\ninterface Ethernet1\n  no switchport\n  ip address 192.168.1.2/24\nexit\ninterface Loopback0\n  ip address 10.10.10.2/32\nexit\n\n# configure BGP\nrouter bgp 65001\n  router-id 10.10.10.2\n  neighbor 192.168.1.1 remote-as 65001\n  network 10.10.10.2/32\nexit\nexit\n</code></pre>"},{"location":"lab-examples/srl-ceos/#verification","title":"Verification","text":"<p>Once BGP peering is established, the routes can be seen in GRT of both nodes:</p> SR LinuxcEOS <pre><code>A:srl# show / network-instance default route-table ipv4-unicast prefix 10.*2/32\n--------------------------------------------------------------------------------------------------------------------------------------------------\nIPv4 unicast route table of network instance default\n--------------------------------------------------------------------------------------------------------------------------------------------------\n+----------------+------+-----------+--------------------+---------+---------+--------+-----------+----------+----------+----------+-------------+\n|     Prefix     |  ID  |   Route   |    Route Owner     | Active  | Origin  | Metric |   Pref    | Next-hop | Next-hop |  Backup  |   Backup    |\n|                |      |   Type    |                    |         | Network |        |           |  (Type)  | Interfac | Next-hop |  Next-hop   |\n|                |      |           |                    |         | Instanc |        |           |          |    e     |  (Type)  |  Interface  |\n|                |      |           |                    |         |    e    |        |           |          |          |          |             |\n+================+======+===========+====================+=========+=========+========+===========+==========+==========+==========+=============+\n| 10.10.10.2/32  | 0    | bgp       | bgp_mgr            | True    | default | 0      | 170       | 192.168. | ethernet |          |             |\n|                |      |           |                    |         |         |        |           | 1.0/24 ( | -1/1.0   |          |             |\n|                |      |           |                    |         |         |        |           | indirect |          |          |             |\n|                |      |           |                    |         |         |        |           | /local)  |          |          |             |\n+----------------+------+-----------+--------------------+---------+---------+--------+-----------+----------+----------+----------+-------------+\n</code></pre> <pre><code>ceos&gt;show ip route\n\nVRF: default\nCodes: C - connected, S - static, K - kernel,\n    O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1,\n    E2 - OSPF external type 2, N1 - OSPF NSSA external type 1,\n    N2 - OSPF NSSA external type2, B - BGP, B I - iBGP, B E - eBGP,\n    R - RIP, I L1 - IS-IS level 1, I L2 - IS-IS level 2,\n    O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary,\n    NG - Nexthop Group Static Route, V - VXLAN Control Service,\n    DH - DHCP client installed default route, M - Martian,\n    DP - Dynamic Policy Route, L - VRF Leaked,\n    RC - Route Cache Route\n\nGateway of last resort:\nK        0.0.0.0/0 [40/0] via 172.20.20.1, Management0\n\nB I      10.10.10.1/32 [200/0] via 192.168.1.1, Ethernet1\nC        10.10.10.2/32 is directly connected, Loopback0\nC        172.20.20.0/24 is directly connected, Management0\nC        192.168.1.0/24 is directly connected, Ethernet1\n</code></pre> <p>Data plane confirms that routes have been programmed to FIB:</p> <pre><code>A:srl# ping 10.10.10.2 network-instance default\nUsing network instance default\nPING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.\n64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=3.47 ms\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> <li> <p>Credentials <code>admin:admin</code> \u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-crpd/","title":"Nokia SR Linux and Juniper cRPD","text":"Description A Nokia SR Linux connected back-to-back with Juniper cRPD Components Nokia SR Linux, Juniper cRPD Resource requirements<sup>1</sup>  2  2 GB Topology file srlcrpd01.clab.yml Name srlcrpd01 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>crpd:20.2R1.10</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-crpd/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Juniper cRPD via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/srl-crpd/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper cRPD network operating systems.</p>"},{"location":"lab-examples/srl-crpd/#ospf","title":"OSPF","text":""},{"location":"lab-examples/srl-crpd/#configuration","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable OSPF on both nodes.</p> srlcrpd <p>Get into SR Linux CLI with <code>docker exec -it clab-srlcrpd01-srl sr_cli</code> and start configuration <pre><code># enter candidate datastore\nenter candidate\n\n# configure loopback and data interfaces\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.1.1/24\n\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4 address 10.10.10.1/32\n\n# configure OSPF\nset / network-instance default router-id 10.10.10.1\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface lo0.0\nset / network-instance default protocols ospf instance main admin-state enable\nset / network-instance default protocols ospf instance main version ospf-v2\nset / network-instance default protocols ospf instance main area 0.0.0.0 interface ethernet-1/1.0 interface-type point-to-point\nset / network-instance default protocols ospf instance main area 0.0.0.0 interface ethernet-1/1.0\n\n# commit config\ncommit now\n</code></pre></p> <p>cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process <code>bash</code> shell and configure interfaces: <code>docker exec -it clab-srlcrpd01-crpd bash</code> <pre><code># configure linux interfaces\nip addr add 192.168.1.2/24 dev eth1\nip addr add 10.10.10.2/32 dev lo\n</code></pre> Then launch the CLI and continue configuration <code>docker exec -it clab-srlcrpd01-crpd cli</code>: <pre><code># enter configuration mode\nconfigure\nset routing-options router-id 10.10.10.2\n\nset protocols ospf area 0.0.0.0 interface eth1 interface-type p2p\nset protocols ospf area 0.0.0.0 interface lo.0 interface-type nbma\n\n# commit configuration\ncommit\n</code></pre></p>"},{"location":"lab-examples/srl-crpd/#verificaton","title":"Verificaton","text":"<p>After the configuration is done on both nodes, verify the control plane by checking the route tables on both ends and ensuring dataplane was programmed as well by pinging the remote loopback</p> srlcrpd <p><pre><code># control plane verification\nA:srl# / show network-instance default route-table ipv4-unicast summary | grep ospf\n| 10.10.10.2/32                 | 0     | true       | ospfv2          | 1       | 10    | 192.168.1.2 (direct)                     | ethernet-1/1.0    |\n</code></pre> <pre><code># data plane verification\nA:srl# ping 10.10.10.2 network-instance default\nUsing network instance default\nPING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.\n64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms\n</code></pre></p> <pre><code># control plane verification\nroot@crpd&gt; show route | match OSPF\n10.10.10.1/32      *[OSPF/10] 00:01:24, metric 1\n224.0.0.5/32       *[OSPF/10] 00:05:49, metric 1\n</code></pre>"},{"location":"lab-examples/srl-crpd/#is-is","title":"IS-IS","text":""},{"location":"lab-examples/srl-crpd/#configuration_1","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable IS-IS on both nodes.</p> srlcrpd <p>Get into SR Linux CLI with <code>docker exec -it clab-srlcrpd01-srl sr_cli</code> and start configuration <pre><code># enter candidate datastore\nenter candidate\n\n# configure loopback and data interfaces\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.1.1/24\n\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4 address 10.10.10.1/32\n\n# configure IS-IS\nset / network-instance default router-id 10.10.10.1\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface lo0.0\nset / network-instance default protocols isis instance main admin-state enable\nset / network-instance default protocols isis instance main net [ 49.0001.0100.1001.0001.00 ]\nset / network-instance default protocols isis instance main interface ethernet-1/1.0 admin-state enable\nset / network-instance default protocols isis instance main interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance main interface lo0.0\n\n# commit config\ncommit now\n</code></pre></p> <p>cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process <code>bash</code> shell and configure interfaces: <code>docker exec -it clab-srlcrpd01-crpd bash</code> <pre><code># configure linux interfaces\nip addr add 192.168.1.2/24 dev eth1\nip addr add 10.10.10.2/32 dev lo\n</code></pre> Then launch the CLI and continue configuration <code>docker exec -it clab-srlcrpd01-crpd cli</code>: <pre><code># enter configuration mode\nconfigure\nset interfaces lo0 unit 0 family iso address 49.0001.0100.1001.0002.00\nset routing-options router-id 10.10.10.2\n\nset protocols isis interface all point-to-point\nset protocols isis interface lo0.0\nset protocols isis level 1 wide-metrics-only\nset protocols isis level 2 wide-metrics-only\nset protocols isis reference-bandwidth 100g\n\n# commit configuration\ncommit\n</code></pre></p>"},{"location":"lab-examples/srl-crpd/#verification","title":"Verification","text":"srlcrpd <p><pre><code># control plane verification\nA:srl# / show network-instance default route-table ipv4-unicast summary | grep isis\n| 10.10.10.2/32                 | 0     | true       | isis            | 10      | 18    | 192.168.1.2 (direct)                     | ethernet-1/1.0    |\n| 172.20.20.0/24                | 0     | true       | isis            | 110     | 18    | 192.168.1.2 (direct)                     | ethernet-1/1.0    |\n</code></pre> <pre><code># data plane verification\nA:srl# ping 10.10.10.2 network-instance default\nUsing network instance default\nPING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.\n64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms\n</code></pre></p> <pre><code># control plane verification\nroot@crpd&gt; show route table inet.0 | match IS-IS\n10.10.10.1/32      *[IS-IS/18] 00:00:13, metric 100\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-frr/","title":"Nokia SR Linux and FRR","text":"Description A Nokia SR Linux connected back-to-back FRR router Components Nokia SR Linux, FRR Resource requirements<sup>1</sup>  2  2 GB Topology file srlfrr01.clab.yml Name srlfrr01 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>frrouting/frr:v7.5.0</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-frr/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with FRR router via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/srl-frr/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic control plane interoperability scenarios between Nokia SR Linux and FRR network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. There you will find the config files to demonstrate a classic iBGP peering use case:</p> <ul> <li><code>daemons</code>: frr daemons config that is bind mounted to the frr container to trigger the start of the relevant FRR services</li> <li><code>frr.cfg</code>: vtysh config lines to configure a basic iBGP peering</li> <li><code>srl.cfg</code>: sr_cli config lines to configure a basic iBGP peering</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-sonic/","title":"Nokia SR Linux and SONiC","text":"Description A Nokia SR Linux connected back-to-back with SONiC-VS Components Nokia SR Linux, SONiC Resource requirements<sup>1</sup>  2  2 GB Topology file sonic01.clab.yml Name sonic01 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>docker-sonic-vs:202012</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-sonic/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Azure SONiC via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p>"},{"location":"lab-examples/srl-sonic/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and SONiC operating systems.</p>"},{"location":"lab-examples/srl-sonic/#bgp","title":"BGP","text":"<p>This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and SONiC. Both nodes exchange NLRI with their loopback prefix making it reachable.</p>"},{"location":"lab-examples/srl-sonic/#configuration","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes.</p> srlsonic <p>Get into SR Linux CLI with <code>docker exec -it clab-sonic01-srl sr_cli</code> and start configuration <pre><code># enter candidate datastore\nenter candidate\n\n# configure loopback and data interfaces\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.1.1/24\n\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4 address 10.10.10.1/32\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface lo0.0\n\n# configure BGP\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp router-id 10.10.10.1\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp group ibgp export-policy export-lo\nset / network-instance default protocols bgp neighbor 192.168.1.2 admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-group ibgp\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-as 65001\n\n# create export policy\nset / routing-policy policy export-lo statement 10 match protocol local\nset / routing-policy policy export-lo statement 10 action accept\n\n# commit config\ncommit now\n</code></pre></p> <p>Get into sonic container shell with <code>docker exec -it clab-sonic01-sonic bash</code> and configure the so-called front-panel ports. Since we defined only one data interface for our sonic/srl nodes, we need to confgure a single port and a loopback interface: <pre><code>config interface ip add Ethernet0 192.168.1.2/24\nconfig interface startup Ethernet0\nconfig loopback add Loopback0\nconfig interface ip add Loopback0 10.10.10.2/32\nconfig interface startup Loopback0\n</code></pre> Now when data interface has been configured, check to make sure in /etc/frr/daemons that \"bgpd=yes\".  Restart the frr service if required and verify that bgpd is running. <pre><code>root@sonic:/# service frr restart\n[ ok ] Stopped watchfrr.\n[ ok . Stopped staticd[....] Stopped zebra[....] Stopped bgpd\n.\n.\n[ ok ] Started watchfrr.\nroot@sonic:/# service frr status\n[ ok ] Status of watchfrr: running.\n[ ok ] Status of zebra: running.\n[ ok ] Status of bgpd: running.\n[ ok ] Status of staticd: running.\n</code></pre> Then enter in the FRR shell to configure BGP by typing <code>vtysh</code> command inside the sonic container. <pre><code># enter configuration mode\nconfigure\n\n# configure BGP\nrouter bgp 65001\n  bgp router-id 10.10.10.2\n  neighbor 192.168.1.1 remote-as 65001\n  address-family ipv4 unicast\n    network 10.10.10.2/32\n  exit-address-family\nexit\naccess-list all seq 5 permit any\n</code></pre></p>"},{"location":"lab-examples/srl-sonic/#verification","title":"Verification","text":"<p>Once BGP peering is established, the routes can be seen in GRT of both nodes:</p> srlsonic <pre><code>A:srl# / show network-instance default route-table ipv4-unicast summary | grep bgp\n| 10.10.10.2/32                 | 0     | true       | bgp             | 0       | 170   | 192.168.1.2 (indirect)                   | None              |\n</code></pre> <pre><code>sonic# sh ip route\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n      O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n      T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n      F - PBR, f - OpenFabric,\n      &gt; - selected route, * - FIB route, q - queued route, r - rejected route\n\nK&gt;* 0.0.0.0/0 [0/0] via 172.20.20.1, eth0, 00:20:55\nB&gt;* 10.10.10.1/32 [200/0] via 192.168.1.1, Ethernet0, 00:01:51\nC&gt;* 10.10.10.2/32 is directly connected, Loopback0, 00:00:53\nC&gt;* 172.20.20.0/24 is directly connected, eth0, 00:20:55\nB   192.168.1.0/24 [200/0] via 192.168.1.0 inactive, 00:01:51\nC&gt;* 192.168.1.0/24 is directly connected, Ethernet0, 00:03:50\n</code></pre> <p>Data plane confirms that routes have been programmed to FIB: <pre><code>sonic# ping 10.10.10.1\nPING 10.10.10.1 (10.10.10.1) 56(84) bytes of data.\n64 bytes from 10.10.10.1: icmp_seq=1 ttl=64 time=2.28 ms\n64 bytes from 10.10.10.1: icmp_seq=2 ttl=64 time=2.84 ms\n</code></pre></p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-vjunos-switch/","title":"Nokia SR Linux and Juniper vJunos-switch","text":"Description A Nokia SR Linux connected back-to-back with Juniper vJunos-switch Components Nokia SR Linux, Juniper vJunos-switch Resource requirements<sup>1</sup>  4  8 GB Topology file srlvjunos01.clab.yml Name srlvjunos01 Version information<sup>2</sup> <code>containerlab:0.45.0</code>, <code>srlinux:23.7.1</code>, <code>vjunos-switch:23.2R1.14</code>, <code>docker-ce:23.0.3</code>"},{"location":"lab-examples/srl-vjunos-switch/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Juniper vJunos-switch via three point-to-point ethernet links. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/srl-vjunos-switch/#use-cases","title":"Use cases","text":"<p>The nodes are provisioned with a basic interface configuration for three interfaces they are connected with. Pings between the nodes should work out of the box using all three interfaces.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-vjunosevolved/","title":"Nokia SR Linux and Juniper vJunosEvolved","text":"Description A Nokia SR Linux connected back-to-back with Juniper vJunosEvolved Components Nokia SR Linux, Juniper vJunosEvolved Resource requirements<sup>1</sup>  6  12 GB Topology file srlvjunos02.clab.yml Name srlvjunos02 Version information<sup>2</sup> <code>containerlab:0.49.0</code>, <code>srlinux:23.7.1</code>, <code>vJunosEvolved-23.2R1-S1.8</code>, <code>docker-ce:24.0.7,</code>"},{"location":"lab-examples/srl-vjunosevolved/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Juniper vJunosEvolved via three point-to-point ethernet links. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/srl-vjunosevolved/#use-cases","title":"Use cases","text":"<p>The nodes are provisioned with a basic interface configuration for three interfaces they are connected with. Pings between the nodes should work out of the box using all three interfaces.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-xrd/","title":"Nokia SR Linux and Cisco XRd","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRd Components Nokia SR Linux, Cisco XRd Resource requirements<sup>1</sup>  2  4 GB Topology file srlxrd01.clab.yml Name srlxrd01 Version information<sup>2</sup> <code>containerlab:0.34.0</code>, <code>srlinux:22.11.1</code>, <code>xrd:7.8.1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-xrd/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Cisco XRd via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p>"},{"location":"lab-examples/srl-xrd/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRd operating systems.</p>"},{"location":"lab-examples/srl-xrd/#configuration","title":"Configuration","text":"<p>Both SR Linux and XRd nodes come with a startup config files referenced for them. These user-defined startup files introduce the following change on top of the default config that these nodes boot with:</p> <ul> <li>On SR Linux, interface <code>ethernet-1/1</code> is configured with <code>192.168.0.0/31</code> address and this interface attached to the default network instance.</li> <li>On XRd, interface <code>Gi 0/0/0/0</code> is configured with <code>192.168.0.1/31</code> address.</li> </ul>"},{"location":"lab-examples/srl-xrd/#verification","title":"Verification","text":"<p>When the deployment of the lab finishes, users can validate that the datapath works between the nodes by pinging the directly connected interfaces from either node.</p> <p>Here is an example from SR Linux side:</p> <pre><code>--{ running }--[  ]--\nA:srl# ping network-instance default 192.168.0.1 \nUsing network instance default\nPING 192.168.0.1 (192.168.0.1) 56(84) bytes of data.\n64 bytes from 192.168.0.1: icmp_seq=1 ttl=255 time=12.0 ms\n64 bytes from 192.168.0.1: icmp_seq=2 ttl=255 time=6.83 ms\n^C\n--- 192.168.0.1 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1001ms\nrtt min/avg/max/mdev = 6.830/9.428/12.027/2.600 ms\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/templated01/","title":"Leaf-spine topology","text":"Description A Full Meshed X Leaf(s), Y Spine(s) Clos topology Components Nokia SR Linux Topology template file templated01.clab.gotmpl Topology variable file templated01.clab_vars.yaml Name templated01"},{"location":"lab-examples/templated01/#description","title":"Description","text":"<p>This lab consists of a customizable Leaf and Spine Clos topology. The number and type of SR Linux Leaf and Spine nodes is configurable, it can be set using the topology variable file <code>templated01.clab_vars.yaml</code>.</p> <p>The type of SR Linux used and the naming prefixes can be customized as well.</p> <pre><code>spines:\n  # SRL spine type\n  type: ixrd3\n  # number of spines\n  num: 2\n  # prefix of spines name: ${prefix}${index}\n  prefix: spine\nleaves:\n  # SRL leaf type\n  type: ixrd3\n  # number of leaves\n  num: 4\n  # prefix of leaf name: ${prefix}${index}\n  prefix: leaf\n</code></pre>"},{"location":"lab-examples/templated01/#configuration","title":"Configuration","text":"<p>Deploy the lab</p> <pre><code>clab deploy -t templated01.clab.gotmpl\n</code></pre> <p>Run <code>configure.sh</code> script to configure the lab</p> <pre><code>bash configure.sh\n</code></pre> <p>The <code>configure.sh</code> script relies on gomplate and gnmic.</p> <ul> <li>gomplate is used to generate the necessary configuration variables based on the number of spines and leaves, their type and prefix.</li> <li>gnmic is used to generate configuration payloads per node and push it using a gNMI Set RPC.</li> </ul>"},{"location":"lab-examples/templated02/","title":"5-stage Clos topology","text":"Description A 5-stage Clos topology with X Pod(s), Y Super Spine(s) Components Nokia SR Linux Topology template file templated02.clab.gotmpl Topology variable file templated02.clab_vars.yaml Name templated02"},{"location":"lab-examples/templated02/#description","title":"Description","text":"<p>This lab consists of a customizable 5 stage Clos topology. Each pod in this lab consists of a configurable number of fully meshed spines and leaves. The spines in each pod are connected to a configurable number of super spines.</p> <p>The topology template is rendered using the variable file shown below:</p> <pre><code>super_spines:\n  # SRL super spine type\n  type: ixrd3\n  # number of super spines\n  num: 2\n  # prefix of super spines name: ${prefix}${index}\n  prefix: super-spine\n\npods:\n  # number of pods\n  num: 2\n  spines:\n    # SRL spine type\n    type: ixrd3\n    # number of spines per pod\n    num: 2\n    # prefix of spines name: ${prefix}${index}\n    prefix: spine\n  leaves:\n    # SRL leaf type\n    type: ixrd2l\n    # number of leaves per pod\n    num: 4\n    # prefix of leaf name: ${prefix}${index}\n    prefix: leaf\n</code></pre>"},{"location":"lab-examples/templated02/#configuration","title":"Configuration","text":"<pre><code>clab deploy -t templated02.clab.gotmpl\n</code></pre> <p>Run <code>configure.sh</code> script to configure the lab</p> <pre><code>bash configure.sh\n</code></pre> <p>The <code>configure.sh</code> script relies on gomplate and gnmic.</p> <ul> <li>gomplate is used to generate the necessary configuration variables based on the number of spines and leaves, their type and prefix.</li> <li>gnmic is used to generate configuration payloads per node and push it using a gNMI Set RPC.</li> </ul>"},{"location":"lab-examples/two-srls/","title":"Two SR Linux nodes","text":"Description Two Nokia SR Linux nodes Components Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file srl02.clab.yml Name srl02 Validated versions<sup>2</sup> <code>containerlab v0.26.2</code>,<code>srlinux:21.11.3</code>"},{"location":"lab-examples/two-srls/#description","title":"Description","text":"<p>A lab consists of two SR Linux nodes connected via a point-to-point link over <code>e1-1</code> interfaces. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/two-srls/#configuration","title":"Configuration","text":"<p>The nodes of this lab have been provided with a startup configuration using <code>startup-config</code> directive. The startup configuration adds loopback and interfaces addressing as per the diagram above.</p> <p>Once the lab is started, the nodes will be able to ping each other via configured interfaces:</p> <pre><code>--{ running }--[  ]--\nA:srl1# ping network-instance default 192.168.0.1\nUsing network instance default\nPING 192.168.0.1 (192.168.0.1) 56(84) bytes of data.\n64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=55.2 ms\n64 bytes from 192.168.0.1: icmp_seq=2 ttl=64 time=6.61 ms\n64 bytes from 192.168.0.1: icmp_seq=3 ttl=64 time=8.92 ms\n64 bytes from 192.168.0.1: icmp_seq=4 ttl=64 time=14.2 ms\n^C\n--- 192.168.0.1 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3005ms\nrtt min/avg/max/mdev = 6.610/21.232/55.173/19.790 ms\n</code></pre>"},{"location":"lab-examples/two-srls/#use-cases","title":"Use cases","text":"<p>This lab, besides having the same objectives as srl01 lab, also enables the following scenarios:</p> <ul> <li>get to know protocols and services configuration</li> <li>verify basic control plane and data plane operations</li> <li>explore SR Linux state datastore for the paths which reflect control plane operation metrics or dataplane counters</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> <li> <p>versions of respective container images or software that was used to create the lab.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-sros/","title":"Nokia SR Linux and Nokia SR OS","text":"Description A Nokia SR Linux connected back-to-back with Nokia SR OS Components Nokia SR Linux, Nokia SR OS Resource requirements<sup>1</sup>  2  5 GB Topology file vr01.clab.yml Name vr01 Version information<sup>2</sup> <code>containerlab:0.27.1</code>, <code>srlinux:22.3.2</code>, <code>vr-sros:22.5.R1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-sros/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Nokia SR OS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Nokia SR OS VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-sros/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Nokia SR OS network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-vmx/","title":"Nokia SR Linux and Juniper vMX","text":"Description A Nokia SR Linux connected back-to-back with Juniper vMX Components Nokia SR Linux, Juniper vMX Resource requirements<sup>1</sup>  2  8 GB Topology file vr02.clab.yml Name vr02 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>vr-vmx:20.2R1.10</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-vmx/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Juniper vMX via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Juniper vMX VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-vmx/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper vMX network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-xrv/","title":"Nokia SR Linux and Cisco XRv","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv Components Nokia SR Linux, Cisco XRv Resource requirements<sup>1</sup>  1  3 GB Topology file vr03.clab.yml Name vr03 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>vr-xrv:6.1.2</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-xrv/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Cisco XRv via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Cisco XRv VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-xrv/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-xrv9k/","title":"Nokia SR Linux and Cisco XRv9k","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv9k Components Nokia SR Linux, Cisco XRv9k Resource requirements<sup>1</sup>  2  12 GB Topology file vr04.clab.yml Name vr04 Version information<sup>2</sup> <code>containerlab:0.9.5</code>, <code>srlinux:20.6.3-145</code>, <code>vr-xrv9k:7.2.1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-xrv9k/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Cisco XRv9k via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Cisco XRv9k VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-xrv9k/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv9k network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vsrx01/","title":"Juniper vSRX","text":"Description A Juniper vSRX connected to two Alpine Linux Hosts Components Juniper vSRX, Multitool Alpine Linux Resource requirements<sup>1</sup>  2  4 GB Topology file vsrx01.yml Name vsrx01 Version information<sup>2</sup> <code>containerlab:0.47.2</code>, <code>junos-vsrx3-x86-64-23.2R1.13.qcow2</code>, <code>docker:24.0.6</code>"},{"location":"lab-examples/vsrx01/#description","title":"Description","text":"<p>This lab consists of one Juniper vSRX router connected to two Alpine Linux nodes.</p> <pre><code>client1&lt;----&gt;vSRX&lt;----&gt;client2\n</code></pre>"},{"location":"lab-examples/vsrx01/#configuration","title":"Configuration","text":"<p>The vSRX takes about 5 minutes to complete its start up. Check using \"docker container ls\" until the vSRX shows up as \"healthy\"</p> <pre><code># docker container ls\nCONTAINER ID   IMAGE                                  COMMAND                  CREATED          STATUS                    PORTS                                        NAMES\n85e3251a27c1   vrnetlab/vr-vsrx:23.2R1.13             \"/launch.py --userna\u2026\"   10 minutes ago   Up 10 minutes (healthy)   22/tcp, 830/tcp, 5000/tcp, 10000-10099/tcp   clab-vsrx1-srx1\nf06a4997ac1b   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   10 minutes ago   Up 10 minutes             80/tcp, 443/tcp, 1180/tcp, 11443/tcp         clab-vsrx1-client1\nc77b68244805   wbitt/network-multitool:alpine-extra   \"/bin/sh /docker-ent\u2026\"   10 minutes ago   Up 10 minutes             80/tcp, 443/tcp, 1180/tcp, 11443/tcp         clab-vsrx1-client2\n</code></pre>"},{"location":"lab-examples/vsrx01/#vsrx1","title":"vsrx1","text":"<p>Log into the vSRX using SSH with <code>ssh admin@clab-vsrx1-srx1</code> and add the configuration from srx01.cfg. Password is <code>admin@123</code>.</p> <pre><code>admin&gt;configure\nset interfaces ge-0/0/0 unit 0 family inet address 192.168.1.1/30\nset interfaces ge-0/0/1 unit 0 family inet address 192.168.2.1/30\nset security zones security-zone trust interfaces ge-0/0/0 host-inbound-traffic system-services all\nset security zones security-zone trust interfaces ge-0/0/1 host-inbound-traffic system-services all\nset system services web-management https system-generated-certificate\nset security forwarding-options family mpls mode packet-based\n# commit \n</code></pre>"},{"location":"lab-examples/vsrx01/#client1","title":"client1","text":"<p>The two clients should be configured with the correct IP addresses and a route to the other client via the vSRX. First attach to the container process <code>docker exec -it clab-vsrx1-client1 ash</code></p> <pre><code>docker exec -it clab-vsrx1-client1 ash\n\n# ip a show dev eth1\n131: eth1@if132: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9500 qdisc noqueue state UP group default\n   link/ether aa:c1:ab:ac:1b:19 brd ff:ff:ff:ff:ff:ff link-netnsid 1\n   inet 192.168.1.2/30 scope global eth1\n      valid_lft forever preferred_lft forever\n   inet6 fe80::a8c1:abff:feac:1b19/64 scope link\n      valid_lft forever preferred_lft forever\n\n# ip route\ndefault via 172.20.20.1 dev eth0\n172.20.20.0/24 dev eth0 proto kernel scope link src 172.20.20.4\n192.168.1.0/30 dev eth1 proto kernel scope link src 192.168.1.2\n192.168.2.0/30 via 192.168.1.1 dev eth1\n</code></pre>"},{"location":"lab-examples/vsrx01/#verification","title":"Verification","text":"<p>Traceroute from client1 to client2 to verify the dataplane via the vSRX.</p>"},{"location":"lab-examples/vsrx01/#client1_1","title":"client1","text":"<pre><code># traceroute 192.168.2.2\ntraceroute to 192.168.2.2 (192.168.2.2), 30 hops max, 46 byte packets\n1  192.168.1.1 (192.168.1.1)  0.397 ms  0.347 ms  0.290 ms\n2  192.168.2.2 (192.168.2.2)  0.263 ms  0.374 ms  0.762 ms\n</code></pre>"},{"location":"lab-examples/vsrx01/#vsrx-web-gui","title":"vSRX Web Gui","text":"<p>To access the vSRX web interface point a browsers at the vSRX management IP address (fxp0) and use https. Login is <code>admin/admin@123</code>.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/wan/","title":"WAN topology","text":"Description WAN emulating topology Components Nokia SR Linux Resource requirements<sup>1</sup>  2  3 GB Topology file srl03.clab.yml Name srl03"},{"location":"lab-examples/wan/#description","title":"Description","text":"<p>Nokia SR Linux while focusing on the data center deployments in the first releases, will also be suitable for WAN deployments. In this lab users presented with a small WAN topology of four interconnected SR Linux nodes with multiple p2p interfaces between them.</p> <p></p>"},{"location":"lab-examples/wan/#use-cases","title":"Use cases","text":"<p>The WAN-centric scenarios can be tested with this lab:</p> <ul> <li>Link aggregation</li> <li>WAN protocols and features</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/cert/","title":"Certificate management","text":"<p>As more and more services move to \"secure by default\" behavior, it becomes important to simplify the PKI/TLS infrastructure provisioning in the lab environments. Containerlab tries to ease the process of certificate provisioning providing the following features:</p> <ol> <li>Automated certificate provisioning for lab nodes.</li> <li>Simplified CLI for CA and end-node keys generation.</li> <li>Ability to use custom/external CA.</li> </ol>"},{"location":"manual/cert/#automated-certificate-provisioning","title":"Automated certificate provisioning","text":"<p>Automated certificate provisioning is a two-stage process. First, containerlab creates a Certificate Authority (CA) and generates a certificate and key for it, storing these artifacts in a lab directory in the <code>.tls</code> directory. Then, containerlab generates a certificate and key for each node of a lab and signs it with the CA. The signed certificate and key are then installed on the node.</p> <p>Note</p> <p>Currently, automated installation of a node certificate is implemented only for Nokia SR Linux.</p>"},{"location":"manual/cert/#ca-certificate","title":"CA certificate","text":"<p>When generating CA certificate and key, containerlab can take in the following optional parameters:</p> <ul> <li><code>.settings.certificate-authority.key-size</code> - the size of the key in bytes, default is 2048</li> <li><code>.settings.certificate-authority.validity-duration</code> - the duration of the certificate. For example: <code>10m</code>, <code>1000h</code>. Max unit is hour. Default is <code>8760h</code> (1 year)</li> </ul>"},{"location":"manual/cert/#node-certificates","title":"Node certificates","text":"<p>The decision to generate node certificates is driven by either of the following two parameters:</p> <ol> <li>node kind</li> <li><code>issue</code> boolean parameter under <code>node-name.certificate</code> section.</li> </ol> <p>For SR Linux nodes the <code>issue</code> parameter is set to <code>true</code> and can't be changed. For other node kinds the <code>issue</code> parameter is set to <code>false</code> by default and can be overridden by the user.</p>"},{"location":"manual/cert/#simplified-cli-for-ca-and-end-node-keys-generation","title":"Simplified CLI for CA and end-node keys generation","text":"<p>Apart automated pipeline for certificate provisioning, containerlab exposes the following commands that can create a CA and node's cert/key:</p> <ul> <li><code>tools cert ca create</code> - creates a Certificate Authority</li> <li><code>tools cert sign</code> - creates certificate/key for a host and signs the certificate with CA</li> </ul> <p>With these two commands users can easily create CA node certificates and secure the transport channel of various protocols. This lab demonstrates how with containerlab's help one can easily create certificates and configure Nokia SR OS to use it for secured gNMI communication.</p>"},{"location":"manual/cert/#external-ca","title":"External CA","text":"<p>Users who require more control over the certificate generation process can use an existing external CA. Containerlab needs to be provided with the CA certificate and key. The CA certificate and key must be provided via <code>.settings.certificate-authority.[key]|[cert]</code> configuration parameters.</p> <pre><code>name: ext-ca\nsettings:\n  certificate-authority:\n    cert: /path/to/ca.crt\n    key: /path/to/ca.key\n</code></pre> <p>When using an external CA, containerlab will not generate a CA certificate and key. Instead, it will use the provided CA certificate and key to sign the node certificates.</p> <p>The paths can be provided in absolute or relative form. If the path is relative, it is relative to the directory where clab file is located.</p> <p>In addition to setting External CA files via <code>settings</code> section, users can also set the following environment variables:</p> <ul> <li><code>CLAB_CA_CERT_FILE</code> - path to the CA certificate</li> <li><code>CLAB_CA_KEY_FILE</code> - path to the CA key</li> </ul>"},{"location":"manual/codespaces/","title":"Containerlab labs in Codespaces","text":"<p>The best labs are the labs that you can run anywhere, anytime, with a single click and preferrably for free.</p> <p>Containerlab commoditized the labbing experience by providing a simple and easy to use tool to create and manage network topologies. But still you have to think a machine to run the lab on. Or, rather, you had.</p> <p>We started to ship a Dev Container<sup>1</sup> package for Containerlab that allows you to run containerlab-based labs in a GitHub Codespaces for free<sup>2</sup> unlocking a whole new level of flexibility and convenience for users.</p>"},{"location":"manual/codespaces/#labs-in-codespaces","title":"Labs in Codespaces","text":"<p>GitHub Codespaces is a cloud-based development environment by GitHub that allows you to spin up a fully configured personal dev environment in the cloud and start coding in seconds. If you think about Containerlab as a Lab-as-Code solution, you can quickly see how these two can be a perfect match.</p> <p>With Containerlab in Codespaces you can:</p> <ol> <li>Spin up an existing lab with a single click without having to install anything on your local machine.</li> <li>Start with Containerlab using the cloud IDE provided by Codespaces.</li> </ol> <p>Here is a quick demo how anyone can run the full SR Linux Streaming Telemetry lab by just clicking on a link. It is hard to imagine a more easy and convenient way to run your labs in the cloud.</p> <p>Fancy a full demo? Check out the 17min  video by Roman. Would you rather straight try it yourself, then click here.</p>"},{"location":"manual/codespaces/#how-does-it-work","title":"How does it work?","text":"<p>The key ingredients in this recipe are GitHub codespaces and the Dev Container image that we provide for Containerlab. When a user clicks on a link<sup>3</sup> to open the lab in Codespaces, GitHub spins up a Codespace environment and uses the Dev Container image to set it up. The Containerlab' Dev Container image has all the necessary tools and dependencies to run Containerlab:</p> <ul> <li>containerlab binary</li> <li>shell configuration</li> <li>system packages and tools (gnmic, gnoic)</li> <li>VS Code plugins</li> <li>and anything else we consider useful to have in the environment</li> </ul> <p>A user can choose to open the Codespace in a browser or in the local VS Code instance. In both cases, you get a fully functional environment with Containerlab installed and ready to use.</p> <p>Codespaces environment boots for a couple of minutes, but once it is up and running, you see the familiar VS Code interface with the terminal window where you can run containerlab (and any other) commands.</p>"},{"location":"manual/codespaces/#codespaces","title":"Codespaces","text":"<p>As we mentioned, the Codepsaces environment is a VM in the cloud; you can install packages, run other workloads, and use the VM in any way you like, you have the full control of it. What makes Codespaces VM different from a any other VM in the cloud is that it is tightly integrated with GitHub and VS Code, and provides a configurable and ready-to-use environment.</p> <p>We said it is free, but it is free to a certain extent, let's dig in.</p>"},{"location":"manual/codespaces/#free-plan","title":"Free plan","text":"<p>The best part about Codespaces is that it has a suitable free tier. GitHub offers 120 cpu-hours/month and 15 GB storage for free<sup>7</sup> to all users. This means that you can run a Codespace environment for 120 cpu-hours per month without any charges. This is a compelling offer for those who</p> <ul> <li>want to spin up a lab provided by others to get through a tutorial or a demo</li> <li>don't need to run the labs 24/7</li> <li>want to demo labs on-the-go without managing the local environment</li> </ul> <p>You can select which GitHub machine type you want to use for your project; each machine type is characterized by the amount of CPUs/RAM/Storage that it is equipped with, and based on that you can calculate how many cpu-hours you would consume running a lab with a chosen machine type.</p> <p>By the time of this writing (Jun 2024) the following machine types were available for GitHub users by default and beefier machines can be requested via GitHub support form.</p> Machine type Memory (GB) Storage (GB) Run time included in the free tier<sup>6</sup>(hours/month) 2 core 4 32 60 (2cpu*60h=120 cpu/hours) 4 core 8 32 30 8 core 16 32 15 16 core 32 64 7.5 32 core 64 128 3.75 (may not be available in your account) <p>If you need more than 120 cpu-hours, you can pay for the additional usage (consult with pricing), and you can always stop the environment when you don't need it to save the quota.</p> <p>Your cpu-hours counter is reset at the beginning of each month, so you can use the free plan every month. And by default you have a $0 spending limit, so you won't be charged unless you explicitly allowed it. Good!</p>"},{"location":"manual/codespaces/#control-panel","title":"Control panel","text":"<p>Whenever you need to check what Codespaces environments you have running or created, you can do it in the Codespaces panel.</p> <p></p> <p>The panel allows you to see and interact with the available Codespaces environments, including starting, stopping, and deleting them. You can also check what repositories are associated with each environment which is useful for a Containerlab user to quickly identify the lab environments.</p>"},{"location":"manual/codespaces/#codespaces-settings","title":"Codespaces settings","text":"<p>Codespaces expose a bunch of per-user settings at the github.com/settings/codespaces page. The following settings are worth mentioning:</p> Idle timeout and retention period Maybe the most important settings that you can configure in Codespaces. They allow you to control how long the environment will be running and when it will be deleted. Read more on this in Billing section Secrets Secrets allow you to store sensitive information that will be available to your Codespaces environments. You can use them to store API keys, passwords, and other sensitive data that you don't want to expose in your code. Setting Sync To make codespaces env feel like home, you can sync your settings across all your Codespaces environments. This includes themes, keybindings, and other settings that you have configured in your local VS Code instance. Editor preference You can choose if you want to run the codespaces in a browser, in a local VS Code instance, or via a bridge to a JetBrains IDE."},{"location":"manual/codespaces/#billing","title":"Billing","text":"<p>It is always a good idea to periodically check how much of the cpu-hours you've consumed and check the remaining quota. Your billing information is available in the Billing settings.</p> <p></p> <p>The screenshot shows that 10 cpu-hours out of 120 available were consumed in the current month' period and the codespaces environments occupy 8.15 GB of storage out of 15 GB included. So far it is all well within the free tier limits.<sup>5</sup></p> <p>Note</p> <p>All users by default have a $0 spending limit<sup>4</sup>, which means that if you exceed the free tier limits, your environments will be stopped and you will not be charged. You can change this limit to a higher value if you want to be able to use Codespaces even after you exceed the free tier limits.</p> <p>To avoid any surprises and lower your anxiety levels, GitHub Codespaces have two important settings that you configure at github.com/settings/codespaces:</p> <ol> <li>Idle timeout     This setting allows you to \"suspend\" the running environment after a certain period of inactivity and defaults to 30 minutes. You can increase/decrease the timeout as you see fit. Consult with the docs to see what counts as activity and what doesn't.</li> <li>Retention period     When you stopped the codespaces environment or it was suspended due to inactivity, it will be automatically deleted after a certain period of time. The default (and maximum) retention period is 30 days, but you can change it to a shorter period.     The stopped environment won't count against your cpu-hours quota, but it will still consume storage space, hence you might want to remove the stopped environments to free up the space.</li> </ol> <p>Safe settings</p> <p>To keep a tight control on the Codespaces free quota usage you can set the following in your Codespaces Settings:</p> <ul> <li>Idle timeout to 15 minutes</li> <li>Retention period to 1 day</li> </ul> <p>That way you can be sure that the environment is not running when you don't need it and it will be deleted after a day of inactivity saving up on the storage space.</p>"},{"location":"manual/codespaces/#adding-codespaces-to-your-lab","title":"Adding codespaces to your lab","text":"<p>By now you should be willing to try running your labs in Codespaces. To our luck, it is super simple, all you need to do is create a <code>.devcontainer/devcontainer.json</code> file in your lab repository that will define the Codespaces environment. The file should look similar to this:</p> <pre><code>{\n    \"image\": \"ghcr.io/srl-labs/containerlab/clab-devcontainer:0.55.0\",\n    \"hostRequirements\": {\n        \"cpus\": 4, // (1)!\n        \"memory\": \"8gb\",\n        \"storage\": \"32gb\"\n    }\n}\n</code></pre> <ol> <li>4-core machine type is used in this example, you can tune the machine type to fit your lab requirements. Maybe it will fit in a 2-core/4GB machine, or you need a beefier 8-core machine, it is up to you.</li> </ol> <p>For a complete Dev Container specification, check out the official docs.</p>"},{"location":"manual/codespaces/#image","title":"Image","text":"<p>The <code>image</code> field points to the Containerlab Dev Container image that would define your Codespaces environment. Containerlab provides devcontainer images, and you can see all available tags on the package' page.</p> <p>The image tag corresponds to the containerlab release version that is pre-installed in the image. You can choose the version that you want to use in your lab.</p>"},{"location":"manual/codespaces/#host-requirements","title":"Host requirements","text":"<p>Another important part of the <code>devcontainer.json</code> file is the <code>hostRequirements</code> field that defines the machine type that Codespaces environment will run on. Codespaces offer a small selection of machine types that differ in the number of CPUs, RAM, and storage. You can choose the machine type that fits your lab requirements.</p> <p>By the time of this writing (Jun 2024) the following machine types were available for GitHub users by default and beefier machines can be requested via GitHub support form.</p> Machine type CPU Memory (GB) Storage (GB) 2 core 2 8 32 4 core 4 16 32 8 core 8 32 64 16 core 16 64 128 <p>Using the machine types displayed above you can tune the <code>hostRequirements</code> section by choosing the machine type that fits the requirements of your lab.</p> <p>Note</p> <p>Codespaces VMs support nested virtualization, so you can run VM-based kinds </p>"},{"location":"manual/codespaces/#testing-the-environment","title":"Testing the environment","text":"<p>Once you added the <code>.devcontainer/devcontainer.json</code> file to your lab repository, you can test the environment locally and in Codespaces.</p>"},{"location":"manual/codespaces/#local-testing","title":"Local testing","text":"<p>Testing the environment locally is a litmus test to ensure that the <code>devcontainer.json</code> file is correct and the environment can be started. But since the environment runs locally, it doesn't test the Codespaces-specific settings like the machine type and CPU/RAM requirements.</p> <p>To deploy the environment locally, make sure you have Dev Containers VS Code extension installed and then use the VS Code command panel (<code>Cmd/Ctrl+Shift+P</code>) to execute <code>Dev Containers: Rebuild And Reopen In Container</code> action. This will trigger the VS Code to build the container and open the environment in the container.</p>"},{"location":"manual/codespaces/#remote-testing","title":"Remote testing","text":"<p>Once you tested the environment locally, you should test it in Codespaces to ensure that the selected machine type is sufficient for your lab.</p> <p>Hopefully you've been adding the Codespaces support in a git branch and created a PR for it. You can open the PR in the GitHub UI and click on the \"Code\" -&gt; \"Create codespace on codespaces\" button to start the environment for the branch you're working on:</p> <p></p> <p>You'll get the environment up and running in a couple of minutes and you can test it to ensure that it works as expected.</p>"},{"location":"manual/codespaces/#launching-the-environment","title":"Launching the environment","text":"<p>Once you are satisfied with the environment, you can add a nice button to the README file that will allow users to start the environment with a single click.</p> ButtonButton code <p> </p> <p>Run this lab in GitHub Codespaces for free. Learn more about Containerlab for Codespaces. Machine type: 2 vCPU \u00b7 8 GB RAM</p> <p>The URL used in the link uses deep link configuration provided by Codespaces, read more about it in the official docs.</p> <p>Do not forget to change the lab repo URL and machine type in the code below!</p> <pre><code>---\n&lt;div align=center markdown&gt;\n&lt;a href=\"https://codespaces.new/srl-labs/srlinux-vlan-handling-lab?quickstart=1\"&gt;\n&lt;img src=\"https://gitlab.com/rdodin/pics/-/wikis/uploads/d78a6f9f6869b3ac3c286928dd52fa08/run_in_codespaces-v1.svg?sanitize=true\" style=\"width:50%\"/&gt;&lt;/a&gt;\n\n**[Run](https://codespaces.new/srl-labs/srlinux-vlan-handling-lab?quickstart=1) this lab in GitHub Codespaces for free**.  \n[Learn more](https://containerlab.dev/manual/codespaces) about Containerlab for Codespaces.  \n&lt;small&gt;Machine type: 2 vCPU \u00b7 8 GB RAM&lt;/small&gt;\n&lt;/div&gt;\n\n---\n</code></pre> <p>Check out srl-labs/srl-streaming-telemetry README where this button is used to start the lab in Codespaces.</p> <p>And of course, you can always launch the Codespace using the GitHub UI by clicking on the \"Code\" button.</p> <p></p> <ol> <li>Click on the \"Code\" button in the GitHub UI.</li> <li>Start the Codespace on the <code>main</code> branch using the devcontainer settings defined in the <code>.devcontainer/devcontainer.json</code> file.</li> <li>Or open up an advanced menu</li> <li>And configure the Codespace settings manually.</li> </ol>"},{"location":"manual/codespaces/#dev-container","title":"Dev Container","text":"<p>The key pillar behind Codespaces is the Containerlab' Dev Container image that defines the environment in which the lab will run. The Dev Container image is a Docker image that contains all the necessary tools and dependencies to run Containerlab and other tools that you might need in the lab.</p> <p>The following two files define Containerlab's Dev Container image:</p> <ol> <li>devcontainer.json - the Dev Container configuration file that defines how the environment is built, configured and launched.</li> <li>Dockerfile - the Dockerfile that the Dev Container is built from.</li> </ol> <p>The resulting Dev Container image contains the following tools and dependencies:</p> <ul> <li>containerlab binary installed via the deb repository</li> <li>docker in docker setup</li> <li>gNMIc and gNOIc tools</li> <li>Go SDK</li> <li>Python 3 with pyenv</li> <li><code>gh</code> CLI tool</li> <li>zsh shell with oh-my-zsh configuration</li> <li>VS Code plugins</li> </ul>"},{"location":"manual/codespaces/#catalog-of-codespaces-enabled-labs","title":"Catalog of Codespaces-enabled labs","text":"<p>Having a codespace-enabled lab makes it super easy for users to start the lab and get to the fun part of the labbing.</p> <p>We encourage lab authors to add <code>codespaces</code> and <code>clab-topo</code> topics to the lab repository that supports Codespaces; that way users would be able to find the labs that they can run in Codespaces by following this link.</p>"},{"location":"manual/codespaces/#tips-tricks-and-known-issues","title":"Tips, tricks and known issues","text":""},{"location":"manual/codespaces/#authenticating-with-ghcrio-container-registry","title":"Authenticating with ghcr.io container registry","text":"<p>If you happen to have a private image that you want to use in Codespaces you can push this image to your personal GitHub registry.</p> <p>To be able to access a private image you would need to (re)authenticate with the <code>read:packages</code> token entitlement against the GitHub registry. Thankfully, it is a matter of a copy-paste exerceise.</p> <p>First, unset the existing token and request the one with <code>read:packages</code> capability:</p> <pre><code>unset GITHUB_TOKEN &amp;&amp; gh auth refresh -s read:packages\n</code></pre> <p>You will be prompted to authenticate with your GitHub account and the new token will be generated for you. Then you can login to the registry using the newly acquired token:</p> <pre><code>gh auth token | \\\ndocker login ghcr.io -u $(cat /home/vscode/.config/gh/hosts.yml | \\\ngrep user: | awk '{print $2}') --password-stdin\n</code></pre> <p>Of course you can also install tailscale or any other 0-tier VPN to access any other self-hosted private registry.</p> <ol> <li> <p>Check out the Dev Container section to learn more about the Containerlab' Dev Container package.\u00a0\u21a9</p> </li> <li> <p>At the moment of writing, GitHub Codespaces offer 120 cpu-hours/month and 15 GB storage for free to all users. See here for more details.\u00a0\u21a9</p> </li> <li> <p>A link points to the codespaces environment and refers a repo with the <code>.devcontainer</code> folder that defines the environment. For example: https://codespaces.new/srl-labs/srl-telemetry-lab?quickstart=1 \u21a9</p> </li> <li> <p>As indicated by the \"Montly spending limit\" text at the very bottom of the report table.\u00a0\u21a9</p> </li> <li> <p>You can also see message about when the quota reset happens.\u00a0\u21a9</p> </li> <li> <p>The runtime assumes no other environments are running at the same time and storage quota is not exceeded.\u00a0\u21a9</p> </li> <li> <p>The terms of the free plan may be subject to change, consult with the official documentation for the most recent information.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/conf-artifacts/","title":"Lab Directory and Configuration Artifacts","text":"<p>When containerlab deploys a lab it creates a Lab Directory in the same directory where your topology (<code>clab.yml</code>) file is. This directory is used to keep all the necessary files that are needed to run/configure the nodes. We call these files configuration artifacts and/or lab state files.</p> <p>Things like:</p> <ul> <li>CA certificate and node' TLS certificate and private keys</li> <li>node config file (if applicable and supported by the kind)</li> <li>node-specific files and directories that are required to launch the container</li> <li>license files if needed</li> </ul> <p>All these artifacts will be available under a Lab Directory.</p>"},{"location":"manual/conf-artifacts/#identifying-a-lab-directory","title":"Identifying a Lab Directory","text":"<p>The lab directory name follows the <code>clab-&lt;lab_name&gt;</code> template. Thus, if the name of your lab is <code>srl02</code> you will find the <code>clab-srl02</code> directory created by default in the directory where topology file is located. The location can be changed by setting the <code>CLAB_LABDIR_BASE</code> environment variable.</p> <pre><code>\u276f ls -lah clab-srl02\ntotal 4.0K\ndrwxr-xr-x  5 root root   40 Dec  1 22:11 .\ndrwxr-xr-x 23 root root 4.0K Dec  1 22:11 ..\ndrwxr-xr-x  5 root root   42 Dec  1 22:11 .tls\ndrwxr-xr-x  3 root root   79 Dec  1 22:11 srl1\ndrwxr-xr-x  3 root root   79 Dec  1 22:11 srl2\n</code></pre> <p>The contents of this directory will contain kind-specific files and directories. Containerlab will name directories after the node names and will only created those if they are needed. For instance, by default any node of kind <code>linux</code> will not have it's own directory under the Lab Directory.</p>"},{"location":"manual/conf-artifacts/#persistence-of-a-lab-directory","title":"Persistence of a Lab Directory","text":"<p>When a user first deploy a lab, the Lab Directory gets created if it was not present. Depending on a node's kind, this directory might act as a persistent storage area for a node. A common case is having the configuration file saved when the changes are made to the node via management interfaces.</p> <p>Below is an example of the <code>srl1</code> node directory contents. It keeps a directory that is mounted to containers configuration path, as well as stores additional files needed to launch and configure the node.</p> <pre><code>~/clab/clab-srl02\n\u276f ls -lah srl1\ndrwxrwxrwx+ 6 1002 1002   87 Dec  1 22:11 config\n-rw-r--r--  1 root root 2.8K Dec  1 22:11 license.key\n-rw-r--r--  1 root root 4.4K Dec  1 22:11 srlinux.conf\n-rw-r--r--  1 root root  233 Dec  1 22:11 topology.clab.yml\n</code></pre> <p>When a user destroys a lab without providing the <code>--cleanup</code> flag to the <code>destroy</code> command, the Lab Directory does not get deleted. This means that every configuration artifact will be kept on disk.</p> <p>Moreover, when the user will deploy the same lab, containerlab will reuse the configuration artifacts if possible, which will, for example, start the nodes with the config files saved from the previous lab run.</p> <p>To be able to deploy a lab without reusing existing configuration artifact use the <code>redeploy</code> command with <code>--cleanup</code> or add <code>--reconfigure</code> flag to the <code>deploy</code> command. With that setting, containerlab will first delete the Lab Directory and then will start the deployment process.</p>"},{"location":"manual/images/","title":"Image management","text":"<p>For a traditional networking lab orchestration system <code>containerlab</code> appears to be quite unique in a way that it runs containers, not VMs. This inherently means that container images need to be available to spin up the nodes.</p> <p>To keep things simple, containerlab adheres to the same principles of referencing container images as common tools like docker, podman, k8s do. The following example shows a clab file that references container images using various forms:</p> <pre><code>name: images\ntopology:\n  nodes:\n    node1:\n      # image from docker hub registry with implicit `latest` tag\n      image: alpine\n    node2:\n      # image from docker hub with explicit tag\n      image: ubuntu:20.04\n    node3:\n      # image from github registry\n      image: ghcr.io/hellt/network-multitool\n    node4:\n      # image from some private registry\n      image: myregistry.local/private/alpine:custom\n</code></pre> <p>When containerlab launches a lab, it reads the image name from the topology file and expects to find the referenced images locally or by pulling them from the registry.</p> <p>If in the example above, the image named <code>myregistry.local/private/alpine:custom</code> was not loaded to docker local image store before, containerlab will attempt to pull this image and will expect the private registry to be reachable.</p> <p>Container images offer a great flexibility and reproducibility of lab builds, to embrace it fully, we wanted to capture some basic image management operations and workflows in this article.</p>"},{"location":"manual/images/#tagging-images","title":"Tagging images","text":"<p>A container image name can appear in various forms. A short form of <code>alpine</code> will be expanded by docker daemon to <code>docker.io/alpine:latest</code>. At the same time an image named <code>myregistry.local/private/alpine:custom</code> is already a fully qualified name and indicates the container registry (<code>myregistry.local</code>) image repository name (<code>private/alpine</code>) and its tag (<code>custom</code>).</p> <p>With a <code>docker tag</code> command it is possible to \"rename\" an image to something else. This can be needed for various purposes, but most common needs are:</p> <ol> <li>rename the image so it can be pushed to another repository</li> <li>rename the image to users liking</li> </ol> <p>Let's imagine that we have a private repository from which we pulled the image with a name <code>registry.srlinux.dev/pub/vr-sros:20.10.R3</code>. By using this name in our clab file we can make use of this image in our lab. But that is quite a lengthy name, we might want to shorten it to something less verbose:</p> <pre><code># docker tag &lt;old-name&gt; &lt;new-name&gt;\ndocker tag registry.srlinux.dev/pub/vr-sros:20.10.R3 sros:20.10.R3\n</code></pre> <p>With that we make a new image named <code>sros:20.10.R3</code> that references the same original image. Now we can use the short name in our clab files.</p>"},{"location":"manual/images/#pushing-to-a-new-registry","title":"Pushing to a new registry","text":"<p>Same <code>docker tag</code> command can be used to rename the image so it can be pushed to another registry. For example consider the newly built SR OS 21.2.R1 vrnetlab image that by default will have a name of <code>vrnetlab/vr-sros:21.2.R1</code>. This container image can't be pushed anywhere in its current form, but retagging will help us out.</p> <p>If we wanted to push this image to a public registry like Github Container Registry, we could do the following:</p> <pre><code># retag the image to a fully qualified name that is suitable for\n# push to github container registry\nsudo docker tag vrnetlab/vr-sros:21.2.R1 ghcr.io/srl-labs/vr-sros:21.2.R1\n\n# and now we can push it\nsudo docker push ghcr.io/srl-labs/vr-sros:21.2.R1\n</code></pre>"},{"location":"manual/images/#exchanging-images","title":"Exchanging images","text":"<p>Container images are a perfect fit for sharing. Once anyone built an image with a certain NOS inside it can share it with anyone via container registry. Sensitive and proprietary images are typically pushed to private registries and internal users pull it from there.</p> <p>But sometimes you need to share an image with a colleague or your own setup that doesn't have access to a private registry. There are couple of ways to achieve that.</p>"},{"location":"manual/images/#as-zipped-tar-archive","title":"As zipped tar archive","text":"<p>A container image can be saved as <code>tar.gz</code> file that you can then share via various channels:</p> <pre><code>sudo docker save vrnetlab/vr-sros:21.2.R1 | xz -T 0 &gt; sros.tar.gz\n</code></pre> <p>Now you can push the tar.gz file to Google Drive, Dropbox, etc.</p> <p>On the receiving end you can load the container image:</p> <pre><code>sudo docker load -i sros.tar.gz\n</code></pre>"},{"location":"manual/images/#via-temp-registry","title":"Via temp registry","text":"<p>Another cool way of sharing a container image is via ttl.sh registry which offers a way to push an image to their public registry but the image will expire with a timeout you set.</p> <p>For example, let's push our image to the ttl.sh registry under a random name and make it expire in 15 minutes.</p> <pre><code># generate random 6 char sequence\nIMAGE=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 6 | head -n 1)\n# set ttl\nTTL=15m\n\n# tag and push\nsudo docker tag vrnetlab/vr-sros:21.2.R1 ttl.sh/$IMAGE:$TTL\nsudo docker push ttl.sh/$IMAGE:$TTL\necho \"pull the image with \\\"docker pull ttl.sh/$IMAGE:$TTL\\\" in the next $TTL\"\n</code></pre> <p>That is a very convenient way of sharing images with a small security compromise.</p>"},{"location":"manual/impairments/","title":"Link Impairments","text":"<p>Labs are meant to be a reflection of real-world scenarios. To make simulated networks exhibit real-life behavior you can set link impairments (delay, jitter, packet loss) on any link that belongs to a container node. Link impairment feature is powered by the <code>tools netem</code> command collection:</p> <ul> <li><code>tools netem set</code></li> <li><code>tools netem show</code></li> </ul> <p>These commands allow users to set link impairments (delay, jitter, packet loss) on any link that belongs to a container node and create labs simulating real-world network conditions.</p> setting packet loss at 10% rate on eth1 interface of clab-netem-r1 node<pre><code>containerlab tools netem set -n clab-netem-r1 -i eth1 --loss 10\n</code></pre>"},{"location":"manual/inventory/","title":"Inventory","text":"<p>To accommodate for smooth transition from lab deployment to subsequent automation activities, containerlab generates inventory files for different automation tools.</p>"},{"location":"manual/inventory/#ansible","title":"Ansible","text":"<p>Ansible inventory is generated automatically for every lab. The inventory file can be found in the lab directory under the <code>ansible-inventory.yml</code> name.</p> <p>Lab nodes are grouped under their kinds in the inventory so that the users can selectively choose the right group of nodes in the playbooks.</p> topology filegenerated Ansible inventory <pre><code>name: ansible\ntopology:\n  nodes:\n    r1:\n      kind: crpd\n      image: crpd:latest\n\n    r2:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux:latest\n\n    r3:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux:latest\n\n    grafana:\n      kind: linux\n      image: grafana/grafana:7.4.3\n</code></pre> <pre><code>all:\n  children:\n    crpd:\n      hosts:\n        clab-ansible-r1:\n          ansible_host: &lt;mgmt-ipv4-address&gt;\n    nokia_srlinux:\n      vars:\n        ansible_network_os: nokia.srlinux.srlinux\n        ansible_connection: ansible.netcommon.httpapi\n      hosts:\n        clab-ansible-r2:\n          ansible_host: &lt;mgmt-ipv4-address&gt;\n          ansible_user: admin\n          ansible_password: NokiaSrl1!\n        clab-ansible-r3:\n          ansible_host: &lt;mgmt-ipv4-address&gt;\n    linux:\n      hosts:\n        clab-ansible-grafana:\n          ansible_host: &lt;mgmt-ipv4-address&gt;\n</code></pre> <p>For certain node kinds containerlab sets default <code>ansible_network_os</code> and <code>ansible_connection</code> variables to enable plug-and-play experience with Ansible. As well as adding username and password known to containerlab as default credentials.</p>"},{"location":"manual/inventory/#removing-ansible_host-var","title":"Removing <code>ansible_host</code> var","text":"<p>If you want to use a plugin<sup>1</sup> that doesn't play well with the <code>ansible_host</code> variable injected by containerlab in the inventory file, you can leverage the <code>ansible-no-host-var</code> label. The label can be set on per-node, kind, or default levels; if set, containerlab will not generate the <code>ansible_host</code> variable in the inventory for the nodes with that label. Note that without the <code>ansible_host</code> variable, the connection plugin will use the <code>inventory_hostname</code> and resolve the name accordingly if network reachability is needed.</p> topology filegenerated Ansible inventory <pre><code>name: ansible\n  topology:\n    defaults:\n      labels:\n        ansible-no-host-var: \"true\"\n    nodes:\n      node1:\n      node2:\n</code></pre> <pre><code>all:\n  children:\n    linux:\n      hosts:\n        clab-ansible-node1:\n        clab-ansible-node2:\n</code></pre>"},{"location":"manual/inventory/#user-defined-groups","title":"User-defined groups","text":"<p>Users can enforce custom grouping of nodes in the inventory by adding the <code>ansible-inventory</code> label to the node definition:</p> <pre><code>name: custom-groups\ntopology:\n  nodes:\n    node1:\n      # &lt;some node config data&gt;\n      labels:\n        ansible-group: spine\n    node2:\n      # &lt;some node config data&gt;\n      labels:\n        ansible-group: extra_group\n</code></pre> <p>As a result of this configuration, the generated inventory will look like this:</p> <pre><code>  children:\n    srl:\n      hosts:\n        clab-custom-groups-node1:\n          ansible_host: 172.100.100.11\n        clab-custom-groups-node2:\n          ansible_host: 172.100.100.12\n    extra_group:\n      hosts:\n        clab-custom-groups-node2:\n          ansible_host: 172.100.100.12\n    spine:\n      hosts:\n        clab-custom-groups-node1:\n          ansible_host: 172.100.100.11\n</code></pre>"},{"location":"manual/inventory/#topology-data","title":"Topology Data","text":"<p>Every time a user runs a <code>deploy</code> command, containerlab automatically exports information about the topology into <code>topology-data.json</code> file in the lab directory. Schema of exported data is determined based on a Go template specified in <code>--export-template</code> parameter, or a default template <code>/etc/containerlab/templates/export/auto.tmpl</code>, if the parameter is not provided.</p> <p>Containerlab internal data that is submitted for export via the template, has the following structure:</p> <pre><code>type TopologyExport struct {\n Name        string                       `json:\"name\"`                  // Containerlab topology name\n Type        string                       `json:\"type\"`                  // Always 'clab'\n Clab        *CLab                        `json:\"clab,omitempty\"`        // Data parsed from a topology definitions yaml file\n NodeConfigs map[string]*types.NodeConfig `json:\"nodeconfigs,omitempty\"` // Definitions of nodes expanded with dynamically created data\n}\n</code></pre> <p>To get the full list of fields available for export, you can export topology data with the following template <code>--export-template /etc/containerlab/templates/export/full.tmpl</code>. Note, some fields exported via <code>full.tmpl</code> might contain sensitive information like TLS private keys. To customize export data, it is recommended to start with a copy of <code>auto.tmpl</code> and change it according to your needs.</p> <p>Example of exported data when using default <code>auto.tmpl</code> template:</p> topology file srl02.clab.ymlsample generated topology-data.json <pre><code>name: srl02\n\ntopology:\n  kinds:\n    srl:\n      type: ixrd3\n      image: ghcr.io/nokia/srlinux\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n    srl2:\n      kind: nokia_srlinux\n\n  links:\n    - endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n</code></pre> <pre><code>{\n  \"name\": \"srl02\",\n  \"type\": \"clab\",\n  \"clab\": {\n    \"config\": {\n      \"prefix\": \"clab\",\n      \"mgmt\": {\n        \"network\": \"clab\",\n        \"bridge\": \"br-&lt;...&gt;\",\n        \"ipv4-subnet\": \"172.20.20.0/24\",\n        \"ipv6-subnet\": \"3fff:172:20:20::/64\",\n        \"mtu\": \"1500\",\n        \"external-access\": true\n      },\n      \"config-path\": \"&lt;full path to a directory with srl02.clab.yml&gt;\"\n    }\n  },\n  \"nodes\": {\n    \"srl1\": {\n      \"index\": \"0\",\n      \"shortname\": \"srl1\",\n      \"longname\": \"clab-srl02-srl1\",\n      \"fqdn\": \"srl1.srl02.io\",\n      \"group\": \"\",\n      \"labdir\": \"&lt;full path to the lab node directory&gt;\",\n      \"kind\": \"srl\",\n      \"image\": \"ghcr.io/nokia/srlinux\",\n      \"mgmt-net\": \"\",\n      \"mgmt-intf\": \"\",\n      \"mgmt-ipv4-address\": \"172.20.20.3\",\n      \"mgmt-ipv4-prefix-length\": 24,\n      \"mgmt-ipv6-address\": \"3fff:172:20:20::3\",\n      \"mgmt-ipv6-prefix-length\": 64,\n      \"mac-address\": \"\",\n      \"labels\": {\n        \"clab-mgmt-net-bridge\": \"br-&lt;...&gt;\",\n        \"clab-node-group\": \"\",\n        \"clab-node-kind\": \"srl\",\n        \"clab-node-lab-dir\": \"&lt;full path to the lab node directory&gt;\",\n        \"clab-node-name\": \"srl1\",\n        \"clab-node-type\": \"ixrd3\",\n        \"clab-topo-file\": \"&lt;full path to the srl02.clab.yml file&gt;\",\n        \"containerlab\": \"srl02\"\n      }\n    },\n    \"srl2\": {\n      \"index\": \"1\",\n      \"shortname\": \"srl2\",\n      \"longname\": \"clab-srl02-srl2\",\n      \"fqdn\": \"srl2.srl02.io\",\n      \"group\": \"\",\n      \"labdir\": \"&lt;full path to the lab node directory&gt;\",\n      \"kind\": \"srl\",\n      \"image\": \"ghcr.io/nokia/srlinux\",\n      \"mgmt-net\": \"\",\n      \"mgmt-intf\": \"\",\n      \"mgmt-ipv4-address\": \"172.20.20.2\",\n      \"mgmt-ipv4-prefix-length\": 24,\n      \"mgmt-ipv6-address\": \"3fff:172:20:20::2\",\n      \"mgmt-ipv6-prefix-length\": 64,\n      \"mac-address\": \"\",\n      \"labels\": {\n        \"clab-mgmt-net-bridge\": \"br-&lt;...&gt;\",\n        \"clab-node-group\": \"\",\n        \"clab-node-kind\": \"srl\",\n        \"clab-node-lab-dir\": \"&lt;full path to the lab node directory&gt;\",\n        \"clab-node-name\": \"srl2\",\n        \"clab-node-type\": \"ixrd3\",\n        \"clab-topo-file\": \"&lt;full path to the srl02.clab.yml file&gt;\",\n        \"containerlab\": \"srl02\"\n      }\n    }\n  },\n  \"links\": [\n    {\n      \"a\": {\n        \"node\": \"srl1\",\n        \"interface\": \"e1-1\",\n        \"mac\": \"&lt;mac address&gt;\",\n        \"peer\": \"z\"\n      },\n      \"z\": {\n        \"node\": \"srl2\",\n        \"interface\": \"e1-1\",\n        \"mac\": \"&lt;mac address&gt;\",\n        \"peer\": \"a\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"manual/inventory/#ssh-config","title":"SSH Config","text":"<p>To simplify SSH access to the nodes started by Containerlab an SSH config file is generated per each deployed lab. The config file instructs SSH clients to not warn users about the changed host keys and also sets the username to the one known by Containerlab:</p> <code>/etc/ssh/ssh_config.d/clab-[lab-name].conf</code><pre><code># Containerlab SSH Config for the srl lab\n\nHost clab-srl-srl\n  User admin\n  StrictHostKeyChecking=no\n  UserKnownHostsFile=/dev/null\n</code></pre> <p>Now you can SSH to the nodes without being prompted to accept the host key and even omitting the username.</p> <pre><code>\u276f ssh clab-srl-srl\nWarning: Permanently added 'clab-srl-srl' (ED25519) to the list of known hosts.\n................................................................\n:                  Welcome to Nokia SR Linux!                  :\n:              Open Network OS for the NetOps era.             :\n:                                                              :\n:    This is a freely distributed official container image.    :\n:                      Use it - Share it                       :\n:                                                              :\n: Get started: https://learn.srlinux.dev                       :\n: Container:   https://go.srlinux.dev/container-image          :\n: Docs:        https://doc.srlinux.dev/23-7                    :\n: Rel. notes:  https://doc.srlinux.dev/rn23-7-1                :\n: YANG:        https://yang.srlinux.dev/release/v23.7.1        :\n: Discord:     https://go.srlinux.dev/discord                  :\n: Contact:     https://go.srlinux.dev/contact-sales            :\n................................................................\n\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl#\n</code></pre> <ol> <li> <p>For example Ansible Docker connection plugin.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/multi-node/","title":"Multi-node labs","text":"<p>Containerlab is a perfect tool of choice when all the lab components/nodes fit into one VM or bare metal server. Unfortunately, sometimes it is hard to satisfy this requirement and fit a big and sophisticated lab on a single host.</p> <p>Although containerlab is not (yet) capable of deploying topologies over a number of container hosts, we have embedded some capabilities that can help you to workaround the single-host resources constraint.</p>"},{"location":"manual/multi-node/#exposing-services","title":"Exposing services","text":"<p>Sometimes all that is needed is to make certain services running inside the nodes launched with containerlab available to a system running outside of the container host. For example, you might have an already running telemetry stack somewhere in your lab and you want to use it with the routing systems deployed with containerlab.</p> <p>In that case, the simple solution would be to expose the nodes' ports which are used to collect telemetry information. Take a look the following example where two nodes are defined in the topology file and get their gNMI port exposed to a host under a user-defined host-port.</p> <pre><code>name: telemetry\n\ntopology:\n  nodes:\n    ceos:\n      kind: ceos\n      image: ceos:latest\n      ports:\n        # host port 57401 is mapped to port 57400 of ceos node\n        - 57401:57400\n    srl:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n      ports:\n        - 57402:57400\n  links:\n    - endpoints: [\"ceos:eth1\", \"srl:e1-1\"]\n</code></pre> <p>Once the container's ports/services are exposed to a host under host-port, the telemetry collector running outside of the container host system can reach each node gNMI service.</p> <p>If container host has IP address of <code>$IP</code>, then telemetry collector can reach <code>ceos</code> telemetry service by <code>$IP:57401</code> address and <code>srl</code> gNMI service will be reachable via <code>$IP:57402</code>.</p>"},{"location":"manual/multi-node/#exposing-management-network","title":"Exposing management network","text":"<p>Exposing services on a per-port basis as shown above is a quick and easy way to make a certain service available via a host port, likely being the most common way of exposing services with containerlab. Unfortunately, not every use case can be covered with such approach.</p> <p>Imagine if you want to integrate an NMS system running elsewhere with a lab you launched with containerlab. Typically you would need to expose the entire management network for an NMS to start managing the nodes with management protocols required. In this scenario you wouldn't get far with exposing services via host-ports, as NMS would expect to have IP connectivity with the node it is about to adopt for managing.</p> <p>For integration tasks like this containerlab users can leverage static routing towards containerlab management network. Consider the following diagram:</p> <p>This solution requires to set up routing between the host which runs the NMS and the container host that has containerlab nodes inside. Since containers are always attached to a common management network, we can make this network reachable by installing, for example, a static route on the NMS host. This will provision the datapath between the NMS and the containerlab management network.</p> <p>By default, containerlab management network is addressed with <code>172.20.20./0</code> IPv4 address, but this can be easily changed to accommodate for network environment.</p>"},{"location":"manual/multi-node/#bridging","title":"Bridging","text":"<p>Previous examples were aiming management network access, but what if we need to rather connect a network interfaces of a certain node with a system running outside of the container host? An example for such connectivity requirement could be a traffic generator connected to a containerized node port.</p> <p>In this case we can leverage the bridge kind<sup>1</sup> that containerlab offers to connect container' interface to a pre-created bridge and slice the network with VLANs to create a L2 connectivity between the ports:</p>"},{"location":"manual/multi-node/#vxlan-tunneling","title":"VxLAN Tunneling","text":"<p>Sometimes VLAN bridging is not possible, for example when the other end of the virtual wire is reachable via routing, not bridging. We have developed a semi-automated solution for this case as well.</p> <p>The idea is to create unicast VxLAN tunnels between the VMs hosting nodes requiring connectivity.</p> <p>Refer to the multinode lab that goes deep in details on how to create this tunneling and explains the technicalities of such dataplane.</p> <ol> <li> <p>Both regular linux bridge and ovs-bridge kinds can be used, depending on the requirements.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/network/","title":"Network","text":"<p>One of the most important tasks in the process of building container based labs is to create a virtual wiring between the containers and the host. That is one of the problems that containerlab was designed to solve.</p> <p>In this document we will discuss the networking concepts that containerlab employs to provide the following connectivity scenarios:</p> <ol> <li>Make containers available from the lab host</li> <li>Interconnect containers to create network topologies of users choice</li> </ol>"},{"location":"manual/network/#management-network","title":"Management network","text":"<p>As governed by the well-established container networking principles containers are able to get network connectivity using various drivers/methods. The most common networking driver that is enabled by default for docker-managed containers is the bridge driver.</p> <p>The bridge driver connects containers to a linux bridge interface named <code>docker0</code> on most linux operating systems. The containers are then able to communicate with each other and the host via this virtual switch (bridge interface).</p> <p>In containerlab we follow a similar approach: containers launched by containerlab will be attached with their interface to a containerlab-managed docker network. It's best to be explained by an example which we will base on a two nodes lab from our catalog:</p> <pre><code>name: srl02\n\ntopology:\n  kinds:\n    nokia_srlinux:\n      type: ixrd3\n      image: ghcr.io/nokia/srlinux\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n    srl2:\n      kind: nokia_srlinux\n\n  links:\n    - endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n</code></pre> <p>As seen from the topology definition file, the lab consists of the two SR Linux nodes which are interconnected via a single point-to-point link.</p> <p>The diagram above shows that these two nodes are not only interconnected between themselves, but also connected to a bridge interface on the lab host. This is driven by the containerlab default management network settings.</p>"},{"location":"manual/network/#default-settings","title":"default settings","text":"<p>When no information about the management network is provided within the topo definition file, containerlab will do the following</p> <ol> <li>create, if not already created, a docker network named <code>clab</code></li> <li>configure the IPv4/6 addressing pertaining to this docker network</li> </ol> <p>Info</p> <p>We often refer to <code>clab</code> docker network simply as management network since it's the network to which management interfaces of the containerized NOS'es are connected.</p> <p>The addressing information that containerlab will use on this network:</p> <ul> <li>IPv4: subnet 172.20.20.0/24, gateway 172.20.20.1</li> <li>IPv6: subnet 3fff:172:20:20::/64, gateway 3fff:172:20:20::1</li> </ul> <p>This management network will be configured with MTU value matching the value of a <code>docker0</code> host interface to match docker configuration on the system. This option is configurable.</p> <p>With these defaults in place, the two containers from this lab will get connected to that management network and will be able to communicate using the IP addresses allocated by docker daemon. The addresses that docker carves out for each container are presented to a user once the lab deployment finishes or can be queried any time after:</p> <pre><code># addressing information is available once the lab deployment completes\n\u276f containerlab deploy -t srl02.clab.yml\n# deployment log omitted for brevity\n+---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| # |      Name       | Container ID |  Image  | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl  |       | running | 172.20.20.3/24 | 3fff:172:20:20::3/80 |\n| 2 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl  |       | running | 172.20.20.2/24 | 3fff:172:20:20::2/80 |\n+---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n\n# addresses can also be fetched afterwards with `inspect` command\n\u276f containerlab inspect -a\n+---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| # | Lab Name |      Name       | Container ID |  Image  | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| 1 | srl02    | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl  |       | running | 172.20.20.3/24 | 3fff:172:20:20::3/80 |\n| 2 | srl02    | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl  |       | running | 172.20.20.2/24 | 3fff:172:20:20::2/80 |\n+---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n</code></pre> <p>The output above shows that srl1 container has been assigned <code>172.20.20.3/24 / 3fff:172:20:20::3/80</code> IPv4/6 address. We can ensure this by querying the srl1 management interfaces address info:</p> <pre><code>\u276f docker exec clab-srl02-srl1 ip address show dummy-mgmt0\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 2a:66:2b:09:2e:4d brd ff:ff:ff:ff:ff:ff\n    inet 172.20.20.3/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever\n    inet6 3fff:172:20:20::3/80 scope global\n       valid_lft forever preferred_lft forever\n</code></pre> <p>Now it's possible to reach the assigned IP address from the lab host as well as from other containers connected to this management network.</p> <pre><code># ping srl1 management interface from srl2\n\u276f docker exec -it clab-srl02-srl2 sr_cli \"ping 172.20.20.3 network-instance mgmt\"\nx -&gt; $176x48\nUsing network instance mgmt\nPING 172.20.20.3 (172.20.20.3) 56(84) bytes of data.\n64 bytes from 172.20.20.3: icmp_seq=1 ttl=64 time=2.43 ms\n</code></pre> <p>Note</p> <p>If you run multiple labs without changing the default management settings, the containers of those labs will end up connecting to the same management network with their management interface.</p>"},{"location":"manual/network/#host-mode-networking","title":"host mode networking","text":"<p>In addition to the bridge-based management network containerlab supports launching nodes in host networking mode. In this mode containers are attached to the host network namespace. Host mode is enabled with network-mode node setting.</p>"},{"location":"manual/network/#configuring-management-network","title":"configuring management network","text":"<p>Most of the time there is no need to change the defaults for management network configuration, but sometimes it is needed. For example, it might be that the default network ranges are overlapping with the existing addressing scheme on the lab host, or it might be desirable to have predefined management IP addresses.</p> <p>For such cases, the users need to add the <code>mgmt</code> container at the top level of their topology definition file:</p> <pre><code>name: srl02\n\nmgmt:\n  network: custom_mgmt                # management network name\n  ipv4-subnet: 172.100.100.0/24       # ipv4 range\n  ipv6-subnet: 3fff:172:100:100::/80  # ipv6 range (optional)\n\ntopology:\n# the rest of the file is omitted for brevity\n</code></pre> <p>With these settings in place, the container will get their IP addresses from the specified ranges accordingly.</p>"},{"location":"manual/network/#user-defined-addresses","title":"user-defined addresses","text":"<p>By default, container runtime will assign the management IP addresses for the containers. But sometimes, it's helpful to have user-defined addressing in the management network.</p> <p>For such cases, users can define the desired IPv4/6 addresses on a per-node basis:</p> <pre><code>mgmt:\n  network: fixedips\n  ipv4-subnet: 172.100.100.0/24\n  ipv6-subnet: 3fff:172:100:100::/80\n\ntopology:\n  nodes:\n    n1:\n      kind: nokia_srlinux\n      mgmt-ipv4: 172.100.100.11       # set ipv4 address on management network\n      mgmt-ipv6: 3fff:172:100:100::11 # set ipv6 address on management network\n</code></pre> <p>Users can specify either IPv4 or IPv6 or both addresses. If one of the addresses is omitted, it will be assigned by container runtime in an arbitrary fashion.</p> <p>Note</p> <ol> <li>If user-defined IP addresses are needed, they must be provided for all containers attached to a given network to avoid address collision.</li> <li>IPv4/6 addresses set on a node level must be from the management network range.</li> <li>IPv6 addresses are truncated by Docker<sup>1</sup>, therefore do not use bytes 5 through 8 of the IPv6 network range.</li> </ol>"},{"location":"manual/network/#auto-assigned-addresses","title":"auto-assigned addresses","text":"<p>The default network addresses chosen by containerlab - 172.20.20.0/24 and 3fff:172:20:20::/64 - may clash with the existing addressing scheme on the lab host. With the user-defined addresses discussed above, users can avoid such conflicts, but this requires manual changes to the lab topology file and may not be convenient.</p> <p>To address this issue, containerlab provides a way to automatically assign the management network v4/v6 addresses. This is achieved by setting the <code>ipv4-subnet</code> and/or <code>ipv6-subnet</code> to <code>auto</code>:</p> <pre><code>mgmt:\n  ipv4-subnet: auto\n  ipv6-subnet: auto\n</code></pre> <p>With this setting in place, containerlab will rely on the container runtime to assign the management network addresses that is not conflicting with the existing addressing scheme on the lab host.</p>"},{"location":"manual/network/#mtu","title":"MTU","text":"<p>The MTU of the management network defaults to an MTU value of <code>docker0</code> interface, but it can be set to a user defined value:</p> <pre><code>mgmt:\n  network: clab_mgmt\n  mtu: 2100 # set mtu of the management network to 2100\n</code></pre> <p>This will result in every interface connected to that network to inherit this MTU value.</p>"},{"location":"manual/network/#network-name","title":"network name","text":"<p>The default container network name is <code>clab</code>. To customize this name, users should specify a new value within the <code>network</code> element:</p> <pre><code>mgmt:\n  network: myNetworkName\n</code></pre>"},{"location":"manual/network/#default-docker-network","title":"default docker network","text":"<p>To make clab nodes start in the default docker network <code>bridge</code>, which uses the <code>docker0</code> bridge interface, users need to mention this explicitly in the configuration:</p> <pre><code>mgmt:\n  network: bridge\n</code></pre> <p>Since <code>bridge</code> network is created by default by docker, using its name in the configuration will make nodes to connect to this network.</p>"},{"location":"manual/network/#bridge-name","title":"bridge name","text":"<p>By default, containerlab will create a linux bridge backing the management docker network with the following name <code>br-&lt;network-id&gt;</code>. The network-id part is coming from the docker network ID that docker manages.</p> <p>We allow our users to change the bridge name that the management network will use. This can be used to connect containers to an already existing bridge with other workloads connected:</p> <pre><code>mgmt:\n  # a bridge with a name mybridge will be created or reused\n  # as a backing bridge for the management network\n  bridge: mybridge\n</code></pre> <p>If the existing bridge has already been addressed with IPv4/6 address, containerlab will respect this address and use it in the IPAM configuration blob of the docker network.</p> <p>If there is no existing IPv4/6 address defined for the custom bridge, docker will assign the first interface from the subnet associated with the bridge.</p> <p>It is possible to set the desired gateway IP (that is the IP assigned to the bridge) with the <code>ipv4-gw/ipv6-gw</code> setting under <code>mgmt</code> container:</p> <pre><code>mgmt:\n  network: custom-net\n  bridge: mybridge\n  ipv4-subnet: 10.20.30.0/24 # ip range for the docker network\n  ipv4-gw: 10.20.30.100 # set custom gateway ip\n</code></pre>"},{"location":"manual/network/#ip-range","title":"IP range","text":"<p>By specifying <code>ipv4-range/ipv6-range</code> under the management network, users limit the network range from which IP addresses are allocated for a management subnet.</p> <pre><code>mgmt:\n  network: custom-net\n  ipv4-subnet: 10.20.30.0/24 #(2)!\n  ipv4-range: 10.20.30.128/25 #(1)!\n</code></pre> <ol> <li>Container runtime will assign IP addresses from the <code>10.20.30.128/25</code> subnet, and <code>10.20.30.0/25</code> will not be considered.</li> <li>The subnet must be specified for IP ranges to work. Also note that if the container network already exists and uses a different range, then the IP range setting won't have effect.</li> </ol> <p>With this approach, users can prevent IP address overlap with nodes deployed on the same management network by other orchestration systems.</p>"},{"location":"manual/network/#external-access","title":"external access","text":"<p>Containerlab will attempt to enable external management access to the nodes by default. This means that external systems/hosts will be able to communicate with the nodes of your topology without requiring any manual iptables/nftables rules to be installed.</p> <p>To allow external communications containerlab installs a rule in the <code>DOCKER-USER</code> chain for v4 and v6, allowing all packets targeting containerlab's management network. The rule looks like follows:</p> <pre><code>sudo iptables -vnL DOCKER-USER\n</code></pre> <pre><code>Chain DOCKER-USER (1 references)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 ACCEPT     all  --  *      br-a8b9fc8b33a2  0.0.0.0/0            0.0.0.0/0            /* set by containerlab */\n12719   79M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0\n</code></pre> <ol> <li>The <code>br-a8b9fc8b33a2</code> bridge interface is the interface that backs up the containerlab's management network (<code>clab</code> docker network).</li> </ol> <p>The rule will be removed together with the management network.</p> <p>RHEL 9 users</p> <p>By default RHEL 9 (and it's derivatives) will use <code>firewalld</code> as the default firewall, containerlab's <code>iptables</code> and <code>nftables</code> rules will not work in this case and you will not have external access to your labs.</p> <p>To fix this you must disable <code>firewalld</code> and enable the <code>nftables</code> service.  </p> <p>Take caution when disabling firewalls, you may be exposing things you shouldn't</p> <pre><code>systemctl disable firewalld\nsystemctl stop firewalld\nsystemctl mask firewalld\n\nsystemctl enable --now nftables\n</code></pre> <p>Should you not want to enable external access to your nodes you can set <code>external-access</code> property to <code>false</code> under the management section of a topology:</p> <pre><code>name: no-ext-access\nmgmt:\n  external-access: false #(1)!\ntopology:\n# your regular topology definition\n</code></pre> <ol> <li>When set to <code>false</code>, containerlab will not touch iptables rules. On most docker installations this will result in restricted external access.</li> </ol> Errors and warnings <p>External access feature requires nftables kernel API to be present. Kernels newer than v4 typically have this API enabled by default. To understand which API is in use one can issue the following command:</p> <pre><code>iptables -V\niptables v1.8.5 (nf_tables)\n</code></pre> <p>If the outputs contains <code>nf_tables</code> you are all set. If it contains <code>legacy</code> or doesn't say anything about <code>nf_tables</code> then nf_tables API is not available and containerlab will not be able to setup external access. You will have to enable it manually (or better yet - upgrade the kernel). Older distros, like Centos 7, are known to use the legacy iptables backend and therefore will emit a warning when containerlab will attempt to launch a lab. The warning will not prevent the lab from starting and running, but you will need to setup iptables rules manually if you want your nodes to be accessible from the outside of your containerlab host.</p> <p>Containerlab will throw an error \"missing DOCKER-USER iptables chain\" when this chain is not found. This error is typically caused by two factors</p> <ol> <li>Old docker version installed. Typically seen on Centos systems. Minimum required docker version is 17.06.</li> <li>Docker is installed incorrectly. It is recommended to follow the official installation procedures by selecting \"Installation per distro\" menu option.</li> </ol> <p>When docker is correctly installed, additional iptables chains will become available and the error will not appear.</p>"},{"location":"manual/network/#connection-details","title":"connection details","text":"<p>When containerlab needs to create the management network, it asks the docker daemon to do this. Docker will fulfill the request and will create a network with the underlying linux bridge interface backing it. The bridge interface name is generated by the docker daemon, but it is easy to find it:</p> <pre><code># list existing docker networks\n# notice the presence of the `clab` network with a `bridge` driver\n\u276f docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n5d60b6ec8420        bridge              bridge              local\nd2169a14e334        clab                bridge              local\n58ec5037122a        host                host                local\n4c1491a09a1a        none                null                local\n\n# the underlying linux bridge interface name follows the `br-&lt;first_12_chars_of_docker_network_id&gt; pattern\n# to find the network ID use:\n\u276f docker network inspect clab -f {{.ID}} | head -c 12\nd2169a14e334\n\n# now the name is known and it's easy to show bridge state\n\u276f brctl show br-d2169a14e334\nbridge name         bridge id      STP enabled   interfaces\nbr-d2169a14e334  8000.0242fe382b74 no        vetha57b950\n                                 vethe9da10a\n</code></pre> <p>As explained in the beginning of this article, containers will connect to this docker network. This connection is carried out by the <code>veth</code> devices created and attached with one end to bridge interface in the lab host and the other end in the container namespace. This is illustrated by the bridge output above and the diagram at the beginning the of the article.</p>"},{"location":"manual/network/#point-to-point-links","title":"Point-to-point links","text":"<p>Management network is used to provide management access to the NOS containers, it does not carry control or dataplane traffic. In containerlab we create additional point-to-point links between the containers to provide the datapath between the lab nodes.</p> <p>The above diagram shows how links are created in the topology definition file. In this example, the datapath consists of the two virtual point-to-point wires between SR Linux and cEOS containers. These links are created on-demand by containerlab itself.</p> <p>The p2p links are typically provided by the <code>veth</code> device pairs where each end of the <code>veth</code> pair is attached to a respective container.</p>"},{"location":"manual/network/#link-mtu","title":"Link MTU","text":"<p>The MTU on the veth links is set by default to 9500B, so a regular jumbo frame shouldn't traverse the links without problems. If you need to change the MTU, you can do so by setting the <code>mtu</code> property in the link definition:</p> <pre><code>topology:\n  links:\n    - endpoints: [\"router2:eth2\", \"router3:eth1\"]\n      mtu: 1500\n</code></pre>"},{"location":"manual/network/#host-links","title":"Host links","text":"<p>It is also possible to interconnect container' data interface not with other container or add it to a bridge, but to attach it to a host's root namespace. This is, for example, needed to create a L2 connectivity between containerlab nodes running on different VMs (aka multi-node labs).</p> <p>This \"host-connectivity\" is achieved by using a reserved node name - <code>host</code> - referenced in the endpoints section. Consider the following example where an SR Linux container has its only data interface connected to a hosts root namespace via veth interface:</p> <pre><code>name: host\n\ntopology:\n  nodes:\n    srl:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n      startup-config: test-srl-config.json\n  links:\n    - endpoints: [\"srl:e1-1\", \"host:srl_e1-1\"]\n</code></pre> <p>With this topology definition, we will have a veth interface with its one end in the container' namespace and its other end in the host namespace. The host will have the interface named <code>srl_e1-1</code> once the lab deployed:</p> <pre><code>ip link\n# SNIP\n433: srl_e1-1@if434: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default \n    link/ether b2:80:e9:60:c7:9d brd ff:ff:ff:ff:ff:ff link-netns clab-srl01-srl\n</code></pre>"},{"location":"manual/network/#additional-connections-to-management-network","title":"Additional connections to management network","text":"<p>By default every lab node will be connected to the docker network named <code>clab</code> which acts as a management network for the nodes.</p> <p>In addition to that mandatory connection, users can attach additional interfaces to this management network. This might be needed, for example, when data interface of a node needs to talk to the nodes on the management network.</p> <p>For such connections a special form of endpoint definition was created - <code>mgmt-net:$iface-name</code>.</p> <pre><code>name: mgmt\ntopology:\n  nodes:\n    n1:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n  links:\n    - endpoints:\n        - \"n1:e1-1\"\n        - \"mgmt-net:n1-e1-1\"\n</code></pre> <p>In the above example the node <code>n1</code> connects with its <code>e1-1</code> interface to the management network. This is done by specifying the endpoint with a reserved name <code>mgmt-net</code> and defining the name of the interface that should be used in that bridge (<code>n1-e1-1</code>).</p> <p>By specifying <code>mgmt-net</code> name of the node in the endpoint definition we tell containerlab to find out which bridge is used by the management network of our lab and use this bridge as the attachment point for our veth pair.</p> <p>This is best illustrated with the following diagram:</p>"},{"location":"manual/network/#macvlan-links","title":"MACVLAN links","text":"<p>In addition to the <code>veth</code> links, containerlab supports <code>macvlan</code> links. This type of links is useful when users want to connect containers to the host interface/network directly. This is achieved by defining a link endpoints which has one end defined with a special <code>macvlan:&lt;host-iface-name&gt;</code> signature.</p> <p>Consider the following example where we connect a Linux container <code>l1</code> to the hosts <code>enp0s3</code> interface:</p> <pre><code>name: macvlan\n\ntopology:\n  nodes:\n    l1:\n      kind: linux\n      image: alpine:3\n\n  links:\n    - endpoints: [\"l1:eth1\", \"macvlan:enp0s3\"]\n</code></pre> <p>This topology will result in l1 node having its <code>eth1</code> interface connected to the <code>enp0s3</code> interface of the host as per the diagram below:</p> <p>Containerlab will create a macvlan interface in the bridge mode, attach it to the parent <code>enp0s3</code> interface and then move it to the container's net namespace and name it <code>eth1</code>, as instructed by the endpoint definition in the topology.</p> <p>Users then can configure the <code>eth1</code> interface inside the container as they would do with any other interface. As per the diagram above, we configure <code>eth1</code> interface with ipv4 address from the host's <code>enp0s3</code> interface subnet:</p> entering the container's shell<pre><code>docker exec -it clab-macvlan-l1 ash\n</code></pre> <pre><code># adding v4 address to the eth1 interface\nip address add 10.0.0.111/24 dev eth1\n</code></pre> <p>Once v4 address is assigned to the macvlan inteface, we can test the connectivity by pinging default gateway of the host:</p> <pre><code>\u276f ping 10.0.0.1\nPING 10.0.0.1 (10.0.0.1): 56 data bytes\n64 bytes from 10.0.0.1: seq=0 ttl=64 time=0.545 ms\n64 bytes from 10.0.0.1: seq=1 ttl=64 time=0.243 ms\n</code></pre> <p>When capturing packets from the hosts's <code>enp0s3</code> interface we can see that the ping packets are coming through it using the mac address assigned to the macvlan inteface:</p> <pre><code>\u276f tcpdump -nnei enp0s3 icmp\ntcpdump: verbose output suppressed, use -v[v]... for full protocol decode\nlistening on enp0s3, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n22:35:02.430901 aa:c1:ab:72:b3:fe &gt; fa:16:3e:af:03:05, ethertype IPv4 (0x0800), length 98: 10.0.0.111 &gt; 10.0.0.1: ICMP echo request, id 24, seq 4, length 64\n22:35:02.431017 fa:16:3e:af:03:05 &gt; aa:c1:ab:72:b3:fe, ethertype IPv4 (0x0800), length 98: 10.0.0.1 &gt; 10.0.0.111: ICMP echo reply, id 24, seq 4, length 64\n</code></pre>"},{"location":"manual/network/#manual-control-over-the-management-network","title":"Manual control over the management network","text":"<p>By default containerlab creates a docker network named <code>clab</code> and attaches all the nodes to this network. This network is used as a management network for the nodes and is managed by the container runtime such as docker or podman.</p> <p>Container runtime is responsible for creating the <code>eth0</code> interface inside the container and attaching it to the <code>clab</code> network. This interface is used by the container to communicate with the management network. For that reason the links users create in the topology's <code>links</code> section typically do not include the <code>eth0</code> interface.</p> <p>However, there might be cases when users want to take control over <code>eth0</code> interface management. For example, they might want to connect <code>eth0</code> to a different network or even another container's interface. To achieve that, users can instruct container runtime to not manage the <code>eth0</code> interface and leave it to the user, using <code>network-mode: none</code> setting.</p> <p>Consider the following example, where node1's management interface <code>eth0</code> is provided in the <code>links</code> section and connects to node2's <code>eth1</code> interface:</p> <pre><code>name: e0\n\ntopology:\n  nodes:\n    node1:\n      kind: linux\n      image: alpine:3\n      network-mode: none\n    node2:\n      kind: linux\n      image: alpine:3\n  links:\n    - endpoints: [\"node1:eth0\", \"node2:eth1\"]\n</code></pre>"},{"location":"manual/network/#dns","title":"DNS","text":"<p>When containerlab finishes the nodes deployment, it also creates static DNS entries inside the <code>/etc/hosts</code> file so that users can access the nodes using their DNS names.</p> <p>The DNS entries are created for each node's IPv4/6 address, and follow the pattern - <code>clab-$labName-$nodeName</code>.</p> <p>For a lab named <code>demo</code> with two nodes named <code>l1</code> and <code>l2</code> containerlab will create the following section inside the <code>/etc/hosts</code> file.</p> <pre><code>###### CLAB-demo-START ######\n172.20.20.2     clab-demo-l1\n172.20.20.3     clab-demo-l2\n3fff:172:20:20::2       clab-demo-l1\n3fff:172:20:20::3       clab-demo-l2\n###### CLAB-demo-END ######\n</code></pre> <ol> <li> <p>See https://github.com/srl-labs/containerlab/issues/1302#issuecomment-1533796941 for details and links to the original discussion.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/node-filtering/","title":"Node filtering","text":"<p>With the node filtering feature containerlab enables users to perform operations on a subset of nodes defined in the topology file. A typical use case is to deploy only a subset of nodes in a lab to save deployment time and host resources.</p> <p>Consider the following topology file that defines 4 interconnected nodes:</p> <pre><code>name: filter\n\ntopology:\n  defaults:\n    image: alpine:3\n  nodes:\n    node1:\n    node2:\n    node3:\n    node4:\n\n  links:\n    - endpoints: [\"node1:eth1\", \"node2:eth1\"]\n    - endpoints: [\"node1:eth2\", \"node3:eth2\"]\n    - endpoints: [\"node1:eth3\", \"node4:eth3\"]\n    - endpoints: [\"node2:eth2\", \"node4:eth2\"]\n    - endpoints: [\"node3:eth1\", \"node4:eth1\"]\n</code></pre> <p>A graphical representation of the topology forms a ring with 4 nodes and 5 links between them:</p>"},{"location":"manual/node-filtering/#deploying-a-subset-of-nodes","title":"Deploying a subset of nodes","text":"<p>By default, all nodes and their links are deployed when the <code>deploy</code> command is executed. But what if each node is a massive VM that consumes a lot of RAM and you want to launch just a few of them? This use case becomes even more relevant when you have a large lab with many nodes and isolated domains.</p> <p>For the sake of this example, let's assume that we want to deploy only nodes <code>node1</code>, <code>node2</code>, and <code>node4</code>, representing a subring. To do that, we can use the <code>--node-filter</code> flag and provide a comma-separated list of nodes names to deploy:</p> <pre><code>clab deploy --node-filter node1,node2,node4\n</code></pre> <p>As a result of this command, only nodes <code>node1</code>, <code>node2</code>, and <code>node4</code> will be deployed, and the links between them will be created. The remaining nodes and links will be ignored.</p> <p>When filtering the nodes to <code>node1</code> and <code>node2</code> the topology becomes a linear chain:</p>"},{"location":"manual/node-filtering/#destroying-a-subset-of-nodes","title":"Destroying a subset of nodes","text":"<p>The <code>destroy</code> command can also be scoped to a subset of nodes. The same <code>--node-filter</code> flag can be used to specify the nodes to destroy. For example, to destroy only nodes <code>node1</code> and <code>node2</code> from the previous example, we can run:</p> <pre><code>clab destroy --node-filter node1,node2\n</code></pre> <p>And only these two nodes will be destroyed (with all links connected to them), leaving the rest of the lab intact.</p>"},{"location":"manual/node-filtering/#other-commands","title":"Other commands","text":"<p>The following commands have support for <code>--node-filter</code> flag:</p> <ul> <li><code>graph</code></li> <li><code>save</code></li> <li><code>config</code></li> </ul>"},{"location":"manual/nodes/","title":"Nodes","text":"<p>Node object is one of the containerlab' pillars. Essentially, it is nodes and links what constitute the lab topology. To let users build flexible and customizable labs the nodes are meant to be configurable.</p> <p>The node configuration is part of the topology definition file and may consist of the following fields that we explain in details below.</p> <pre><code># part of topology definition file\ntopology:\n  nodes:\n    node1:  # node name\n      kind: nokia_srlinux\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n      startup-config: /root/mylab/node1.cfg\n      binds:\n        - /usr/local/bin/gobgp:/root/gobgp\n        - /root/files:/root/files:ro\n      ports:\n      - 80:8080\n      - 55555:43555/udp\n      - 55554:43554/tcp\n      user: test\n      env:\n        ENV1: VAL1\n      cmd: /bin/bash script.sh\n</code></pre>","boost":6},{"location":"manual/nodes/#kind","title":"kind","text":"<p>The <code>kind</code> property selects which kind this node is of. Kinds are essentially a way of telling containerlab how to treat the nodes properties considering the specific flavor of the node. We dedicated a separate section to discuss kinds in details.</p> <p>Note</p> <p>Kind must be defined either by setting the kind for a node specifically (as in the example above), or by setting the default kind: <pre><code>topology:\n  defaults:\n    kind: nokia_srlinux\n  nodes:\n    node1:\n    # kind value of `srl` is inherited from defaults section\n</code></pre></p>","boost":6},{"location":"manual/nodes/#type","title":"type","text":"<p>With <code>type</code> the user sets a type of the node. Types work in combination with the kinds, such as the type value of <code>ixrd2l</code> sets the chassis type for SR Linux node, thus this value only makes sense to nodes of kind <code>nokia_srlinux</code>.</p> <p>Other nodes might treat <code>type</code> field differently, that will depend on the kind of the node. The <code>type</code> values and effects defined in the documentation for a specific kind.</p>","boost":6},{"location":"manual/nodes/#group","title":"group","text":"<p><code>group</code> is a freeform string that denotes which group a node belongs to. The grouping is currently only used to sort topology elements on a graph.</p>","boost":6},{"location":"manual/nodes/#image","title":"image","text":"<p>The common <code>image</code> attribute sets the container image name that will be used to start the node. The image name should be provided in a well-known format of <code>repository(:tag)</code>.</p> <p>We use <code>&lt;repository&gt;</code> image name throughout the docs articles. This means that the image with <code>&lt;repository&gt;:latest</code> name will be looked up. A user will need to add the latest tag if they want to use the same loose-tag naming:</p> <pre><code># tagging srlinux:20.6.1-286 as srlinux:latest\n# after this change it's possible to use `srlinux:latest` or `srlinux` image name\ndocker tag srlinux:20.6.1-286 srlinux:latest\n</code></pre>","boost":6},{"location":"manual/nodes/#image-pull-policy","title":"image-pull-policy","text":"<p>With <code>image-pull-policy</code> a user defines the container image pull policy.</p> <p>Valid values are:</p> <ul> <li><code>IfNotPresent</code> - Pull container image if it is not already present. (If e.g. the <code>:latest</code> tag has been updated on a remote registry, containerlab will not re-pull it, if an image with the same tag is already present)</li> <li><code>Never</code> - Do not at all try to pull the image from a registry. An error will be thrown and the execution is stopped if the image is not available locally.</li> <li><code>Always</code> - Always try to pull the new image from a registry. An error will be thrown if pull fails. This will ensure fetching latest image version even if it exists locally.</li> </ul> <p>The default value is <code>IfNotPresent</code>.</p> <pre><code>topology:\n  nodes:\n    srl:\n      image: ghcr.io/nokia/srlinux\n      image-pull-policy: Always\n</code></pre>","boost":6},{"location":"manual/nodes/#restart-policy","title":"restart-policy","text":"<p>With <code>restart-policy</code> a user defines the restart policy of a container as per docker docs.</p> <p>Valid values are:</p> <ul> <li><code>no</code> - Don't automatically restart the container.</li> <li><code>on-failure</code> - Restart the container if it exits due to an error, which manifests as a non-zero exit code. The on-failure policy only prompts a restart if the container exits with a failure. It doesn't restart the container if the daemon restarts.</li> <li><code>always</code> - Always restart the container if it stops. If it's manually stopped, it's restarted only when Docker daemon restarts or the container itself is manually restarted.</li> <li><code>unless-stopped</code> Similar to always, except that when the container is stopped (manually or otherwise), it isn't restarted even after Docker daemon restarts.</li> </ul> <p><code>no</code> is the default restart policy value for all kinds, but <code>linux</code>. Linux kind defaults to <code>always</code>.</p> <pre><code>topology:\n  nodes:\n    srl:\n      image: ghcr.io/nokia/srlinux\n      kind: nokia_srlinux\n      restart-policy: always\n    alpine:\n      kind: linux\n      image: alpine\n      restart-policy: \"no\"\n</code></pre>","boost":6},{"location":"manual/nodes/#license","title":"license","text":"<p>Some containerized NOSes require a license to operate or can leverage a license to lift-off limitations of an unlicensed version. With <code>license</code> property a user sets a path to a license file that a node will use. The license file will then be mounted to the container by the path that is defined by the <code>kind/type</code> of the node.</p>","boost":6},{"location":"manual/nodes/#startup-config","title":"startup-config","text":"<p>For all Network OS kinds, it's possible to provide startup configuration that the node applies on boot. The startup config can be provided in two ways:</p> <ol> <li>As a path to a file that is available on the host machine and contains the config blob that the node understands.</li> <li>As an embedded config blob that is provided as a multiline string.</li> </ol>","boost":6},{"location":"manual/nodes/#path-to-a-startup-config-file","title":"path to a startup-config file","text":"<p>When a path to a startup-config file is provided, containerlab either mounts the file to the container by a path that NOS expects to have its startup-config file, or it will apply the config via using the NOS-dependent method.</p> <pre><code>topology:\n  nodes:\n    srl:\n      startup-config: ./some/path/to/startup-config.cfg\n</code></pre> <p>Check the particular kind documentation to see if the startup-config is supported and how it is applied.</p> Startup-config path variable <p>By default, the startup-config references are either provided as an absolute or a relative (to the current working dir) path to the file to be used.</p> <p>Consider a two-node lab <code>mylab.clab.yml</code> with seed configs that the user may wish to use in their lab. A user could create a directory for such files similar to this:</p> <pre><code>.\n\u251c\u2500\u2500 cfgs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node1.partial.cfg\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 node2.partial.cfg\n\u2514\u2500\u2500 mylab.clab.yml\n\n2 directories, 3 files\n</code></pre> <p>Then to leverage these configs, the node could be configured with startup-config references like this:</p> <pre><code>name: mylab\ntopology:\n  nodes:\n    node1:\n      startup-config: cfgs/node1.partial.cfg\n    node2:\n      startup-config: cfgs/node2.partial.cfg\n</code></pre> <p>while this configuration is correct, it might be considered verbose as the number of nodes grows. To remove this verbosity, the users can use a special variable <code>__clabNodeName__</code> in their startup-config paths. This variable will expand to the node-name for the parent node that the startup-config reference falls under.</p> <pre><code>name: mylab\ntopology:\n  nodes:\n    node1:\n      startup-config: cfg/__clabNodeName__.partial.cfg\n    node2:\n      startup-config: cfgs/__clabNodeName__.partial.cfg\n</code></pre> <p>The <code>__clabNodeName__</code> variable can also be used in the kind and default sections of the config.  Using the same directory structure from the example above, the following shows how to use the magic variable for a kind.</p> <pre><code>name: mylab\ntopology:\n  defaults:\n    kind: nokia_srlinux\n  kinds:\n    nokia_srlinux:\n      startup-config: cfgs/__clabNodeName__.partial.cfg\n  nodes:\n    node1:\n    node2:\n</code></pre> <p>The following example shows how one would do it using defaults.</p> <pre><code>name: mylab\ntopology:\n  defaults:\n    kind: nokia_srlinux\n    startup-config: cfgs/__clabNodeName__.partial.cfg\n  nodes:\n    node1:\n    node2:\n</code></pre>","boost":6},{"location":"manual/nodes/#embedded-startup-config","title":"embedded startup-config","text":"<p>It is possible to embed the startup configuration in the topology file itself. This is done by providing the startup-config as a multiline string.</p> <pre><code>topology:\n  nodes:\n    srl:\n      startup-config: |\n        system information location \"I am embedded config\"\n</code></pre> <p>Note</p> <p>If a config file exists in the lab directory for a given node, then it will take preference over the startup config passed with this setting. If it is desired to discard the previously saved config and use the startup config instead, use the <code>enforce-startup-config</code> setting or deploy a lab with the <code>reconfigure</code> flag.</p>","boost":6},{"location":"manual/nodes/#remote-startup-config","title":"remote startup-config","text":"<p>It is possible to specify a remote <code>http(s)</code> location for a startup-config file. Simply provide a URL that can be accessed from the containerlab host.</p> <pre><code>topology:\n  kinds:\n    nokia_srlinux:\n      type: ixrd3\n      image: ghcr.io/nokia/srlinux\n      startup-config: https://raw.githubusercontent.com/srl-labs/containerlab/main/tests/02-basic-srl/srl2-startup.cli\n</code></pre> <p>The remote file will be downloaded to the containerlab's temp directory at <code>$TMP/.clab/&lt;filename&gt;</code> path and provided to the node as a locally available startup-config file. The filename will have a generated name that follows the pattern <code>&lt;lab-name&gt;-&lt;node-name&gt;-&lt;filename-from-url&gt;</code>, where <code>&lt;filename-from-url&gt;</code> is the last element of the URL path.</p> <p>Note</p> <ul> <li>Upon deletion of a lab, the downloaded startup-config files will not be removed. A manual cleanup should be performed if required.</li> <li>If a lab is redeployed with the lab name and startup-config paths unchanged, the local file will be overwritten.</li> <li>For https locations the certificates won't be verified to allow fetching artifacts from servers with self-signed certificates.</li> </ul>","boost":6},{"location":"manual/nodes/#enforce-startup-config","title":"enforce-startup-config","text":"<p>By default, containerlab will use the config file that is available in the lab directory for a given node even if the <code>startup config</code> parameter points to another file. To make a node to boot with the config set with <code>startup-config</code> parameter no matter what, set the <code>enforce-startup-config</code> to <code>true</code>.</p>","boost":6},{"location":"manual/nodes/#suppress-startup-config","title":"suppress-startup-config","text":"<p>By default, containerlab will create a startup-config when initially creating a lab.  To prevent a startup-config file from being created (in a Zero-Touch Provisioning lab, for example), set the <code>suppress-startup-config</code> to <code>true</code>.</p>","boost":6},{"location":"manual/nodes/#auto-remove","title":"auto-remove","text":"<p>By default, containerlab will not remove the failed or stopped nodes so that you can read their logs and understand the reason of a failure. If it is required to remove the failed/stopped nodes, use <code>auto-remove: true</code> property.</p> <p>The property can be set on all topology levels.</p>","boost":6},{"location":"manual/nodes/#startup-delay","title":"startup-delay","text":"<p>To make certain node(s) to boot/start later than others use the <code>startup-delay</code> config element that accepts the delay amount in seconds.</p> <p>This setting can be applied on node/kind/default levels.</p>","boost":6},{"location":"manual/nodes/#binds","title":"binds","text":"<p>Users can leverage the bind mount capability to expose host files to the containerized nodes.</p> <p>Binds instructions are provided under the <code>binds</code> container of a default/kind/node configuration section. The format of those binding instructions follows the same of the docker's --volume parameter.</p> <pre><code>topology:\n  nodes:\n    testNode:\n      kind: linux\n      # some other node parameters\n      binds:\n        - /usr/local/bin/gobgp:/root/gobgp # (1)!\n        - /root/files:/root/files:ro # (2)!\n        - somefile:/somefile # (3)!\n        - ~/.ssh/id_rsa:/root/.ssh/id_rsa # (4)!\n        - /var/run/somedir # (5)!\n</code></pre> <ol> <li>mount a host file found by the path <code>/usr/local/bin/gobgp</code> to a container under <code>/root/gobgp</code> (implicit RW mode)</li> <li>mount a <code>/root/files</code> directory from a host to a container in RO mode</li> <li>when a host path is given in a relative format, the path is considered relative to the topology file and not a current working directory.</li> <li>The <code>~</code> char will be expanded to a user's home directory.</li> <li>mount an anonymous volume to a container under <code>/var/run/somedir</code> (implicit RW mode)</li> </ol> Bind variables <p>By default, binds are either provided as an absolute or a relative (to the current working dir) path. Although the majority of cases can be very well covered with this, there are situations in which it is desirable to use a path that is relative to the node-specific example.</p> <p>Consider a two-node lab <code>mylab.clab.yml</code> with node-specific files, such as state information or additional configuration artifacts. A user could create a directory for such files similar to that:</p> <pre><code>.\n\u251c\u2500\u2500 cfgs\n\u2502   \u251c\u2500\u2500 n1\n\u2502   \u2502   \u2514\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 n2\n\u2502       \u2514\u2500\u2500 conf\n\u2514\u2500\u2500 mylab.clab.yml\n\n3 directories, 3 files\n</code></pre> <p>Then to mount those files to the nodes, the nodes would have been configured with binds like that:</p> <pre><code>name: mylab\ntopology:\n  nodes:\n    n1:\n      binds:\n        - cfgs/n1/conf:/conf\n    n2:\n      binds:\n        - cfgs/n2/conf:/conf\n</code></pre> <p>while this configuration is correct, it might be considered verbose as the number of nodes grows. To remove this verbosity, the users can use a special variable <code>__clabNodeDir__</code> in their bind paths. This variable will expand to the node-specific directory that containerlab creates for each node.</p> <p>This means that you can create a directory structure that containerlab will create anyhow and put the needed files over there. With the lab named <code>mylab</code> and the nodes named <code>n1</code> and <code>n2</code> the structure containerlab uses is as follows:</p> <pre><code>.\n\u251c\u2500\u2500 clab-mylab\n\u2502   \u251c\u2500\u2500 n1\n\u2502   \u2502   \u2514\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 n2\n\u2502       \u2514\u2500\u2500 conf\n\u2514\u2500\u2500 mylab.clab.yml\n\n3 directories, 3 files\n</code></pre> <p>With this structure in place, the clab file can leverage the <code>__clabNodeDir__</code> variable:</p> <pre><code>name: mylab\ntopology:\n  nodes:\n    n1:\n      binds:\n        - __clabNodeDir__/conf:/conf\n    n2:\n      binds:\n        - __clabNodeDir__/conf:/conf\n</code></pre> <p>Notice how <code>__clabNodeDir__</code> hides the directory structure and node names and removes the verbosity of the previous approach.</p> <p>Another special variable the containerlab topology file can use is <code>__clabDir__</code>. In the example above, it would expand into <code>clab-mylab</code> folder. With <code>__clabDir__</code> variable it becomes convenient to bind files like <code>ansible-inventory.yml</code> or <code>topology-data.json</code> that containerlab automatically creates:</p> <pre><code>name: mylab\ntopology:\n  nodes:\n    ansible:\n      binds:\n        - __clabDir__/ansible-inventory.yml:/ansible-inventory.yml:ro\n    graphite:\n      binds:\n        - __clabDir__/topology-data.json:/htdocs/clab/topology-data.json:ro\n</code></pre> <p>Binds defined on multiple levels (defaults -&gt; kind -&gt; node) will be merged with the duplicated values removed (the lowest level takes precedence).</p> <p>When a bind with the same destination is defined on multiple levels, the lowest level takes precedence. This allows to override the binds defined on the higher levels.</p>","boost":6},{"location":"manual/nodes/#ports","title":"ports","text":"<p>To bind the ports between the lab host and the containers the users can populate the <code>ports</code> object inside the node:</p> <pre><code>ports:\n  - 80:8080 # tcp port 80 of the host is mapped to port 8080 of the container\n  - 55555:43555/udp\n  - 55554:43554/tcp\n</code></pre> <p>The list of port bindings consists of strings in the same format that is acceptable by <code>docker run</code> command's <code>-p/--export</code> flag.</p> <p>This option is only configurable under the node level.</p>","boost":6},{"location":"manual/nodes/#env","title":"env","text":"<p>To add environment variables to a node use the <code>env</code> container that can be added at <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The variables values are merged when the same vars are defined on multiple levels with nodes level being the most specific.</p> <pre><code>topology:\n  defaults:\n    env:\n      ENV1: 3 # ENV1=3 will be set if it's not set on kind or node level\n      ENV2: glob # ENV2=glob will be set for all nodes\n  kinds:\n    nokia_srlinux:\n      env:\n        ENV1: 2 # ENV1=2 will be set to if it's not set on node level\n        ENV3: kind # ENV3=kind will be set for all nodes of srl kind\n  nodes:\n    node1:\n      env:\n        ENV1: 1 # ENV1=1 will be set for node1\n        # env vars expansion is available, for example\n        # ENV2 variable will be set to the value of the environment variable SOME_ENV\n        # that is defined for the shell you run containerlab with\n        ENV2: ${SOME_ENV} \n</code></pre> <p>You can also specify a magic ENV VAR - <code>__IMPORT_ENVS: true</code> - which will import all environment variables defined in your shell to the relevant topology level.</p> <p><code>NO_PROXY</code> variable</p> <p>If you use an http(s) proxy on your host, you typically set the <code>NO_PROXY</code> environment variable in your containers to ensure that when containers talk to one another, they don't send traffic through the proxy, as that would lead to broken communication. And setting those env vars is tedious.</p> <p>Containerlab automates this process by automatically setting <code>NO_PROXY</code>/<code>no_proxy</code> environment variables in the containerlab nodes with the values of:</p> <ol> <li><code>localhost,127.0.0.1,::1,*.local</code></li> <li>management network range for v4 and v6 (e.g. <code>172.20.20.0/24</code>)</li> <li>IPv4/IPv6 management addresses of the nodes of the lab</li> <li>node names as stated in your topology file</li> </ol>","boost":6},{"location":"manual/nodes/#env-files","title":"env-files","text":"<p>To add environment variables defined in a file use the <code>env-files</code> property that can be defined at <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The variable defined in the files are merged across all of them wtit more specific definitions overwriting less specific. Node level is the most specific one.</p> <p>Files can either be specified with their absolute path or a relative path. The base path for the relative path resolution is the directory that holds the topology definition file.</p> <pre><code>topology:\n  defaults:\n    env-files:\n      - envfiles/defaults\n      - /home/user/clab/default-env\n  kinds:\n    nokia_srlinux:\n      env-files:\n        - envfiles/common\n        - ~/spines\n  nodes:\n    node1:\n      env-files:\n        - /home/user/somefile\n</code></pre>","boost":6},{"location":"manual/nodes/#user","title":"user","text":"<p>To set a user which will be used to run a containerized process use the <code>user</code> configuration option. Can be defined at <code>node</code>, <code>kind</code> and <code>global</code> levels.</p> <pre><code>topology:\n  defaults:\n    user: alice # alice user will be used for all nodes unless set on kind or node levels\n  kinds:\n    nokia_srlinux:\n      user: bob # bob user will be used for nodes of kind srl unless it is set on node level\n  nodes:\n    node1:\n      user: clab # clab user will be used for node1\n</code></pre>","boost":6},{"location":"manual/nodes/#entrypoint","title":"entrypoint","text":"<p>Changing the entrypoint of the container is done with <code>entrypoint</code> config option. It accepts the \"shell\" form and can be set on all levels.</p> <pre><code>topology:\n  defaults:\n    entrypoint: entrypoint.sh\n  kinds:\n    nokia_srlinux:\n      entrypoint: entrypoint.sh\n  nodes:\n    node1:\n      entrypoint: entrypoint.sh\n</code></pre>","boost":6},{"location":"manual/nodes/#cmd","title":"cmd","text":"<p>It is possible to set/override the command of the container image with <code>cmd</code> configuration option. It accepts the \"shell\" form and can be set on all levels.</p> <pre><code>topology:\n  defaults:\n    cmd: bash cmd.sh\n  kinds:\n    nokia_srlinux:\n      cmd: bash cmd2.sh\n  nodes:\n    node1:\n      cmd: bash cmd3.sh\n</code></pre>","boost":6},{"location":"manual/nodes/#labels","title":"labels","text":"<p>To add container labels to a node use the <code>labels</code> container that can be added at <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The label values are merged when the same vars are defined on multiple levels with nodes level being the most specific.</p> <p>Consider the following example, where labels are defined on different levels to show value propagation.</p> <pre><code>topology:\n  defaults:\n    labels:\n      label1: value1\n      label2: value2\n  kinds:\n    nokia_srlinux:\n      labels:\n        label1: kind_value1\n        label3: value3\n  nodes:\n    node1:\n      labels:\n        label1: node_value1\n</code></pre> <p>As a result of such label distribution, node1 will have the following labels:</p> <pre><code>label1: node_value1 # most specific label wins\nlabel2: value2 # inherited from defaults section\nlabel3: value3 # inherited from kinds section\n</code></pre> <p>Note</p> <p>Both user-defined and containerlab-assigned labels also promoted to environment variables prefixed with <code>CLAB_LABEL_</code> prefix.</p>","boost":6},{"location":"manual/nodes/#mgmt-ipv4","title":"mgmt-ipv4","text":"<p>To make a node to boot with a user-specified management IPv4 address, the <code>mgmt-ipv4</code> setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab.</p> <p>Read more about user-defined management addresses here.</p> <pre><code>nodes:\n    r1:\n      kind: nokia_srlinux\n      mgmt-ipv4: 172.20.20.100\n</code></pre>","boost":6},{"location":"manual/nodes/#mgmt_ipv6","title":"mgmt_ipv6","text":"<p>To make a node to boot with a user-specified management IPv4 address, the <code>mgmt_ipv6</code> setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab.</p> <p>Read more about user-defined management addresses here.</p> <pre><code>nodes:\n    r1:\n      kind: nokia_srlinux\n      mgmt_ipv6: 3fff:172:20:20::100\n</code></pre>","boost":6},{"location":"manual/nodes/#dns","title":"DNS","text":"<p>To influence the DNS configuration a particular node uses, the <code>dns</code> configuration knob should be used. Within this blob, DNS server addresses, options and search domains can be provisioned.</p> <pre><code>topology:\n  nodes:\n    r1:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n      dns:\n        servers:\n          - 1.1.1.1\n          - 8.8.4.4\n        search:\n          - foo.com\n        options:\n          - some-opt\n</code></pre>","boost":6},{"location":"manual/nodes/#network-mode","title":"network-mode","text":"<p>By default containerlab nodes use bridge-mode driver - nodes are created with their first interface connected to a docker network (management network).</p> <p>It is possible to change this behavior using <code>network-mode</code> property of a node.</p>","boost":6},{"location":"manual/nodes/#host-mode","title":"host mode","text":"<p>The <code>network-mode</code> configuration option set to <code>host</code> will launch the node in the host networking mode.</p> <pre><code># example node definition with host networking mode\nmy-node:\n  image: alpine:3\n  network-mode: host\n</code></pre>","boost":6},{"location":"manual/nodes/#container-mode","title":"container mode","text":"<p>Additionally, a node can join network namespace of another container - by referencing the node in the format of <code>container:parent_node_name</code><sup>2</sup>:</p> <pre><code># example node definition with shared network namespace\nmy-node:\n  kind: linux\nsidecar-node:\n  kind: linux\n  network-mode: container:my-node # (1)\n  startup-delay: 10 # (2)\n</code></pre> <ol> <li><code>my-node</code> portion of a <code>network-mode</code> property instructs <code>sidecar-node</code> to join the network namespace of a <code>my-node</code>.</li> <li><code>startup-delay</code> is required in this case in order to properly initialize the namespace of a parent container.</li> </ol> <p>Container name used after <code>container:</code> portion can refer to a node defined in containerlab topology or can refer to a name of a container that was launched outside of containerlab. This is useful when containerlab node needs to connect to a network namespace of a container deployed by 3<sup>rd</sup> party management tool (e.g. k8s kind).</p>","boost":6},{"location":"manual/nodes/#none-mode","title":"none mode","text":"<p>If you want to completely disable the networking stack on a container, you can use the <code>none</code> network mode. In this mode containerlab will deploy nodes without <code>eth0</code> interface and docker networking. See docker docs for more details.</p>","boost":6},{"location":"manual/nodes/#runtime","title":"runtime","text":"<p>By default containerlab nodes will be started by <code>docker</code> container runtime. Besides that, containerlab has experimental support for <code>podman</code>, and <code>ignite</code> runtimes.</p> <p>It is possible to specify a global runtime with a global <code>--runtime</code> flag, or set the runtime on a per-node basis:</p> <p>Options for the runtime parameter are:</p> <ul> <li><code>docker</code></li> <li><code>podman</code></li> <li><code>ignite</code></li> </ul> <p>The default runtime can also be influenced via the <code>CLAB_RUNTIME</code> environment variable, which takes the same values as mentioned above.</p> <pre><code># example node definition with per-node runtime definition\nmy-node:\n  image: alpine:3\n  runtime: podman\n</code></pre>","boost":6},{"location":"manual/nodes/#exec","title":"exec","text":"<p>Containers typically have some process that is launched inside the sandboxed environment. The said process and its arguments are provided via container instructions such as <code>entrypoint</code> and <code>cmd</code> in Docker's case.</p> <p>Quite often, it is needed to run additional commands inside the containers when they finished booting. Instead of modifying the <code>entrypoint</code> and <code>cmd</code> it is possible to use the <code>exec</code> parameter and specify a list of commands to execute:</p> <pre><code># two commands will be executed for node `my-node` once it finishes booting\nmy-node:\n  image: alpine:3\n  kind: linux\n  binds:\n    - myscript.sh:/myscript.sh\n  exec:\n    - echo test123\n    - bash /myscript.sh\n</code></pre> <p>The <code>exec</code> is particularly helpful to provide some startup configuration for linux nodes such as IP addressing and routing instructions.</p> exec and access to env vars <p>When you want the <code>exec</code> command to have access to the env variables defined in the topology file or in the container' environment you have to escape the <code>$</code> sign:</p> <pre><code>  nodes:\n    test:\n      kind: linux\n      image: alpine:3\n      env:\n        FOO: BAR\n      exec:\n        - ash -c 'echo $$FOO'\n</code></pre>","boost":6},{"location":"manual/nodes/#memory","title":"memory","text":"<p>By default, container runtimes do not impose any memory resource constraints<sup>1</sup>. A container can use too much of the host's memory, making the host OS unstable.</p> <p>The <code>memory</code> parameter can be used to limit the amount of memory a node/container can use.</p> <pre><code># my-node will have access to at most 1Gb of memory.\nmy-node:\n  image: alpine:3\n  kind: linux\n  memory: 1Gb\n</code></pre> <p>Supported memory suffixes (case insensitive): <code>b</code>, <code>kib</code>, <code>kb</code>, <code>mib</code>, <code>mb</code>, <code>gib</code>, <code>gb</code>.</p>","boost":6},{"location":"manual/nodes/#cpu","title":"cpu","text":"<p>By default, container runtimes do not impose any CPU resource constraints<sup>1</sup>. A container can use as much as the host's scheduler allows.</p> <p>The <code>cpu</code> parameter can be used to limit the number of CPUs a node/container can use.</p> <pre><code># my-node will have access to at most 1.5 of the CPUs\n# available in the host machine.\nmy-node:\n  image: alpine:3\n  kind: linux\n  cpu: 1.5\n</code></pre>","boost":6},{"location":"manual/nodes/#cpu-set","title":"cpu-set","text":"<p>The <code>cpu-set</code> parameter can be used to limit the node CPU usage to specific cores of the host system.</p> <p>Valid syntaxes:</p> <ul> <li><code>0-3</code>: Cores 0, 1, 2 and 3</li> <li><code>0,3</code>: Cores 0 and 3</li> <li><code>0-1,4-5</code>: Cores 0, 1, 4 and 5</li> </ul> <pre><code># my-node will have access to CPU cores 0, 1, 4 and 5.\nmy-node:\n  image: alpine:3\n  kind: linux\n  cpu-set: 0-1,4-5\n</code></pre>","boost":6},{"location":"manual/nodes/#sysctls","title":"sysctls","text":"<p>The sysctl container' setting can be set via the <code>sysctls</code> knob under the <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The sysctl values will be merged. Certain kinds already set up sysctl values in the background, which take precedence over the user-defined values.</p> <p>The following is an example on how to setup the sysctls.</p> <pre><code>topology:\n  defaults:\n    sysctls:\n      net.ipv4.ip_forward: 1\n      net.ipv6.icmp.ratelimi: 100\n  kinds:\n    nokia_srlinux:\n      sysctls:\n        net.ipv4.ip_forward: 0\n\n  nodes:\n    node1:\n      sysctls:\n        net.ipv6.icmp.ratelimit: 1000\n</code></pre>","boost":6},{"location":"manual/nodes/#stages","title":"stages","text":"<p>Stages are a way to define stages a node goes through during its lifecycle and the interdependencies between the different stages of different nodes in the lab.</p> <p>The stages are currently mainly used to host the <code>wait-for</code> knob, which is used to define the startup dependencies between nodes.</p> <p>The following stages have been defined for a node:</p> <ul> <li><code>create</code> - a node enters this stage when containerlab is about to create the node's container. The node finishes this stage when the container is created and is in the <code>created</code> state.</li> <li><code>create-links</code> - a node enters this stage when containerlab is about to attach the links to the node. The node finishes this stage when all the links have been attached to the node.</li> <li><code>configure</code> - a node enters this stage when containerlab is about to run post-deploy commands associated with the node. The node finishes this stage when post deployment commands have been completed.</li> <li><code>healthy</code> - this stage has no distinctive enter/exit points. It is used to define a stage where a node is considered healthy. The healthiness of a node is defined by the healthcheck configuration of the node and the appropriate container status.</li> <li><code>exit</code> - a node reaches the exit state when the container associated with the node has <code>exited</code> status.</li> </ul> <p>Stages can be defined on the <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p>","boost":6},{"location":"manual/nodes/#wait-for","title":"wait-for","text":"<p>For the explicit definition of interstage dependencies between the nodes, the <code>wait-for</code> knob under the <code>stages</code> level can be used.</p> <p>In the example below node four nodes are defined with different stages and <code>wait-for</code> dependencies between the stages.</p> <ol> <li><code>node1</code> will enter the <code>create</code> stage only after <code>node2</code> has finished its <code>create</code> stage.</li> <li><code>node2</code> will enter its <code>create</code> stage only after <code>node3</code> has finished its <code>create-links</code> stage. This means that all the links associated with <code>node3</code> has been attached to the <code>node3</code> node.</li> <li><code>node3</code> will enter its <code>create</code> stage only after <code>node4</code> has been found <code>healthy</code>. This means that <code>node4</code> container must be healthy for <code>node3</code> to enter the creation stage.</li> <li><code>node4</code> doesn't \"wait\" for any of the nodes, but it defines its own healthcheck configuration.</li> </ol> <pre><code>  nodes:\n    node1:\n      stages:\n        create:\n          wait-for:\n            - node: node2\n              stage: create\n\n    node2:\n      stages:\n        create:\n          wait-for:\n            - node: node3\n              stage: create-links\n\n    node3:\n      stages:\n        create:\n          wait-for:\n            - node: node4\n              stage: healthy\n\n    node4:\n      healthcheck:\n        start-period: 5\n        interval: 1\n        test:\n          - CMD-SHELL\n          - cat /etc/os-release\n</code></pre> <p>Containerlab's built-in Dependency Manger takes care of all the dependencies, both explicitly-defined and implicit ones. It will inspect the dependency graph and make sure it is acyclic. The output of the Dependency Manager graph is visible in the debug mode.</p> <p>Note, that <code>wait-for</code> is a list, a node's stage may depend on several other nodes' stages.</p> <p>Usage scenarios</p> <p>One of the use cases where <code>wait-for</code> might be crucial is when a number of VM-based nodes are deployed. Typically, simultaneous deployment of VMs might lead to shortage of CPU resources and VMs might fail to boot. In such cases, <code>wait-for</code> can be used to define the order of VM deployment, thus ensuring that certain VMs enter their <code>create</code> stage after certain nodes have reached <code>healthy</code> status.</p>","boost":6},{"location":"manual/nodes/#per-stage-command-execution","title":"Per-stage command execution","text":"<p>The existing <code>exec</code> node configuration parameter is used to run commands when then node has finished all its deployment stages. Whilst this is the most common use case, it has its limitations, namely you can't run commands when the node is about to deploy its links, or when it is about to enter the <code>healthy</code> stage.</p> <p>These more advanced command execution scenarios are enabled in the per-stage command execution feature.</p> <p>With per-stage command execution the user can define <code>exec</code> block under each stage; moreover, it is possible to specify when the commands should be run <code>on-enter</code> or <code>on-exit</code> of the stage. And if that is not enough, you can also specify where the command should be executed, on the host or in the container.</p> <pre><code>nodes:\n  node1:\n    stages:\n      create-links:\n        exec:\n          - command: ls /sys/class/net/\n            target: container #(1)!\n            phase: on-enter #(2)!\n</code></pre> <ol> <li><code>target</code> defaults to \"container\" and can be omitted. Possible values <code>container</code> or <code>host</code></li> <li><code>phase</code> defaults to \"on-enter\" and can be omitted. Possible values <code>on-enter</code> or <code>on-exit</code></li> </ol> <p>In the example above, the <code>ls /sys/class/net/</code> command will be executed when <code>node1</code> is about to enter the <code>create-links</code> stage. As expected, the command will list only interfaces provisioned by docker (eth0 and lo), but none of the containerlab-provisioned interfaces, since the create-links stage has not been finished yet.</p> <p>Per-stage command execution gives you additional flexibility in terms of when the commands are executed, and what commands are executed at each stage.</p>","boost":6},{"location":"manual/nodes/#host-exec","title":"Host exec","text":"<p>The stage's <code>exec</code> property runs the commands in the container namespace and therefore targets the container node itself. This is super useful in itself, but sometimes you need to run a command on the host as a reaction to a stage enter/exit event.</p> <p>This is what <code>target</code> property of the stage's <code>exec</code> is designed for. It runs the command in the host namespace and therefore targets the host itself.</p> <pre><code>nodes:\n  node1:\n    stages:\n      create-links:\n        exec:\n          - command: touch /tmp/hello\n            target: host\n            phase: on-enter\n</code></pre> <p>In the example above, containerlab will run <code>touch /tmp/hello</code> command when the <code>node1</code> is about to enter the <code>create-links</code> stage.</p>","boost":6},{"location":"manual/nodes/#certificate","title":"certificate","text":"<p>To automatically generate a TLS certificate for a node and sign it with the Certificate Authority created by containerlab, use <code>certificate.issue: true</code> parameter. The signed certificate will be stored in the Lab directory under the <code>.tls/&lt;NODE_NAME&gt;/</code> folder.</p> <p>Note, that nodes which by default rely on TLS-enabled interfaces will generate a certificate regardless of this parameter.</p> <pre><code>name: cert-gen\n\ntopology:\n  nodes:\n    a1:\n      kind: linux\n      image: alpine:latest\n      certificate:\n        issue: true\n</code></pre> <p>To configure key size and certificate validity duration use the following options:</p> <pre><code>  certificate:\n    issue: true\n    key-size: 4096\n    validity-duration: 1h\n</code></pre>","boost":6},{"location":"manual/nodes/#subject-alternative-names-san","title":"subject alternative names (SAN)","text":"<p>With <code>SANs</code> field of the certificate block the user sets the Subject Alternative Names that will be added to the node's certificate.</p> <p>For a topology node named \"srl\" in a lab named \"srl01\", the following SANs are set by default:</p> <ul> <li><code>srl</code></li> <li><code>clab-srl01-srl</code></li> <li><code>srl.srl01.io</code></li> <li>IPv4/6 addresses of the node</li> </ul> <pre><code>topology:\n\n  nodes:\n    srl:\n      kind: nokia_srlinux\n      certificate:\n        sans:\n          - \"test.com\"\n          - 192.168.96.155\n</code></pre>","boost":6},{"location":"manual/nodes/#healthcheck","title":"healthcheck","text":"<p>Containerlab supports the docker healthcheck configuration for the nodes. The healthcheck instruction can be set on the <code>defaults</code>, <code>kind</code> or <code>node</code> level, with the node level likely being the most used one.</p> <p>Healtcheck allows containerlab users to define the healthcheck configuration that will be used by the container runtime to check the health of the container.</p> <pre><code>topology:\n  nodes:\n    l1:\n      kind: linux\n      image: alpine:3\n      healthcheck:\n        test:\n          - CMD-SHELL\n          - cat /etc/os-release\n        start-period: 3\n        retries: 1\n        interval: 5\n        timeout: 2\n</code></pre> <p>The healthcheck instruction is a dictionary that can contain the following keys:</p> <ul> <li><code>test</code> - the command to run to check the health of the container. The command is provided as a list of strings. The first element of the list is the type of the command - either <code>CMD</code> or <code>CMD-SHELL</code>, the rest are the arguments.     When <code>CMD</code> type is used, the command and its arguments should be provided as a separate list elements. The <code>CMD-SHELL</code> allows you to specify the command that will be evaluated by the container's shell.</li> <li><code>start-period</code> - the time in seconds to wait for the container to bootstrap before running the first health check. The default value is 0 seconds.</li> <li><code>interval</code> - the time interval between the health checks. The default value is 30 seconds.</li> <li><code>timeout</code> - the time to wait for a single health check operation to complete. The default value is 30 seconds.</li> <li><code>retries</code> - the number of consecutive healthcheck failures needed to report the container as unhealthy. The default value is 3.</li> </ul> <p>When the node is configured with a healthcheck the health status is visible in the <code>docker inspect</code> and <code>docker ps</code> outputs.</p>","boost":6},{"location":"manual/nodes/#aliases","title":"aliases","text":"<p>To define additional hostnames for the node use the <code>aliases</code> configuration option. Other containers on the same network can use these aliases to communicate with the node.</p> <pre><code>topology:\n  nodes:\n    r1:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n      aliases:\n        - r1.example.com\n</code></pre> <ol> <li> <p>docker runtime resources constraints.\u00a0\u21a9\u21a9</p> </li> <li> <p>this deployment model makes two containers to use a shared network namespace, similar to a Kubernetes pod construct.\u00a0\u21a9</p> </li> </ol>","boost":6},{"location":"manual/topo-def-file/","title":"Topology definition","text":"<p>Containerlab builds labs based on the topology information that users pass to it. This topology information is expressed as a code contained in the topology definition file which structure is the prime focus of this document.</p>"},{"location":"manual/topo-def-file/#topology-definition-components","title":"Topology definition components","text":"<p>The topology definition file is a configuration file expressed in YAML and has a name pattern of <code>*.clab.yml</code><sup>1</sup>. In this document, we take a pre-packaged Nokia SR Linux and Arista cEOS lab and explain the topology definition structure using its definition file srlceos01.clab.yml which is pasted below:</p> <pre><code>name: srlceos01\n\ntopology:\n  nodes:\n    srl:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    ceos:\n      kind: ceos\n      image: ceos:4.32.0F\n\n  links:\n    - endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n</code></pre> <p>Note</p> <p>Containerlab provides a JSON schema file for the topology file. The schema is used to live-validate user's input if a code editor supports this feature.</p> <p>This topology results in the two nodes being started up and interconnected with each other using a single point-to-point interface:</p> <p>Let's touch on the key components of the topology definition file used in this example.</p>"},{"location":"manual/topo-def-file/#name","title":"Name","text":"<p>The topology must have a name associated with it. The name is used to distinct one topology from another, to allow multiple topologies to be deployed on the same host without clashes.</p> <pre><code>name: srlceos01\n</code></pre> <p>Its user's responsibility to give labs unique names if they plan to run multiple labs.</p> <p>The name is a free-formed string, though it is better not to use dashes (<code>-</code>) as they are used to separate lab names from node names.</p> <p>When containerlab starts the containers, their names will be generated using the following pattern: <code>clab-${lab-name}-${node-name}</code>. The lab name here is used to make the container's names unique between two different labs, even if the nodes are named the same.</p>"},{"location":"manual/topo-def-file/#prefix","title":"Prefix","text":"<p>It is possible to change the prefix that containerlab adds to node names. The <code>prefix</code> parameter is in charge of that. It follows the below-mentioned logic:</p> <ol> <li>When <code>prefix</code> is not present in the topology file, the default prefix logic applies. Containers will be named as <code>clab-&lt;lab-name&gt;-&lt;node-name&gt;</code>.</li> <li>When <code>prefix</code> is set to some value, for example, <code>myprefix</code>, this string is used instead of <code>clab</code>, and the resulting container name will be: <code>myprefix-&lt;lab-name&gt;-&lt;node-name&gt;</code>.</li> <li>When <code>prefix</code> is set to a magic value <code>__lab-name</code> the resulting container name will not have the <code>clab</code> prefix, but will keep the lab name: <code>&lt;lab-name&gt;-&lt;node-name&gt;</code>.</li> <li>When set to an empty string, the node names will not be prefixed at all. If your node is named <code>mynode</code>, you will get the <code>mynode</code> container in your system.</li> </ol> <p>Warning</p> <p>In the case of an empty prefix, you have to keep in mind that nodes need to be named uniquely across all labs.</p> <p>Examples:</p> custom prefixempty prefix <pre><code>name: mylab\nprefix: myprefix\nnodes:\n  n1:\n  # &lt;some config&gt;\n</code></pre> <p>With a prefix set to <code>myprefix</code> the container name for node <code>n1</code> will be <code>myprefix-mylab-n1</code>.</p> <pre><code>name: mylab\nprefix: \"\"\nnodes:\n  n1:\n  # &lt;some config&gt;\n</code></pre> <p>When a prefix is set to an empty string, the container name will match the node name - <code>n1</code>.</p> <p>Note</p> <p>Even when you change the prefix, the lab directory is still uniformly named using the <code>clab-&lt;lab-name&gt;</code> pattern.</p>"},{"location":"manual/topo-def-file/#topology","title":"Topology","text":"<p>The topology object inside the topology definition is the core element of the file. Under the <code>topology</code> element you will find all the main building blocks of a topology such as <code>nodes</code>, <code>kinds</code>, <code>defaults</code> and <code>links</code>.</p>"},{"location":"manual/topo-def-file/#nodes","title":"Nodes","text":"<p>As with every other topology the nodes are in the center of things. With nodes we define which lab elements we want to run, in what configuration and flavor.</p> <p>Let's zoom into the two nodes we have defined in our topology:</p> <pre><code>topology:\n  nodes:\n    srl:                    # this is a name of the 1st node\n      kind: nokia_srlinux\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n    ceos:                   # this is a name of the 2nd node\n      kind: ceos\n      image: ceos:4.32.0F\n</code></pre> <p>We defined individual nodes under the <code>topology.nodes</code> container. The name of the node is the key under which it is defined. Following the example, our two nodes are named <code>srl</code> and <code>ceos</code> respectively.</p> <p>Each node can have multiple configuration properties which make containerlab quite a flexible tool. The <code>srl</code> node in our example is defined with the a few node-specific properties:</p> <pre><code>srl:\n  kind: nokia_srlinux\n  type: ixrd2l\n  image: ghcr.io/nokia/srlinux\n</code></pre> <p>Refer to the node configuration document to meet all other options a node can have.</p>"},{"location":"manual/topo-def-file/#links","title":"Links","text":"<p>Although it is absolutely fine to define a node without any links (like in this lab), we usually interconnect the nodes to make topologies. One of containerlab purposes is to make the interconnection of the nodes simple.</p> <p>Links are defined under the <code>topology.links</code> section of the topology file. Containerlab understands two formats of link definition - brief and extended. A brief form of a link definition compresses link parameters in a single string and provide a quick way to define a link at the cost of link features available. A more expressive extended form exposes all link features, but requires more typing if done manually. The extended format is perfect for machine-generated link topologies.</p>"},{"location":"manual/topo-def-file/#interface-naming","title":"Interface naming","text":"<p>Containerlab supports two kinds of interface naming: Linux interfaces<sup>2</sup> and interface aliases.</p> <p>The \"raw\" Linux interface names are the names of the interfaces as they are expected to be seen inside the container (but not necessarily how they look like in the configuration file). Have a look at this topology that features SR Linux and cEOS nodes interconnected with a single link using Linux interface names:</p> using Linux interface names<pre><code># nodes configuration omitted for clarity\ntopology:\n  nodes:\n    srl:\n    ceos:\n\n  links:\n    - endpoints: [\"srl:e1-2\", \"ceos:eth2\"] # (1)!\n</code></pre> <ol> <li>In this example, the <code>srl</code> node has an interface named <code>e1-2</code> and the <code>ceos</code> node has an interface named <code>eth2</code>. These are the Linux interface names which can be seen if you enter the container' shell and issue <code>ip link</code>.</li> </ol>"},{"location":"manual/topo-def-file/#aliases","title":"Aliases","text":"<p>The downside of using Linux interface names is that they often do not match the interface naming convention used by the Network OS. This is where Interface Aliases feature (added in Containerlab v0.56.0) comes in handy.</p> <p>Imagine we want to create a lab with four different Kinds: SR Linux, vEOS, CSR1000v and vSRX, cabled like this:</p> A side B side SR Linux ethernet-1/1 vEOS Ethernet1/1 vSRX ge-0/0/2 vEOS Ethernet1/2 CSR1000v Gi5 vSRX ge-0/0/5 vEOS Ethernet1/3 CSR1000v Gi3 Using Linux interfacesUsing interface aliases <p>Using the <code>ethX</code> interface naming convention, the topology would look like this:</p> <pre><code>links:\n  - endpoints: [\"srl:e1-1\", \"vEOS:eth1\"]\n  - endpoints: [\"vSRX:eth3\", \"vEOS:eth2\"]\n  - endpoints: [\"CSR1000v:eth4\", \"vSRX:eth6\"]\n  - endpoints: [\"vEOS:eth3\", \"CSR1000v:eth2\"]\n</code></pre> <p>Note the four different kinds of offset used here on the four different NOSes!</p> <p>Using aliased interface names, the topology definition becomes much more straightforward:</p> <pre><code>links:\n  - endpoints: [\"srl:ethernet-1/1\", \"vEOS:Ethernet1/1\"]\n  - endpoints: [\"vSRX:ge-0/0/2\", \"vEOS:Ethernet1/2\"]\n  - endpoints: [\"CSR1000v:Gi5\", \"vSRX:ge-0/0/5\"]\n  - endpoints: [\"vEOS:Ethernet1/3\", \"CSR1000v:Gi3\"]\n</code></pre> <p>Both topology definitions result in the same lab being deployed, but the latter is easier to write and to understand.</p> <p>Many Kinds (but not all) support interface aliases and the alias names are provided in the respective kind' documentation.</p> <p>Containerlab transparently maps from interface aliases to Linux interface names, and there's no additional syntax or configuration needed to specify either an interface alias or a Linux interface name in topologies.</p> How do aliases work? <p>Internally, interface aliases end up being deterministically mapped to Linux interface names, which conform to Linux interface naming standards: at most 15 characters, spaces and forward slashes (<code>/</code>) not permitted.</p> <p>Since many NOSes use long interface names (<code>GigabitEthernet1</code>, that's exactly 1 character longer than permitted), and like to use slashes in their interface naming conventions, these NOS interface names cannot be directly used as interface names for the container interfaces created by Containerlab. For example, SR Linux maps its <code>ethernet-1/2</code> interface to the Linux interface <code>e1-2</code>. On the other hand, Juniper vSRX maps its <code>ge-0/0/1</code> interface to <code>eth2</code>.</p>"},{"location":"manual/topo-def-file/#brief-format","title":"Brief format","text":"<p>The brief format of link definition looks as follows.</p> <pre><code># nodes configuration omitted for clarity\ntopology:\n  nodes:\n    srl:\n    ceos:\n\n  links:\n    - endpoints: [\"srl:ethernet-1/1\", \"ceos:Ethernet1/1\"] #(1)!\n    - endpoints: [\"srl:e1-2\", \"ceos:eth2\"]\n</code></pre> <ol> <li>This example features two interface naming conventions: Linux interface names and interface aliases.</li> </ol> <p>As you see, the <code>topology.links</code> element is a list of individual links. The link itself is expressed as pair of <code>endpoints</code>. This might sound complicated, lets use a graphical explanation:</p> <p>As demonstrated on a diagram above, the links between the containers are the point-to-point links which are defined by a pair of interfaces. The link defined as:</p> <pre><code>endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n</code></pre> <p>will result in a creation of a p2p link between the node named <code>srl</code> and its <code>e1-1</code> interface and the node named <code>ceos</code> and its <code>eth1</code> interface. The p2p link is realized with a veth pair.</p>"},{"location":"manual/topo-def-file/#extended-format","title":"Extended format","text":"<p>The extended link format allows a user to set every supported link parameter in a structured way. The available link parameters depend on the Link type and provided below.</p>"},{"location":"manual/topo-def-file/#veth","title":"veth","text":"<p>The veth link is the most common link type used in containerlab. It creates a virtual ethernet link between two endpoints where each endpoint refers to a node in the topology.</p> <pre><code>links:\n  - type: veth\n    endpoints:\n      - node: &lt;NodeA-Name&gt;                  # mandatory\n        interface: &lt;NodeA-Interface-Name&gt;   # mandatory\n        mac: &lt;NodeA-Interface-Mac&gt;          # optional\n      - node: &lt;NodeB-Name&gt;                  # mandatory\n        interface: &lt;NodeB-Interface-Name&gt;   # mandatory\n        mac: &lt;NodeB-Interface-Mac&gt;          # optional\n    mtu: &lt;link-mtu&gt;                         # optional\n    vars: &lt;link-variables&gt;                  # optional (used in templating)\n    labels: &lt;link-labels&gt;                   # optional (used in templating)\n</code></pre>"},{"location":"manual/topo-def-file/#mgmt-net","title":"mgmt-net","text":"<p>The mgmt-net link type represents a veth pair that is connected to a container node on one side and to the management network (usually a bridge) instantiated by the container runtime on the other.</p> <pre><code>  links:\n  - type: mgmt-net\n    endpoint:\n      node: &lt;NodeA-Name&gt;                  # mandatory\n      interface: &lt;NodeA-Interface-Name&gt;   # mandatory\n      mac: &lt;NodeA-Interface-Mac&gt;          # optional\n    host-interface: &lt;interface-name         # mandatory\n    mtu: &lt;link-mtu&gt;                         # optional\n    vars: &lt;link-variables&gt;                  # optional (used in templating)\n    labels: &lt;link-labels&gt;                   # optional (used in templating)\n</code></pre> <p>The <code>host-interface</code> is the desired interface name that will be attached to the management network in the host namespace.</p>"},{"location":"manual/topo-def-file/#macvlan","title":"macvlan","text":"<p>The macvlan link type creates a MACVlan interface with the <code>host-interface</code> as its parent interface. The MACVlan interface is then moved to a node's network namespace and renamed to the <code>endpoint.interface</code> name.</p> <pre><code>  links:\n  - type: macvlan\n    endpoint:\n      node: &lt;NodeA-Name&gt;                  # mandatory\n      interface: &lt;NodeA-Interface-Name&gt;   # mandatory\n      mac: &lt;NodeA-Interface-Mac&gt;          # optional\n    host-interface: &lt;interface-name&gt;        # mandatory\n    mode: &lt;macvlan-mode&gt;                    # optional (\"bridge\" by default)\n    vars: &lt;link-variables&gt;                  # optional (used in templating)\n    labels: &lt;link-labels&gt;                   # optional (used in templating)\n</code></pre> <p>The <code>host-interface</code> is the name of the existing interface present in the host namespace.</p> <p>Modes are <code>private</code>, <code>vepa</code>, <code>bridge</code>, <code>passthru</code> and <code>source</code>. The default is <code>bridge</code>.</p>"},{"location":"manual/topo-def-file/#host","title":"host","text":"<p>The host link type creates a veth pair between a container and the host network namespace. In comparison to the veth type, no bridge or other namespace is required to be referenced in the link definition for a \"remote\" end of the veth pair.</p> <pre><code>  links:\n  - type: host\n    endpoint:\n      node: &lt;NodeA-Name&gt;                  # mandatory\n      interface: &lt;NodeA-Interface-Name&gt;   # mandatory\n      mac: &lt;NodeA-Interface-Mac&gt;          # optional\n    host-interface: &lt;interface-name&gt;        # mandatory\n    mtu: &lt;link-mtu&gt;                         # optional\n    vars: &lt;link-variables&gt;                  # optional (used in templating)\n    labels: &lt;link-labels&gt;                   # optional (used in templating)\n</code></pre> <p>The <code>host-interface</code> parameter defines the name of the veth interface in the host's network namespace.</p>"},{"location":"manual/topo-def-file/#vxlan","title":"vxlan","text":"<p>The vxlan type results in a vxlan tunnel interface that is created in the host namespace and subsequently pushed into the nodes network namespace.</p> <pre><code>  links:\n    - type: vxlan                       \n      endpoint:                              # mandatory\n        node: &lt;Node-Name&gt;                    # mandatory\n        interface: &lt;Node-Interface-Name&gt;     # mandatory\n        mac: &lt;Node-Interface-Mac&gt;            # optional\n      remote: &lt;Remote-VTEP-IP&gt;               # mandatory\n      vni: &lt;VNI&gt;                             # mandatory\n      udp-port: &lt;VTEP-UDP-Port&gt;              # mandatory\n      mtu: &lt;link-mtu&gt;                        # optional\n      vars: &lt;link-variables&gt;                 # optional (used in templating)\n      labels: &lt;link-labels&gt;                  # optional (used in templating)\n</code></pre>"},{"location":"manual/topo-def-file/#vxlan-stitched","title":"vxlan-stitched","text":"<p>The vxlan-stitched type results in a veth pair linking the host namespace and the nodes namespace and a vxlan tunnel that also terminates in the host namespace. In addition to these interfaces, tc rules are being provisioned to stitch the vxlan tunnel and the host based veth interface together.</p> <pre><code>  links:\n    - type: vxlan-stitch\n      endpoint:                              # mandatory\n        node: &lt;Node-Name&gt;                    # mandatory\n        interface: &lt;Node-Interface-Name&gt;     # mandatory\n        mac: &lt;Node-Interface-Mac&gt;            # optional\n      remote: &lt;Remote-VTEP-IP&gt;               # mandatory\n      vni: &lt;VNI&gt;                             # mandatory\n      udp-port: &lt;VTEP-UDP-Port&gt;              # mandatory\n      mtu: &lt;link-mtu&gt;                        # optional\n      vars: &lt;link-variables&gt;                 # optional (used in templating)\n      labels: &lt;link-labels&gt;                  # optional (used in templating)\n</code></pre>"},{"location":"manual/topo-def-file/#dummy","title":"dummy","text":"<p>The dummy type creates a dummy interface that provides a virtual network device to route packets through without actually transmitting them.</p> <p>Such interfaces are useful for testing and debugging purposes where we want to make sure that the NOS detects network ports, but doesn't actually need to send or receive packets via these ports.</p> <pre><code>  links:\n  - type: dummy\n    endpoint:\n      node: &lt;NodeA-Name&gt;                    # mandatory\n      interface: &lt;NodeA-Interface-Name&gt;     # mandatory\n      mac: &lt;NodeA-Interface-Mac&gt;            # optional\n    mtu: &lt;link-mtu&gt;                         # optional\n    vars: &lt;link-variables&gt;                  # optional (used in templating)\n    labels: &lt;link-labels&gt;                   # optional (used in templating)\n</code></pre>"},{"location":"manual/topo-def-file/#kinds","title":"Kinds","text":"<p>Kinds define the behavior and the nature of a node, it says if the node is a specific containerized Network OS, virtualized router or something else. We go into details of kinds in its own document section, so here we will discuss what happens when <code>kinds</code> section appears in the topology definition:</p> <pre><code>topology:\n  kinds:\n    nokia_srlinux:\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n    srl2:\n      kind: nokia_srlinux\n    srl3:\n      kind: nokia_srlinux\n</code></pre> <p>In the example above the <code>topology.kinds</code> element has <code>srl</code> kind referenced. With this, we set some values for the properties of the <code>srl</code> kind. A configuration like that says that nodes of <code>srl</code> kind will also inherit the properties (type, image) defined on the kind level.</p> <p>Essentially, what <code>kinds</code> section allows us to do is to shorten the lab definition in cases when we have a number of nodes of a same kind. All the nodes (<code>srl1</code>, <code>srl2</code>, <code>srl3</code>) will have the same values for their <code>type</code> and <code>image</code> properties.</p> <p>Consider how the topology would have looked like without setting the <code>kinds</code> object:</p> <pre><code>topology:\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n    srl2:\n      kind: nokia_srlinux\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n    srl3:\n      kind: nokia_srlinux\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n</code></pre> <p>A lot of unnecessary repetition is eliminated when we set <code>srl</code> kind properties on kind level.</p>"},{"location":"manual/topo-def-file/#defaults","title":"Defaults","text":"<p><code>kinds</code> set the values for the properties of a specific kind, whereas with the <code>defaults</code> container it is possible to set values globally.</p> <p>For example, to set the environment variable for all the nodes of a topology:</p> <pre><code>topology:\n  defaults:\n    env:\n      MYENV: VALUE\n  nodes:\n    srl1:\n    srl2:\n    srl3:\n</code></pre> <p>Now every node in this topology will have environment variable <code>MYENV</code> set to <code>VALUE</code>.</p>"},{"location":"manual/topo-def-file/#settings","title":"Settings","text":"<p>Global containerlab settings are defined in <code>settings</code> container. The following settings are supported:</p>"},{"location":"manual/topo-def-file/#certificate-authority","title":"Certificate authority","text":"<p>Global certificate authority settings section allows users to tune certificate management in containerlab. Refer to the Certificate management doc for more details.</p>"},{"location":"manual/topo-def-file/#environment-variables","title":"Environment variables","text":"<p>Topology definition file may contain environment variables anywhere in the file. The syntax is the same as in the bash shell:</p> <pre><code>name: linux\n\ntopology:\n  nodes:\n    l1:\n      kind: linux\n      image: alpine:${ALPINE_VERSION:=3}\n</code></pre> <p>In the example above, the <code>ALPINE_VERSION</code> environment variable is used to set the version of the alpine image. If the variable is not set, the value of <code>3</code> will be used. The following syntax is used to expand the environment variable:</p> Expression Meaning <code>${var}</code> Value of var (same as <code>$var</code>) <code>${var-$DEFAULT}</code> If var not set, evaluate expression as $DEFAULT <code>${var:-$DEFAULT}</code> If var not set or is empty, evaluate expression as $DEFAULT <code>${var=$DEFAULT}</code> If var not set, evaluate expression as $DEFAULT <code>${var:=$DEFAULT}</code> If var not set or is empty, evaluate expression as $DEFAULT <code>${var+$OTHER}</code> If var set, evaluate expression as $OTHER, otherwise as empty string <code>${var:+$OTHER}</code> If var set, evaluate expression as $OTHER, otherwise as empty string <code>$$var</code> Escape expressions. Result will be <code>$var</code>."},{"location":"manual/topo-def-file/#generated-topologies","title":"Generated topologies","text":"<p>To further simplify parametrization of the topology files, containerlab allows users to template the topology files using Go Template engine.</p> <p>Using templating approach it is possible to create a lab template and instantiate different labs from it, by simply changing the variables in the variables file.</p> <p>Standard Go templating language has been extended with the functions provided in docs.gomplate.ca project, which opens the doors to a very flexible topology generation workflows.</p> <p>To help you get started, we created the following lab examples which demonstrate how topology templating can be used:</p> <ul> <li>Leaf-Spine topology with parametrized number of leaves/spines</li> <li>5-stage Clos topology with parametrized number of pods and super-spines</li> </ul> <ol> <li> <p>if the filename has <code>.clab.yml</code> or <code>-clab.yml</code> suffix, the YAML file will have autocompletion and linting support in VSCode editor.\u00a0\u21a9</p> </li> <li> <p>also referred to as \"mapped\" or \"raw\" interfaces in some parts of the documentation\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/vrnetlab/","title":"VM-based routers integration","text":"<p>Containerlab focuses on containers, but many routing products ship only in virtual machine packaging. Leaving containerlab users without the ability to create topologies with both containerized and VM-based routing systems would have been a shame.</p> <p>Keeping this requirement in mind from the very beginning, we added <code>bridge</code>/<code>ovs-bridge</code> kind that allows bridging your containerized topology with other resources available via a bridged network. For example, a VM based router:</p> <p>With this approach, you could bridge VM-based routing systems by attaching interfaces to the bridge you define in your topology. However, it doesn't allow users to define the VM-based nodes in the same topology file. With <code>vrnetlab</code> integration, containerlab is now capable of launching topologies with VM-based routers defined in the same topology file.</p> <p>Danger</p> <p>Containerlab uses the forked version of vrnetlab hosted at https://github.com/hellt/vrnetlab. You will not be able to build a compatible container with the upstream vrnetlab project.</p>"},{"location":"manual/vrnetlab/#vrnetlab","title":"Vrnetlab","text":"<p>Vrnetlab packages a regular VM inside a container and makes it runnable as if it was a container image.</p> <p>To make this work, vrnetlab provides a set of scripts that build the container image out of a user-provided VM disk. This integration enables containerlab to build topologies that consist both of native containerized NOSes and VMs:</p> <p>Warning</p> <p>Ensure that the VM that containerlab runs on has Nested virtualization enabled to support vrnetlab-based containers.</p>"},{"location":"manual/vrnetlab/#compatibility-matrix","title":"Compatibility matrix","text":"<p>To make vrnetlab images to work with container-based networking in containerlab, we needed to fork vrnetlab project and implement the necessary improvements. VM-based routers that you intend to run with containerlab should be built with <code>hellt/vrnetlab</code> project, and not with the upstream <code>vrnetlab/vrnetlab</code>.</p> <p>Containerlab depends on <code>hellt/vrnetlab</code> project, and sometimes features added in containerlab must be implemented in <code>vrnetlab</code> (and vice-versa). This leads to a cross-dependency between these projects. The following table provides a link between the version combinations for the recent versions:</p> containerlab<sup>1</sup> vrnetlab<sup>2</sup> Notes <code>0.58.0</code> <code>0.20.1</code> New platforms: Cisco IOL, Cisco vIOS, Huawei VRP The vrnetlab version (commit) is now part of the image labels under the <code>vrnetlab-version</code> name. This should help you identify what version of vrnetlab is used to build the image. <code>0.56.0</code> <code>0.18.1</code> Added support for Dell SONiC, SONiC VM, Cisco Catalyst 9000v <code>0.55.0</code> <code>0.17.0</code> Added support for Juniper vJunos-router, Generic VM, support for setting qemu parameters via env vars for the nodes <ul> <li>When new VM-based platform support is added to <code>hellt/vrnetlab</code>, it is usually accompanied by a new containerlab version. In this case the table row will have both containerlab and vrnetlab versions.  </li> <li>When vrnetlab adds new features that don't require containerlab changes, the table will state only the vrnetlab version.  </li> <li>When containerlab adds new features that don't require vrnetlab changes, the table will state only the containerlab version.</li> </ul> <p>It is worth noting, that you can use the latest containerlab version with a given vrnetlab version, even if the table doesn't list the latest containerlab version.</p> Older release mappings containerlab<sup>1</sup> vrnetlab<sup>2</sup> Notes <code>0.10.4</code> <code>0.1.0-cl</code> Initial release. Images: sros, vmx, xrv, xrv9k <code>0.11.0</code> <code>0.2.0</code> added vr-veos, support for boot-delay, SR OS will have a static route to docker network, improved XRv startup chances <code>0.2.1</code> added timeout for SR OS images to allow eth interfaces to appear in the container namespace. Other images are not touched. <code>0.2.2</code> fixed serial (telnet) access to SR OS nodes <code>0.2.3</code> set default cpu/ram for SR OS images <code>0.13.0</code> <code>0.3.0</code> added support for Cisco CSR1000v via <code>cisco_csr</code> and MikroTik routeros via <code>mikrotik_ros</code> kind <code>0.3.1</code> enhanced SR OS boot sequence <code>0.4.0</code> fixed SR OS CPU allocation and added Palo Alto PAN support <code>paloaltp_pan</code> <code>0.16.0</code> <code>0.5.0</code> added support for Cisco Nexus 9000v via <code>cisco_n9kv</code> kind, added support for non-continuous interfaces provisioning <code>0.19.0</code> <code>0.6.0</code> added experimental support for Juniper vQFX via <code>juniper_vqfx</code> kind, added support Dell FTOS via <code>dell_ftosv</code> <code>0.6.2</code> support for IPv6 management for SR OS; support for RouterOS v7+ <code>0.7.0</code> startup-config support for vqfx and vmx <code>0.32.2</code> <code>0.8.0</code> startup-config support for the rest of the kinds, support for multi line card SR OS <code>0.34.0</code> <code>0.8.2</code> startup-config support for PANOS, ISA support for Nokia VSR-I and MGMT VRF for VMX <code>0.9.0</code> Support for IPInfusion OcNOS with vrnetlab <code>0.41.0</code> <code>0.11.0</code> Added support for Juniper vSRX3.0 via <code>juniper_vsrx</code> kind <code>0.45.0</code> <code>0.12.0</code> Added support for Juniper vJunos-switch via <code>juniper_vjunosswitch</code> kind <code>0.49.0</code> <code>0.14.0</code> Added support for Juniper vJunos-Evolved, Cisco FTDv, OpenBSD <code>0.53.0</code> <code>0.15.0</code> Added support for Fortigate, freebsd, added lots of FP5 types to Nokia SR OS and support for external cf1/2 disks <code>0.54.0</code> <code>0.16.0</code> Added support for Cisco c8000v"},{"location":"manual/vrnetlab/#building-vrnetlab-images","title":"Building vrnetlab images","text":"<p>To build a vrnetlab image compatible with containerlab, users first need to ensure that the versions of both projects follow compatibility matrix.</p> <ol> <li>Clone <code>hellt/vrnetlab</code> and checkout to a version compatible with containerlab release:</li> </ol> <pre><code>git clone https://github.com/hellt/vrnetlab &amp;&amp; cd vrnetlab\n\n# assuming we are running containerlab 0.11.0,\n# the latest compatible vrnetlab version is 0.2.3\n# at the moment of this writing\ngit checkout v0.2.3\n</code></pre> <ol> <li>Enter the directory for the image of interest</li> </ol> <pre><code>cd sros\n</code></pre> <ol> <li>Follow the build instructions from the README.md file in the image directory</li> </ol>"},{"location":"manual/vrnetlab/#supported-vm-products","title":"Supported VM products","text":"<p>The images that work with containerlab will appear in the supported list as we implement the necessary integration.</p> Product Kind Demo lab Notes Nokia SR OS nokia_sros SRL &amp; SR OS When building SR OS vrnetlab image for use with containerlab, do not provide the license during the image build process. The license shall be provided in the containerlab topology definition file<sup>3</sup>. Juniper vMX juniper_vmx SRL &amp; vMX Juniper vQFX juniper_vqfx Juniper vSRX juniper_vsrx Juniper vJunos-Router juniper_vjunosrouter Juniper vJunos-Switch juniper_vjunosswitch Juniper vJunosEvolved juniper_vjunosevolved Cisco XRv cisco_xrv SRL &amp; XRv Cisco XRv9k cisco_xrv9k SRL &amp; XRv9k Cisco CSR1000v cisco_csr Cisco Nexus 9000v cisco_nexus9kv Cisco FTDv cisco_ftdv Arista vEOS arista_veos MikroTik RouterOS mikrotik_ros Palo Alto PAN paloalto_pan Dell FTOS10v dell_ftosv Aruba AOS-CX aruba_aoscx IPInfusion OcNOS ipinfusion_ocnos Checkpoint Cloudguard checkpoint_cloudguard Fortinet Fortigate fortinet_fortigate OpenBSD openbsd FreeBSD freebsd SONiC (VM) sonic-vm"},{"location":"manual/vrnetlab/#tuning-qemu-parameters","title":"Tuning qemu parameters","text":"<p>When vrnetlab starts a VM inside the container it uses <code>qemu</code> command to define the VM parameters such as disk drives, cpu type, memory, etc. Containerlab allows users to tune some of these parameters by setting the environment variables in the topology file. The values from these variables will override defaults set by vrnetlab for this particular VM image.</p> <p>The following env vars are supported:</p> <ul> <li><code>QEMU_SMP</code> - sets the number of vCPU cores and their configuration. Use this when the default number of vCPUs is not enough or excessive.</li> <li><code>QEMU_MEMORY</code> - sets the amount of memory allocated to the VM in MB. Use this when you want to alter the amount of allocated memory for the VM. Note, that some kinds have a different way to set CPU/MEM parameters, which is explained in the kind's documentation.</li> <li><code>QEMU_CPU</code> - sets the default CPU model/type for the node. Use this when the default cpu type is not suitable for your host or you want to experiment with others.</li> <li><code>QEMU_ADDITIONAL_ARGS</code> - allows users to pass additional qemu arguments to the VM. These arguments will be appended to the list of the existing arguments. Use this when you need to pass some specific qemu arguments to the VM overriding the defaults set by vrnetlab.</li> </ul>"},{"location":"manual/vrnetlab/#datapath-connectivity","title":"Datapath connectivity","text":"<p>By hosting a VM inside a container, we made it easy to run VM-based routers in a containerized environment. However, how would you connect container's interfaces to the VM's tap interfaces in a transparent way?</p> <p>To solve this challenge containerlab uses tc backend<sup>4</sup>, which mirrors the traffic to and from container interfaces to the appropriate VM interfaces. A huge bonus of <code>tc</code> is that there are not bridges inbetween, and we have a clear channel that supports transparent passage of any frames, like LACP, for example.</p>"},{"location":"manual/vrnetlab/#management-interface","title":"Management interface","text":"<p>By default, vrnetlab uses the qemu user mode networking to connect the VM's (guest) management interface to the host. In this mode, the host can reach the guests management interface using a predefined IP address, which is in vrnetlab's case is <code>10.0.0.15</code>.</p> <p>In the VM's CLI you will note the management interface to be configured with the said address:</p> <pre><code>csr-r1#show ip int br\nInterface              IP-Address      OK? Method Status                Protocol\nGigabitEthernet1       10.0.0.15       YES manual up                    up\nGigabitEthernet2       unassigned      YES unset  administratively down down\n</code></pre> <p>The downside of this method of exposing the management interface to the host is that ALL VM nodes will have the same IP address assigned to their management interface. This poses two problems:</p> <ol> <li>What if you require the management address to have a specific IP that matches your production IP plan?</li> <li>How to integrate the nodes with external management systems that often glean the management interface IP address to identify the node against the inventory?</li> </ol> <p>Starting with hellt/vrnetlab v0.21.0 a new datapath mode has been added to the vrnetlab called Transparent Management. When this mode is enabled, the IP address that Containerlab assigns to the management interface will be configured in the Network OS configuration.</p> <p>To enabled this mode, set the <code>CLAB_MGMT_PASSTHROUGH</code> to <code>true</code> in your topology file:</p> snippet from the clab file<pre><code>topology:\n  defaults:\n    env:\n      CLAB_MGMT_PASSTHROUGH: \"true\"\n</code></pre> <p>This will result in all your VM-based nodes that have been adapted to use this mode (see vrnetlab/#286 for the supported NOSes) to be configured with the IP addresses that you see in the containerlab table upon deploy.</p>"},{"location":"manual/vrnetlab/#networking","title":"Networking","text":"<p>Vrnetlab-based container images expose their management interface on the <code>eth0</code> interface. Further <code>eth</code> interfaces, that is, <code>eth1</code> and up, are considered data interfaces and are connected to the VM using <code>tc</code> connection mode. Data plane interfaces are connected to the VM preserving both the order of and discontinuity between <code>eth</code> data-plane interfaces. Internally, the vrnetlab launch script achieves this by mapping the <code>eth</code> interfaces to virtualised NICs at the corresponding PCI bus addresses, filling out gaps with dummy network interfaces.</p> <p>For example, a vrnetlab node with endpoints <code>eth2</code>, <code>eth3</code> and <code>eth5</code> would have these devices mapped to PCI bus addresses 2, 3 and 5 respectively, while addresses 1 and 4 would be allocated an unconnected (dummy) virtualised NIC.</p> <p>For convenience and easier adaptation of configurations and lab diagrams to Containerlab topologies, vrnetlab-based nodes also support interface aliasing. Interface aliasing allows for the use of the same interface naming conventions in containerlab topologies as in the NOS, as long as interface aliasing is implemented for the NOS' kind. Note that not all NOS' implementations have support for interface aliases at the moment. For information about the supported interface naming conventions for each NOS, check out their specific Kinds page.</p>"},{"location":"manual/vrnetlab/#boot-order","title":"Boot order","text":"<p>A simultaneous boot of many qemu nodes may stress the underlying system, which sometimes renders in a boot loop or system halt. If the container host doesn't have enough capacity to bear the simultaneous boot of many qemu nodes, it is still possible to successfully run them by scheduling their boot time.</p> <p>Starting with v0.51.0 users may define a \"staged\" boot process by defining the <code>stages</code> and <code>wait-for</code> dependencies between the VM-based nodes.</p> <p>Consider the following example where the first SR OS nodes will boot immediately, whereas the second node will wait till the first node is reached the <code>healthy</code> stage:</p> <pre><code>name: boot-order\ntopology:\n  nodes:\n    sr1:\n      kind: nokia_sros\n      image: nokia_sros:latest\n    sr2:\n      kind: nokia_sros\n      image: nokia_sros:latest\n      stages:\n        create:\n          wait-for:\n            - node: sr1\n              stage: healthy\n</code></pre>"},{"location":"manual/vrnetlab/#boot-delay","title":"Boot delay","text":"<p>A predecessor of the Boot Order is the boot delay that can be set with <code>BOOT_DELAY</code> environment variable that the supported VM-based nodes will respect.</p> <p>Consider the following example where the first SR OS nodes will boot immediately, whereas the second node will sleep for 30 seconds and then start the boot process:</p> <pre><code>name: boot-delay\ntopology:\n  nodes:\n    sr1:\n      kind: nokia_sros\n      image: nokia_sros:21.2.R1\n      license: license-sros21.txt\n    sr2:\n      kind: nokia_sros\n      image: nokia_sros:21.2.R1\n      license: license-sros21.txt\n      env:\n        # boot delay in seconds\n        BOOT_DELAY: 30\n</code></pre> <p>This method is not as flexible as the Boot Order, since you rely on the fixed delay, and it doesn't allow for the dynamic boot order based on the node health.</p>"},{"location":"manual/vrnetlab/#memory-optimization","title":"Memory optimization","text":"<p>Typically a lab consists of a few types of VMs which are spawned and interconnected with each other. Consider a lab consisting of 5 interconnected routers; one router uses VM image X, and four routers use VM image Y.</p> <p>Effectively we run just two types of VMs in that lab, and thus we can implement a memory deduplication technique that drastically reduces the memory footprint of a lab. In Linux, this can be achieved with technologies like KSM (via <code>ksmtuned</code>). Install KSM package on your distribution and enable it to save memory.</p> <p>Find some examples below (or contribute a new one)</p> Debian/Ububntu <pre><code>sudo apt-get update -y\nsudo apt-get install -y ksmtuned\n\nsudo systemctl status ksm.service\nsudo systemctl restart ksm.service\nsudo echo 1 &gt; /sys/kernel/mm/ksm/run\n\ngrep . /sys/kernel/mm/ksm/*\n</code></pre> <p>If you want KSM always active you could change <code>#KSM_THRES_COEF=20</code> in <code>/etc/ksmtuned.conf</code> to <code>KSM_THRES_COEF=99</code>. That way KSM will kick in as soon as free RAM dops below 99% instead of below the default 20% of free RAM.</p> <ol> <li> <p>to install a certain version of containerlab, use the instructions from installation doc.\u00a0\u21a9\u21a9</p> </li> <li> <p>to have a guaranteed compatibility checkout to the mentioned tag and build the images.\u00a0\u21a9\u21a9</p> </li> <li> <p>see this example lab with a license path provided in the topology definition file\u00a0\u21a9</p> </li> <li> <p>pros and cons of different datapaths were examined here \u21a9</p> </li> </ol>"},{"location":"manual/wireshark/","title":"Packet capture &amp; Wireshark","text":"<p>Every lab emulation software must provide its users with the packet capturing abilities. Looking at the frames as they traverse the network links is not only educational, but also helps to troubleshoot the issues that might arise during the lab development.</p> <p> </p> <p>Containerlab offers a simple way to capture the packets from any interface of any node in the lab. This article will explain how to do that.</p> <p>Tip</p> <p>If you are looking for a free Web UI for packet capture with Wireshark, check out our Edgeshark integration.</p> <p>Consider the following lab topology which highlights the typical points of packet capture.</p> <p>Since containerlab leverages Linux network devices, users can use any tool to sniff packets. Let's see how one can use either or a combination of the following well-known packet capturing tools: <code>tcpdump</code>, <code>tshark</code> and <code>wireshark</code>.</p>"},{"location":"manual/wireshark/#packet-capture-namespaces-and-interfaces","title":"Packet capture, namespaces and interfaces","text":"<p>To capture the packets from a given interface requires having that interface's name and its network namespace (netns). And that's it.</p> <p>The diagram below shows the two nodes connected with a single link and how network namespaces are used to isolate one node's networking stack from another. Looking at the diagram, it is clear which interface belongs to which network namespace.</p> <p>When a lab like the one above is deployed, containerlab creates the following containers:</p> <ul> <li><code>clab-quickstart-srl</code></li> <li><code>clab-quickstart-ceos</code></li> </ul> <p>And the namespace names would be named accordingly to the container names, namely <code>clab-quickstart-srl</code> and <code>cla-quickstart-ceos</code>.</p>"},{"location":"manual/wireshark/#capture-modes","title":"Capture modes","text":"<p>Now when it is clear which netns names corresponds to which container and which interfaces are available inside the given lab node, it's easy to start capturing traffic. But how to do that?</p> <p>There are two most common ways of capturing the traffic:</p> <ul> <li>local capture: when the capture is started from the containerlab host itself.</li> <li>remote capture: when the capture is started from a remote machine that connects via SSH to the containerlab host and starts the capture.</li> </ul> <p>In both cases, the capturing software (tcpdump or tshark) needs to be available on the containerlab host.</p>"},{"location":"manual/wireshark/#local-capture","title":"local capture","text":"<p>Local capture assumes the capture is initiated from the containerlab host. For instance, to capture from the <code>e1-1</code> interface of the <code>clab-quickstart-srl</code> node use:</p> <pre><code>ip netns exec clab-quickstart-srl tcpdump -nni e1-1\n</code></pre> <p>In this example we first entered the namespace where the target interface is located using <code>ip netns exec</code> command and then started the capture with <code>tcpdump</code> providing the interface name to it.</p> <p>The downside of local capture is that typically containerlab hosts run in a headless (no UI) mode and thus the visibility of the captured traffic is limited to the console output. This is where <code>tshark</code> might come in in handy by providing more readable output. Still, the lack of Wireshark UI is a downside, therefore it is our recommendation for you to get familiar with the remote capture method.</p>"},{"location":"manual/wireshark/#remote-capture","title":"remote capture","text":"<p>The limitations of the local capture are lifted when the remote capture is used. In the remote capture model you initiate the packet capture from your descktop/laptop and the traffic is sent to your machine where it can be displayed in the Wireshark UI.</p> <p>Before we start mixing in the Wireshark, lets see how the remote capture is initiated:</p> <pre><code>ssh $containerlab_host_address \\\n    \"ip netns exec clab-quickstart-srl tcpdump -nni e1-1\"\n</code></pre> <p>Assuming we ran the above command from our laptop, we rely on <code>$containerlab_host_address</code> being reachable from our laptop. We use <code>ssh</code> to connect to the remote containerlab host and execute the same command we did in the local capture.  </p> <p>But simply seeing the tcpdump output on your laptop's terminal doesn't offer much difference to the local capture.</p> <p>The true power the remote capture has is in being able to use the Wireshark installed on your machine to display the captured traffic. To do that we need to pipe the output of the <code>tcpdump</code> command to the <code>wireshark</code> command. This is done by adding the <code>-w -</code> option to the <code>tcpdump</code> command which tells it to write the captured traffic to the standard output. The output is then piped to the <code>wireshark</code> command which is invoked with the <code>-k -i -</code> options. The <code>-k</code> option tells wireshark to start capturing immediately and the <code>-i -</code> option tells it to read the input from the standard input.</p> <pre><code>ssh $containerlab_host_address \\\n    \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | \\\n    wireshark -k -i -\n</code></pre> <p>This will start the capture from a given interface and redirect the received flow to the Wireshark!</p> <p>Windows users</p> <p>Windows users should use WSL and invoke the command similar to the following:</p> <pre><code>ssh $containerlab_host_address \\\n    \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | \\\n    /mnt/c/Program\\ Files/Wireshark/wireshark.exe -k -i -\n</code></pre>"},{"location":"manual/wireshark/#capture-script","title":"Capture script","text":"<p>Since capturing is a so popular it makes sense to create a tiny helper script that will simplify the process of capturing from a given interface of a given node. The script presented below hides all the irrelevant details and makes sniffing a breeze. Let's see how it works:</p> pcap.sh<pre><code>#!/bin/sh\n# call this script as\n# pcap.sh &lt;containerlab-host&gt; &lt;container-name&gt; &lt;interface-name&gt;\n# example: pcap.sh clab-vm srl e1-1\n\n# to support multiple interfaces, pass them as comma separated list\n# split $3 separate by comma as -i &lt;interface1&gt; -i &lt;interface2&gt; -i &lt;interface3&gt;\nIFS=',' read -ra ADDR &lt;&lt;&lt; \"$3\"\nIFACES=\"\"\nfor i in \"${ADDR[@]}\"; do\n    IFACES+=\" -i $i\"\ndone\n\nssh $1 \"ip netns exec $2 tshark -l ${IFACES} -w -\" | \\\n    /Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i -\n</code></pre> <p>If you put this script somewhere in your <code>$PATH</code> you can invoke it as follows:</p> <pre><code>pcap.sh clab-vm srl e1-1\n</code></pre> <p>where</p> <ul> <li><code>clab-vm</code> is the containerlab host address that is reachable from your machine</li> <li><code>srl</code> is the container name of the node that has the interface you want to capture from</li> <li><code>e1-1</code> is the interface name you want to capture from</li> </ul> <p>The script uses the <code>tshark</code> CLI tool instead of <code>tcpdump</code> to be able to capture from multiple interfaces at once. This is achieved by splitting the interface names by comma and passing them to the <code>tshark</code> command as <code>-i &lt;interface1&gt; -i &lt;interface2&gt; -i &lt;interface3&gt;</code>.</p> <p>Here is a short demo of how the script works and how to use it based on the lab topology with linux nodes and SR Linux NOS:</p> <p>Note</p> <p>The script uses the Mac OS version of the Wireshark. If you are on Linux, you can simply replace the last line with <code>wireshark -k -i -</code>.</p>"},{"location":"manual/wireshark/#edgeshark-integration","title":"Edgeshark integration","text":"<p>The capture script already makes it easy to dump packets off of an interface and piping it to a Wireshark UI. Is there anything that can make it even easier?</p> <p>How about a Web UI that displays every interface of every container and can start a wireshark session by a click of a button? Let us introduce you to the Edgeshark.</p> <p>Quote</p> <p>Edgeshark visualizes the communication of containers and thus helps in diagnosing it, both in-between containers as well as with the \"outside world\". It can be deployed to Linux stand-alone container hosts, including KinD deployments. Edgeshark also supports capturing container traffic using Wireshark.</p> <p>Yep, you got it right, edgeshark is a Web UI for Wireshark<sup>1</sup> that is capable of capturing traffic from any interface of any container (and physical interface) in your lab. Moreover, it plugs into containerlab natively, and is free and open-source.</p> <p>This diagram shows a typical integration of edgeshark with containerlab:</p> <p>From a user's perspective, the integration is as simple as running the following command on the containerlab host:</p> InstallUninstall <pre><code>curl -sL \\\nhttps://github.com/siemens/edgeshark/raw/main/deployments/wget/docker-compose.yaml \\\n| DOCKER_DEFAULT_PLATFORM= docker compose -f - up -d\n</code></pre> <pre><code>curl -sL \\\nhttps://github.com/siemens/edgeshark/raw/main/deployments/wget/docker-compose.yaml \\\n| DOCKER_DEFAULT_PLATFORM= docker compose -f - down\n</code></pre> <p>This will deploy the edgeshark containers and expose the Web UI on the containerlab host's port 5001. If you have a network reachability to the containerlab host, you can open the Web UI (<code>https://&lt;containerlab-host-address&gt;:5001</code>) in your browser and see the Edgeshark UI.</p> SSH port forwarding in case you don't have direct reachability <p>If you don't have direct reachability, you can use SSH port forwarding to access the Web UI:</p> ssh port forwarding<pre><code>ssh -L 5001:localhost:5001 $containerlab_host\n</code></pre> <p>Then open your browser and navigate to <code>http://localhost:5001</code> to see the Edgeshark UI.</p> <p>Check out the Edgeshark &amp; Containerlab integration in action:</p>"},{"location":"manual/wireshark/#wireshark-and-system-configuration","title":"Wireshark and system configuration","text":"<p>There is a small price one needs to pay to make integrate Edgeshark with Wireshark, it consists of two steps:</p> <ol> <li>Configuring the system to handle the <code>packetflix://</code> URL schema and open it with the Wireshark.</li> <li>Installing the external capture plugin for Wireshark.</li> </ol> <p>Luckily, you only need to do it once and it will work for all the future captures on any system you install EdgeShark on.</p> <p>Edgeshark documentation provides a detailed guide on how to perform these two steps for Windows and Linux systems.</p> <p>There was a tiny gap in MacOS support, but we contributed the necessary piece<sup>2</sup> and here is how you configure your MacOS system to work with Edgeshark:</p> <ol> <li> <p>Download the zip file with the AppleScript that enables the <code>packetflix://</code> URL schema handling, unarchive it and move the EdgeShark-handler script to the Applications folder. Here is the script that does all that, run it from your MacOS:</p> <pre><code>mkdir -p /tmp/pflix-handler &amp;&amp; cd /tmp/pflix-handler &amp;&amp; \\\nrm -rf packetflix-handler.zip packetflix-handler.app __MACOSX &amp;&amp; \\\ncurl -sLO https://github.com/srl-labs/containerlab/files/14278951/packetflix-handler.zip &amp;&amp; \\\nunzip packetflix-handler.zip &amp;&amp; \\\nsudo mv packetflix-handler.app /Applications\n</code></pre> </li> <li> <p>Download the external capture plugin for Darwin OS and your architecture, unarchive and copy it to the <code>/Applications/Wireshark.app/Contents/MacOS/extcap/</code> directory.</p> </li> </ol> <p>With these steps done<sup>3</sup>, you should be able to click on the \"fin\" icon next to the interface name and see the Wireshark UI opening up and starting the capture.</p>"},{"location":"manual/wireshark/#examples","title":"Examples","text":"<p>Lets take the first diagram of this article and see which commands are used to sniff from the highlighted interfaces.</p> <p>In the examples below the wireshark will be used as a sniffing tool and the following naming simplifications and conventions used:</p> <ul> <li><code>$clab_host</code> - address of the containerlab host</li> <li><code>clab-pcap-srl</code>, <code>clab-pcap-ceos</code>, <code>clab-pcap-linux</code> - container names of the SRL, cEOS and Linux nodes accordingly.</li> </ul> SR Linux [1], [4]cEOS [2]Linux container [3]management bridge [5] <p>SR Linux linecard interfaces are named as <code>e&lt;linecard_num&gt;-&lt;port_num&gt;</code> which translates to <code>ethernet-&lt;linecard_num&gt;/&lt;port_num&gt;</code> name inside the NOS itself. So to capture from <code>ethernet-1/1</code> interface the following command should be used:</p> <pre><code>ssh $clab_host \\\n    \"ip netns exec $clab-pcap-srl tcpdump -U -nni e1-1 -w -\" | \\\n    wireshark -k -i -\n</code></pre> <p>The management interface on the SR Linux container is named <code>mgmt0</code>, so the relevant command will look like:</p> <pre><code>ssh $clab_host \\\n    \"ip netns exec $clab-pcap-srl tcpdump -U -nni mgmt0 -w -\" | \\\n    wireshark -k -i -\n</code></pre> <p>Similarly to SR Linux example, to capture the data interface of cEOS is no different. Just pick the right interface:</p> <pre><code>ssh $clab_host \\\n    \"ip netns exec $clab-pcap-ceos tcpdump -U -nni eth1 -w -\" | \\\n    wireshark -k -i -\n</code></pre> <p>A bare linux container is no different, its interfaces are named <code>ethX</code> where <code>eth0</code> is the interface connected to the containerlab management network. So to capture from the first data link we will use <code>eth1</code> interface:</p> <pre><code>ssh $clab_host \\\n    \"ip netns exec $clab-pcap-linux tcpdump -U -nni eth1 -w -\" | \\\n    wireshark -k -i -\n</code></pre> <p>It is also possible to listen for all management traffic that traverses the containerlab's management network. To do that you firstly need to find out the name of the linux bridge and then capture from it:</p> <pre><code>ssh $clab_host \"tcpdump -U -nni brXXXXXX -w -\" | wireshark -k -i -\n</code></pre> <p>Note that in this case you do not need to drill into the network namespace, since management bridge is in the default netns.</p>"},{"location":"manual/wireshark/#useful-commands","title":"Useful commands","text":"<p>To list available network namespaces:</p> <pre><code>ip netns list\n</code></pre> <p>To list the interfaces (links) of a given container leverage the <code>ip</code> utility:</p> <pre><code># where $netns_name is the container name of a node\nip netns exec $netns_name ip link\n</code></pre> <ol> <li> <p>It is more than just a UI for Wireshark, but in the context of pcap capture we focus on this feature solely.\u00a0\u21a9</p> </li> <li> <p>https://github.com/siemens/cshargextcap/issues/14#issuecomment-1932267889 \u21a9</p> </li> <li> <p>You may be asked to allow running the application downloaded from Internet as per MacOS security policy.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/clabernetes/","title":"Clabernetes","text":"<p>pronounciation: Kla-ber-net-ees</p> <p>Love containerlab? Want containerlab, just distributed in a kubernetes cluster? Enter clabernetes or simply c9s.</p> <p></p> <p> </p> <p>Clabernetes deploys containerlab topologies into a kubernetes cluster. The goal of Clabernetes is to scale Containerlab beyond a single node while keeping the user experience you love.</p> <p>If all goes to plan, Clabernetes is going to be one of the solutions to enable multi-node labs and allow its users to create large topologies powered by a k8s cluster.</p> <p>Eager to try it out? Check out the Quickstart! Have questions, join our Discord.</p> <p>Warning</p> <p>We are sharing Clabernetes \u03b2 version to allow people to see what we're working on and potentially attract contributors and early adopters. You may not need any k8s knowledge to use it, but if something goes wrong, you might need to dig into k8s logs and resources to figure out what's happening.</p> <p>In the beta release we focus on the core topology constructs working our way towards full feature parity with Containerlab (and even more).</p>"},{"location":"manual/clabernetes/#quick-links","title":"Quick Links","text":"<ul> <li>Helm chart on ArtifactHub</li> <li>CRD reference</li> <li>Source code on GitHub</li> </ul>"},{"location":"manual/clabernetes/install/","title":"Installation","text":"<p>Clabernetes runs on a Kubernetes cluster and hence requires one to be available before you start your Clabernetes journey. Although we don't have a strict requirement on the k8s version, we recommend using the version 1.21 or higher.</p> <p>Clabernetes project consists of two components:</p> <ul> <li>Clabernetes manager (a.k.a. controller) - a k8s controller that watches for the Clabernetes topology resources and deploys them to the cluster.</li> <li>Clabverter - a CLI tool that converts containerlab topology files into Clabernetes topology resources.</li> </ul>"},{"location":"manual/clabernetes/install/#clabernetes-manager","title":"Clabernetes Manager","text":"<p>Clabernetes manager (a.k.a. controller) is packaged as a Helm chart; this means if you don't have Helm - install it or use it in a container packaging:</p> For self-hostedFor GCP/GKE <pre><code>alias helm='docker run --network host -ti --rm -v $(pwd):/apps -w /apps \\\n    -v ~/.kube:/root/.kube -v ~/.helm:/root/.helm \\\n    -v ~/.config/helm:/root/.config/helm \\\n    -v ~/.cache/helm:/root/.cache/helm \\\n    alpine/helm:3.12.3'\n</code></pre> <p>GKE clusters require [<code>gke-gcloud-auth-plugin</code>][gke-auth-plugin] to be available. Make sure you have it installed and mounted into the container.</p> <pre><code>alias helm='docker run --network host -ti --rm -v $(pwd):/apps -w /apps \\\n    -v /usr/bin/gke-gcloud-auth-plugin:/usr/bin/gke-gcloud-auth-plugin \\\n    -v ~/.kube:/root/.kube -v ~/.helm:/root/.helm \\\n    -v ~/.config/helm:/root/.config/helm \\\n    -v ~/.cache/helm:/root/.cache/helm \\\n    alpine/helm:3.12.3'\n</code></pre> <p>With Helm installed, proceed to install the Clabernetes manager.</p> install latest versioninstall specific versionlatest dev versionuninstall <p>To install the latest Clabernetes release with Helm to an existing k8s cluster<sup>1</sup> run the following command:</p> <pre><code>helm upgrade --install --create-namespace --namespace c9s \\\n    clabernetes oci://ghcr.io/srl-labs/clabernetes/clabernetes\n</code></pre> <p>To upgrade to the latest version re-run the installation command and the latest version will be installed on the cluster replacing the older running version.</p> <p>To install a specific clabernetes version add <code>--version</code> flag like so:</p> <pre><code>helm upgrade --version 0.0.25 --install \\\n    clabernetes oci://ghcr.io/srl-labs/clabernetes/clabernetes\n</code></pre> <p>Clabernetes iterates fast, and you might want to try the latest development version until we cut a release. To do so, use the <code>0.0.0</code> version:</p> <pre><code>helm upgrade --install --version 0.0.0 --create-namespace --namespace c9s \\\n    --set manager.managerLogLevel=debug \\\n    --set manager.controllerLogLevel=debug \\\n    --set manager.imagePullPolicy=Always \\\n    --set globalConfig.deployment.launcherImagePullPolicy=Always \\\n    --set globalConfig.deployment.launcherLogLevel=debug \\\n    clabernetes oci://ghcr.io/srl-labs/clabernetes/clabernetes\n</code></pre> <p>We also set the log level to <code>debug</code> for all the components to see more verbose logs. Trust us, you might need it </p> <p>To uninstall clabernetes from the cluster:</p> <pre><code>helm uninstall --namespace c9s clabernetes\n</code></pre>"},{"location":"manual/clabernetes/install/#clabverter","title":"Clabverter","text":"<p>What a name, huh? Clabverter is a helper CLI tool that takes your existing containerlab topology converts it to a Clabernetes topology resource and applies it to the cluster.</p> <p>Clabverter is versioned in the same way as Clabernetes, and the easiest way to use it is by leveraging the container image<sup>2</sup>:</p> latest versionspecific versiondevelopment version set up <code>clabverter</code> alias<pre><code>alias clabverter='sudo docker run --user $(id -u) \\\n    -v $(pwd):/clabernetes/work --rm \\\n    ghcr.io/srl-labs/clabernetes/clabverter'\n</code></pre> <p>In case you need to install a specific version:</p> <pre><code>alias clabverter='sudo docker run --user $(id -u) \\\n    -v $(pwd):/clabernetes/work --rm \\\n    ghcr.io/srl-labs/clabernetes/clabverter:0.0.22'\n</code></pre> <p>To use the latest development version of clabverter:</p> <pre><code>alias clabverter='sudo docker run --pull always --user $(id -u) \\\n    -v $(pwd):/clabernetes/work --rm \\\n    ghcr.io/srl-labs/clabernetes/clabverter:dev-latest'\n</code></pre> <ol> <li> <p>Want to quickly spin up a local k8s cluster with clabernetes? Check out our Quickstart.\u00a0\u21a9</p> </li> <li> <p>You already have Docker installed if you use containerlab, right?\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/clabernetes/pcap/","title":"Packet capture in Clabernetes","text":"<p>It is quite interesting to see how Clabernetes uses different datapath stitching tricks to connect lab nodes running in different containers. Sometimes looking at the packets exchanged between the nodes can help to understand the inner workings of the setup and often comes in handy when troubleshooting.</p> <p>Capturing packets in Clabernetes is similar to capturing packets in Containerlab, with just one more indirection level added. Check the basics of packet capture in Containerlab to get started, because we will use the same technique here.</p>"},{"location":"manual/clabernetes/pcap/#capture-script","title":"Capture script","text":"<p>The most straightforward way to capture packets in Clabernetes is to leverage the capture script similar to the one we did for Containerlab, but instead of using <code>ip netns exec</code> we will use <code>kubectl exec</code> to run the packet capture in the container and piping the output to Wireshark.</p> <p>Below you will find two script variants, one for a case when <code>kubectl</code> runs on the same machine where Wireshark is installer, and the second one when the <code>kubectl</code> runs on a remote machine.</p> <p>Note</p> <p>The examples below are given for the MacOs, for Windows users running WSL the path to the Wireshark will be <code>/mnt/c/Program\\ Files/Wireshark/wireshark.exe</code> and Linux users will figure it out without hints </p> local kubectlremote kubectl <p>Since the <code>kubectl</code> is installed locally, we can straight away use <code>kubectl exec</code> to connect to the pod. The script below is used like:</p> <pre><code>bash c9spcap.sh &lt;k8s-namespace&gt; &lt;pod name&gt; &lt;interface name&gt;\n</code></pre> c9spcap.sh<pre><code>#!/bin/sh\n\nkubectl exec -n $1 -it $2 -- tcpdump -U -nni $3 -w - | \\\n/Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i -\n</code></pre> <p>Since the <code>kubectl</code> is installed remotely, we need to use <code>ssh</code> to connect to the remote machine first. The script below is used like:</p> <pre><code>bash c9spcap.sh &lt;host-with-kubectl&gt; &lt;k8s-namespace&gt; &lt;pod name&gt; &lt;interface name&gt;\n</code></pre> c9spcap.sh<pre><code>#!/bin/sh\n\nssh $1 \"kubectl exec -n $2 -it $3 -- tcpdump -U -nni $4 -w -\" | \\\n/Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i -\n</code></pre> <p>It is a smart idea to save the script in a directory that is in your <code>PATH</code> so that you can run it from anywhere anytime you need to capture some packets.</p>"},{"location":"manual/clabernetes/quickstart/","title":"Clabernetes Quickstart","text":"<p>The best way to understand how clabernetes works is to walk through a short example where we deploy a simple but representative lab using clabernetes.</p> <p>Do you already have a kubernetes cluster? Great! You can skip the cluster creation step and jump straight to Installing Clabernetes part.</p> <p>But if you don't have a cluster yet, don't panic, we'll create one together. We are going to use kind to create a local kubernetes cluster and then install clabernetes into it. Once clabernetes is installed we deploy a small topology with two SR Linux nodes and two client nodes.</p> <p>If all goes to plan, the lab will be successfully deployed! Clabverter &amp; clabernetes work in unison to make the original topology files deployable onto the cluster with tunnels stitching lab nodes together to form point to point connections between the nodes.</p> <p>Let's see how it all works, buckle up!</p>"},{"location":"manual/clabernetes/quickstart/#creating-a-cluster","title":"Creating a cluster","text":"<p>Clabernetes goal is to allow users to run networking labs with containerlab's simplicity and ease of use, but with the scaling powers of kubernetes. Surely, it is best to have a real deal available to you, but for demo purposes we'll use <code>kind</code> v0.22.0 to create a local multi-node kubernetes cluster. If you already have a k8s cluster, feel free to use it instead -- clabernetes can run in any kubernetes cluster<sup>1</sup>!</p> <p>With the following command we instruct kind to set up a three node k8s cluster with two worker and one control plane nodes.</p> <pre><code>kind create cluster --name c9s --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n  - role: worker\n  - role: worker\ncontainerdConfigPatches:\n- |-\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n    discard_unpacked_layers = false\nEOF\n</code></pre> <p>Don't forget to install <code>kubectl</code>!</p> <p>Check that the cluster is ready and proceed with installing clabernetes.</p> <pre><code>\u276f kubectl get nodes \nNAME                STATUS   ROLES           AGE     VERSION\nc9s-control-plane   Ready    control-plane   5m6s    v1.29.2\nc9s-worker          Ready    &lt;none&gt;          4m46s   v1.29.2\nc9s-worker2         Ready    &lt;none&gt;          4m42s   v1.29.2\n</code></pre>"},{"location":"manual/clabernetes/quickstart/#installing-clabernetes","title":"Installing clabernetes","text":"<p>Clabernetes is installed into a kubernetes cluster using helm:</p> <p>We use <code>alpine/helm</code> container image here instead of installing Helm locally; you can skip this step if you already have <code>helm</code> installed.</p> For self-hostedFor GCP/GKE <pre><code>alias helm='docker run --network host -ti --rm -v $(pwd):/apps -w /apps \\\n    -v ~/.kube:/root/.kube -v ~/.helm:/root/.helm \\\n    -v ~/.config/helm:/root/.config/helm \\\n    -v ~/.cache/helm:/root/.cache/helm \\\n    alpine/helm:3.12.3'\n</code></pre> <p>GKE clusters require <code>gke-gcloud-auth-plugin</code> to be available. Make sure you have it installed and mounted into the container.</p> <pre><code>alias helm='docker run --network host -ti --rm -v $(pwd):/apps -w /apps \\\n    -v /usr/bin/gke-gcloud-auth-plugin:/usr/bin/gke-gcloud-auth-plugin \\\n    -v ~/.kube:/root/.kube -v ~/.helm:/root/.helm \\\n    -v ~/.config/helm:/root/.config/helm \\\n    -v ~/.cache/helm:/root/.cache/helm \\\n    alpine/helm:3.12.3'\n</code></pre> <pre><code>helm upgrade --install --create-namespace --namespace c9s \\\n    clabernetes oci://ghcr.io/srl-labs/clabernetes/clabernetes\n</code></pre> <p>Note, that we install clabernetes in a <code>c9s</code> namespace. This is not a requirement, but it is a good practice to keep clabernetes manager deployment in a separate namespace.</p> <p>A successful installation will result in a <code>clabernetes-manager</code> deployment of three pods running in the cluster:</p> <pre><code>kubectl get -n c9s pods -o wide #(1)!\n</code></pre> <ol> <li>Note, that <code>clabernetes-manager</code> is installed as a 3-node deployment, and you can see that two pods might be in Init stay for a little while until the leader election is completed.</li> </ol> <pre><code>NAME                                   READY   STATUS    RESTARTS   AGE    IP            NODE          NOMINATED NODE   READINESS GATES\nclabernetes-manager-7ccb98897c-7ctnt   1/1     Running   0          103s   10.244.2.15   c9s-worker    &lt;none&gt;           &lt;none&gt;\nclabernetes-manager-7ccb98897c-twzxw   1/1     Running   0          96s    10.244.1.15   c9s-worker2   &lt;none&gt;           &lt;none&gt;\nclabernetes-manager-7ccb98897c-xhgkl   1/1     Running   0          103s   10.244.1.14   c9s-worker2   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"manual/clabernetes/quickstart/#installing-load-balancer","title":"Installing Load Balancer","text":"<p>To get access to the nodes deployed by clabernetes from outside the k8s cluster we need a load balancer. Any load balancer will do, but we will use kube-vip in this quickstart.</p> <p>Note</p> <p>Load Balancer installation can be skipped if you don't need external access to the lab nodes. You can still access the nodes from inside the cluster by entering the pod's shell and then logging into the node.</p> <p>Following kube-vip + kind installation instructions we execute the following commands:</p> <pre><code>kubectl apply -f https://kube-vip.io/manifests/rbac.yaml\nkubectl apply -f https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml\nkubectl create configmap --namespace kube-system kubevip \\\n  --from-literal range-global=172.18.1.10-172.18.1.250\n</code></pre> <p>Next we set up the kube-vip CLI:</p> <pre><code>KVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | \\\n  jq -r \".[0].name\")\nalias kube-vip=\"docker run --network host \\\n  --rm ghcr.io/kube-vip/kube-vip:$KVVERSION\"\n</code></pre> <p>And install kube-vip load balancer daemonset in ARP mode:</p> <pre><code>kube-vip manifest daemonset --services --inCluster --arp --interface eth0 | \\\nkubectl apply -f -\n</code></pre> <p>We can check kube-vip daemonset pods are running on both worker nodes:</p> <pre><code>kubectl get pods -A -o wide | grep kube-vip\n</code></pre> <pre><code>kube-system          kube-vip-cloud-provider-54c878b6c5-qwvf5    1/1     Running   0          91s   10.244.0.5   c9s-control-plane   &lt;none&gt;           &lt;none&gt;\nkube-system          kube-vip-ds-fj7qp                           1/1     Running   0          9s    172.18.0.3   c9s-worker2         &lt;none&gt;           &lt;none&gt;\nkube-system          kube-vip-ds-z8q67                           1/1     Running   0          9s    172.18.0.4   c9s-worker          &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"manual/clabernetes/quickstart/#clabverter","title":"Clabverter","text":"<p>Clabernetes motto is \"containerlab at scale\" and therefore we wanted to make it work with the same topology definition file format as containerlab does. Understandably though, the original Containerlab's topology file is not something you can deploy on Kubernetes cluster as is.</p> <p>To make sure you have a smooth sailing in the clabernetes waters we've created a clabernetes companion tool called <code>clabverter</code>; it takes a containerlab topology file and converts it to several manifests native to Kubernetes and clabernetes. Clabverter then can also apply those manifests to the cluster on your behalf.</p> <p>Clabverter is not a requirement to run clabernetes, but it is a helper tool to convert containerlab topologies to clabernetes resources and kubernetes objects.</p> <p>As per clabverter's installation instructions we will setup an alias that uses the latest available clabverter container image:</p> set up <code>clabverter</code> alias<pre><code>alias clabverter='sudo docker run --user $(id -u) \\\n    -v $(pwd):/clabernetes/work --rm \\\n    ghcr.io/srl-labs/clabernetes/clabverter'\n</code></pre>"},{"location":"manual/clabernetes/quickstart/#deploying-with-clabverter","title":"Deploying with clabverter","text":"<p>We are now ready to deploy our lab using clabernetes with the help of clabverter. First we clone the lab repository:</p> Cloning the lab<pre><code>git clone --depth 1 https://github.com/srl-labs/srlinux-vlan-handling-lab.git \\\n  &amp;&amp; cd srlinux-vlan-handling-lab\n</code></pre> <p>And then, while standing in the lab directory, let <code>clabverter</code> do its job:</p> Converting the containerlab topology to clabernetes manifests and applying it<pre><code>clabverter --stdout --naming non-prefixed | \\\nkubectl apply -f - #(1)!\n</code></pre> <ol> <li> <p><code>clabverter</code> converts the original containerlab topology to a set of k8s manifests and applies them to the cluster.</p> <p>We will cover what <code>clabverter</code> does in more details in the user manual some time later, but if you're curious, you can check the manifests it generates by running <code>clabverter --stdout &gt; manifests.yml</code> and inspecting the <code>manifests.yml</code> file.</p> <p>The <code>non-prefixed</code> naming scheme instructs clabernetes to not use additional prefixes for the resources, as in this scenario we control the namespace and the resources are not going to clash with other resources in the cluster.</p> </li> </ol> <p>In the background, <code>clabverter</code> created the <code>Topology</code> custom resource (CR) in the <code>c9s-vlan</code><sup>5</sup> namespace that defines our topology and also created a set of config maps for each startup config used in the lab.</p>"},{"location":"manual/clabernetes/quickstart/#verifying-the-deployment","title":"Verifying the deployment","text":"<p>Once clabverter is done, clabernetes controller casts its spell known as reconciliation in the k8s world. It takes the spec of the <code>Topology</code> CR and creates a set of deployments, config maps and services that are required for lab's operation.</p> <p>Let's run some verification commands to see what we have in our cluster so far.</p>"},{"location":"manual/clabernetes/quickstart/#namespace","title":"Namespace","text":"<p>Remember how in Containerlab world if you wanted to run multiple labs on the same host you would give each lab a distinct name and containerlab would use that name to create a unique prefix for containers? In k8s world, we use namespaces to achieve the same goal.</p> <p>When Clabverter parses the original topology file, it takes the lab name value, prepends it with <code>c9s-</code> string and uses it as a namespace for the lab resources. This way, we can run multiple labs in the same k8s cluster without worrying about resource name clashes.</p> <pre><code>kubectl get ns\n</code></pre> <pre><code>NAME                 STATUS   AGE\nc9s                  Active   11m\nc9s-vlan             Active   9m8s\ndefault              Active   12h\nkube-node-lease      Active   12h\nkube-public          Active   12h\nkube-system          Active   12h\nlocal-path-storage   Active   12h\n</code></pre> <p>As you can see, we have two namespaces: <code>c9s</code> and <code>c9s-vlan</code>. The <code>c9s</code> namespace is where clabernetes manager is running and the <code>c9s-vlan</code> namespace is where our lab resources are deployed.</p>"},{"location":"manual/clabernetes/quickstart/#topology-resource","title":"Topology resource","text":"<p>The main clabernetes resource is called <code>Topology</code> and we should be able to find it in the <code>c9s-vlan</code> namespace where all lab resources are deployed:</p> <pre><code>kubectl get --namespace c9s-vlan Topology\n</code></pre> <pre><code>NAME   KIND           AGE\nvlan   containerlab   14h\n</code></pre> <p>Looking in the Topology CR we can see that the original containerlab topology definition can be found under the <code>spec.definition.containerlab</code> field of the custom resource. Clabernetes took the original topology and split it to sub-topologies that are outlined in the <code>status.configs</code> section of the resource:</p> <pre><code>kubectl get --namespace c9s-vlan Topology vlan -o yaml\n</code></pre> <code>spec.config</code><code>status.configs</code> <pre><code>spec:\n  definition:\n    containerlab: |-\n      name: vlan\n\n      topology:\n        nodes:\n          srl1:\n            kind: nokia_srlinux\n            image: ghcr.io/nokia/srlinux:23.10.1\n            startup-config: configs/srl.cfg\n\n          srl2:\n            kind: nokia_srlinux\n            image: ghcr.io/nokia/srlinux:23.10.1\n            startup-config: configs/srl.cfg\n\n          client1:\n            kind: linux\n            image: ghcr.io/srl-labs/alpine\n            binds:\n              - configs/client.sh:/config.sh\n            exec:\n              - \"ash -c '/config.sh 1'\"\n\n          client2:\n            kind: linux\n            image: ghcr.io/srl-labs/alpine\n            binds:\n              - configs/client.sh:/config.sh\n            exec:\n              - \"ash -c '/config.sh 2'\"\n\n        links:\n          # links between client1 and srl1\n          - endpoints: [client1:eth1, srl1:e1-1]\n\n          # inter-switch link\n          - endpoints: [srl1:e1-10, srl2:e1-10]\n\n          # links between client2 and srl2\n          - endpoints: [srl2:e1-1, client2:eth1]\n</code></pre> <pre><code># --snip--\nstatus:\n  configs:\n    client1: |\n      name: clabernetes-client1\n      prefix: \"\"\n      topology:\n          defaults:\n              ports:\n                  - 60000:21/tcp\n                  # here goes a list of exposed ports\n          nodes:\n              client1:\n                  kind: linux\n                  image: ghcr.io/srl-labs/alpine\n                  exec:\n                      - ash -c '/config.sh 1'\n                  binds:\n                      - configs/client.sh:/config.sh\n                  ports: []\n          links:\n              - endpoints:\n                  - client1:eth1\n                  - host:client1-eth1\n      debug: false\n    client2: |\n      name: clabernetes-client2\n      # similar configuration as for client1\n    srl1: |\n      name: clabernetes-srl1\n      prefix: \"\"\n      topology:\n          defaults:\n              ports:\n                  - 60000:21/tcp\n                  # here goes a list of exposed ports\n          nodes:\n              srl1:\n                  kind: nokia_srlinux\n                  startup-config: configs/srl.cfg\n                  image: ghcr.io/nokia/srlinux:23.10.1\n                  ports: []\n          links:\n              - endpoints:\n                  - srl1:e1-1\n                  - host:srl1-e1-1\n              - endpoints:\n                  - srl1:e1-10\n                  - host:srl1-e1-10\n      debug: false\n    srl2: |\n      name: clabernetes-srl2\n      # similar configuration as for srl1\n</code></pre> <p>If you take a closer look at the sub-topologies you will see that they are just mini, one-node-each, containerlab topologies. Clabernetes deploys these sub-topologies as deployments in the cluster.</p>"},{"location":"manual/clabernetes/quickstart/#deployments","title":"Deployments","text":"<p>The deployment objects created by Clabernetes are the vessels that carry the lab nodes. Let's list those deployments:</p> <pre><code>kubectl -n c9s-vlan get deployments\n</code></pre> <pre><code>NAME      READY   UP-TO-DATE   AVAILABLE   AGE\nclient1   1/1     1            1           16m\nclient2   1/1     1            1           16m\nsrl1      1/1     1            1           16m\nsrl2      1/1     1            1           16m\n</code></pre> <p>Those deployment names should be familiar as they are named exactly as the nodes in the original topology file.</p>"},{"location":"manual/clabernetes/quickstart/#pods","title":"Pods","text":"<p>Each deployment consists of exactly one k8s pod.</p> Listing lab pods<pre><code>kubectl get pods --namespace c9s-vlan -o wide\n</code></pre> <pre><code>NAME                       READY   STATUS    RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES\nclient1-5c4698f68c-v4z2n   1/1     Running   0          19m   10.244.1.15   c9s-worker    &lt;none&gt;           &lt;none&gt;\nclient2-6dfc49bc8f-hpkd4   1/1     Running   0          19m   10.244.2.15   c9s-worker2   &lt;none&gt;           &lt;none&gt;\nsrl1-78bdc85795-l9bl4      1/1     Running   0          19m   10.244.1.14   c9s-worker    &lt;none&gt;           &lt;none&gt;\nsrl2-7fffcdb79-vxfn9       1/1     Running   0          19m   10.244.2.16   c9s-worker2   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>We see four pods running, one pod per each lab node of our original containerlab topology. Pods are scheduled on different worker nodes by the k8s scheduler ensuring optimal resource utilization<sup>2</sup>.</p> <p>Each pod is a docker-in-docker container with Containerlab running inside. Inside each pod, containerlab runs the sub-topology as if it would run on a standalone Linux system. It has access to the Docker API and schedules nodes in exactly the same way as if no k8s exists  We can enter the pod's shell and use containerlab CLI as usual:</p> Fancy command to enter the pod's shellRegular command <pre><code>NS=c9s-vlan POD=client1; \\\nkubectl -n $NS exec -it \\\n  $(kubectl -n $NS get pods | grep ^$POD | awk '{print $1}') -- bash\n</code></pre> <pre><code>kubectl exec -it -n c9s-vlan client1-5c4698f68c-v4z2n -- bash\n</code></pre> <p>And in the pod's shell we swim in the familiar containerlab waters:</p> <pre><code>[*]\u2500[client1]\u2500[/clabernetes]\n\u2514\u2500\u2500&gt; containerlab inspect #(1)!\n</code></pre> <ol> <li>If you do not see any nodes in the <code>inspect</code> output give it a few minutes, as containerlab is pulling the image and starting the nodes. Monitor this process with <code>tail -f containerlab.log</code>.</li> </ol> <pre><code>INFO[0000] Parsing &amp; checking topology file: topo.clab.yaml\n+---+---------+--------------+-------------------------+-------+---------+----------------+----------------------+\n| # |  Name   | Container ID |          Image          | Kind  |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+---------+--------------+-------------------------+-------+---------+----------------+----------------------+\n| 1 | client1 | dbde60209a3b | ghcr.io/srl-labs/alpine | linux | running | 172.20.20.2/24 | 3fff:172:20:20::2/64 |\n+---+---------+--------------+-------------------------+-------+---------+----------------+----------------------+\n</code></pre> <p>We can <code>cat topo.clab.yaml</code> to see the subset of a topology that containerlab started in this pod.</p> <code>topo.clab.yaml</code> <pre><code>[*]\u2500[client1]\u2500[/clabernetes]\n\u2514\u2500\u2500&gt; cat topo.clab.yaml\nname: clabernetes-client1\nprefix: \"\"\ntopology:\n    defaults:\n        ports:\n            - 60000:21/tcp\n            - 60001:22/tcp\n            - 60002:23/tcp\n            - 60003:80/tcp\n            - 60000:161/udp\n            - 60004:443/tcp\n            - 60005:830/tcp\n            - 60006:5000/tcp\n            - 60007:5900/tcp\n            - 60008:6030/tcp\n            - 60009:9339/tcp\n            - 60010:9340/tcp\n            - 60011:9559/tcp\n            - 60012:57400/tcp\n    nodes:\n        client1:\n            kind: linux\n            image: ghcr.io/srl-labs/alpine\n            exec:\n                - ash -c '/config.sh 1'\n            binds:\n                - configs/client.sh:/config.sh\n            ports: []\n    links:\n        - endpoints:\n            - client1:eth1\n            - host:client1-eth1\ndebug: false\n</code></pre> <p>It is worth reiterating, that unmodified containerlab runs inside a pod as if it would've run on a Linux system in a standalone mode. It has access to the Docker API and schedules nodes in exactly the same way as if no k8s exists.</p>"},{"location":"manual/clabernetes/quickstart/#accessing-the-nodes","title":"Accessing the nodes","text":"<p>There are two common ways to access the lab nodes deployed with clabernetes:</p> <ol> <li>Using external address provided by the Load Balancer service.</li> <li>Entering the pod's shell and from there log in the running lab node. No load balancer required.</li> </ol> <p>We are going to show you both options and you can choose the one that suits you best.</p>"},{"location":"manual/clabernetes/quickstart/#load-balancer","title":"Load Balancer","text":"<p>Adding a Load Balancer to the k8s cluster makes accessing the nodes almost as easy as when working with containerlab. The kube-vip load balancer that we added before is going to provide an external IP address for a LoadBalancer k8s service that clabernetes creates for each deployment under its control.</p> <p>By default, clabernetes exposes<sup>3</sup> the following ports for each lab node:</p> Protocol Ports tcp <code>21</code>, <code>80</code>, <code>443</code>, <code>830</code>, <code>5000</code>, <code>5900</code>, <code>6030</code>, <code>9339</code>, <code>9340</code>, <code>9559</code>, <code>57400</code> udp <code>161</code> <p>Let's list the services in the <code>c9s-vlan</code> namespace (excluding the services for VXLAN tunnels<sup>6</sup>):</p> <pre><code>kubectl get -n c9s-vlan svc | grep -iv vx\n</code></pre> <pre><code>NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                                                                                                                   AGE\nclient1      LoadBalancer   10.96.211.232   172.18.1.12   161:30091/UDP,21:30581/TCP,22:31874/TCP,23:32665/TCP,80:31942/TCP,443:30624/TCP,830:32443/TCP,5000:31655/TCP,5900:30127/TCP,6030:30897/TCP,9339:31986/TCP,9340:31502/TCP,9559:30593/TCP,57400:30514/TCP   34m\nclient2      LoadBalancer   10.96.235.247   172.18.1.13   161:32008/UDP,21:31200/TCP,22:32186/TCP,23:30796/TCP,80:30124/TCP,443:30187/TCP,830:30599/TCP,5000:30600/TCP,5900:32719/TCP,6030:30678/TCP,9339:30998/TCP,9340:31592/TCP,9559:30312/TCP,57400:32051/TCP   34m\nsrl1         LoadBalancer   10.96.229.4     172.18.1.10   161:31220/UDP,21:32631/TCP,22:30595/TCP,23:32519/TCP,80:32630/TCP,443:30780/TCP,830:32259/TCP,5000:30647/TCP,5900:30540/TCP,6030:31110/TCP,9339:31216/TCP,9340:31168/TCP,9559:32483/TCP,57400:31009/TCP   34m\nsrl2         LoadBalancer   10.96.106.189   172.18.1.11   161:31380/UDP,21:30286/TCP,22:32764/TCP,23:31631/TCP,80:31049/TCP,443:32272/TCP,830:31237/TCP,5000:31495/TCP,5900:32592/TCP,6030:31776/TCP,9339:30649/TCP,9340:31538/TCP,9559:30628/TCP,57400:30586/TCP   34m\n</code></pre> <p>We see four <code>LoadBalancer</code> services created for each node of our distributed topology. Each service points (using selectors) to the corresponding pod.</p> <p>The LoadBalancer services (powered by the <code>kube-vip</code>) also provide us with the external IPs for the lab nodes. The long list of ports are the ports clabernetes exposes by default which spans both regular SSH and other well-known management interfaces and their ports.</p> <p>For instance, we see that <code>srl1</code> node has been assigned <code>172.18.1.10</code> IP, and we can immediately SSH into it from the outside world using the following command:</p> <pre><code>ssh admin@172.18.1.10\n</code></pre> <pre><code>................................................................\n:                  Welcome to Nokia SR Linux!                  :\n:              Open Network OS for the NetOps era.             :\n:                                                              :\n:    This is a freely distributed official container image.    :\n:                      Use it - Share it                       :\n:                                                              :\n: Get started: https://learn.srlinux.dev                       :\n: Container:   https://go.srlinux.dev/container-image          :\n: Docs:        https://doc.srlinux.dev/23-10                   :\n: Rel. notes:  https://doc.srlinux.dev/rn23-10-1               :\n: YANG:        https://yang.srlinux.dev/release/v23.10.1       :\n: Discord:     https://go.srlinux.dev/discord                  :\n: Contact:     https://go.srlinux.dev/contact-sales            :\n................................................................\n\nadmin@172.18.1.10's password:\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl1#  \n</code></pre> <p>Other services, like gNMI, JSON-RPC, SNMP are available as well since those ports are already exposed.</p> \"gNMI access\" <pre><code>gnmic -a 172.18.1.10 -u admin -p 'NokiaSrl1!' --skip-verify -e json_ietf \\\n  get --path /system/information/version\n</code></pre> <pre><code>[\n  {\n    \"source\": \"172.18.1.10\",\n    \"timestamp\": 1707828542585726740,\n    \"time\": \"2024-02-13T14:49:02.58572674+02:00\",\n    \"updates\": [\n      {\n        \"Path\": \"srl_nokia-system:system/srl_nokia-system-info:information/version\",\n        \"values\": {\n          \"srl_nokia-system:system/srl_nokia-system-info:information/version\": \"v23.10.1-218-ga3fc1bea5a\"\n        }\n      }\n    ]\n  }\n]\n</code></pre>"},{"location":"manual/clabernetes/quickstart/#pod-shell","title":"Pod shell","text":"<p>Load Balancer makes it easy to get external access to the lab nodes, but don't panic if for whatever reason you can't have one. It is still possible to access the nodes without the load balancer and external IP addresses using various techniques. One of them is to enter the pod's shell and from there log in the running lab node.</p> <p>For example, to access <code>srl1</code> lab node in our k8s cluster we can leverage <code>kubectl exec</code> command to get to the shell of the pod that runs <code>srl1</code> node.</p> <p>Note</p> <p>You may have a stellar experience with <code>k9s</code> project that offers a terminal UI to interact with k8s clusters. It is a great tool to have in your toolbox.</p> <p>If Terminal UI is not your jam, take a look at <code>kubectl</code> shell completions. They come in super handy, install them if you haven't yet.</p> <p>Since all pods are named after the nodes they are running, we can find the right one by listing all pods in a namespace:</p> <pre><code>kubectl get pods -n c9s-vlan\n</code></pre> <pre><code>NAME                       READY   STATUS    RESTARTS   AGE\nclient1-5c4698f68c-v4z2n   1/1     Running   0          66m\nclient2-6dfc49bc8f-hpkd4   1/1     Running   0          66m\nsrl1-78bdc85795-l9bl4      1/1     Running   0          66m\nsrl2-7fffcdb79-vxfn9       1/1     Running   0          66m\n</code></pre> <p>Looking at the pod named <code>srl1-78bdc85795-l9bl4</code> we clearly see that it runs the <code>srl1</code> node we specified in the topology. To get shell access to this node we can run:</p> fancy commandregular command <pre><code>NS=c9s-vlan POD=srl1; \\\nkubectl -n $NS exec -it \\\n  $(kubectl -n $NS get pods | grep ^$POD | awk '{print $1}') -- ssh $POD\n</code></pre> <pre><code>kubectl -n c9s-vlan exec -it srl1-78bdc85795-l9bl4 -- ssh srl1 #(1)!\n</code></pre> <ol> <li>If you installed <code>kubectl</code> shell completions, the pod names will be suggested as you type the command. Very handy!</li> </ol> <p>We essentially execute <code>ssh srl1</code> command inside the pod, as you'd normally do with containerlab.</p>"},{"location":"manual/clabernetes/quickstart/#datapath-stitching","title":"Datapath stitching","text":"<p>One of the challenges associated with the distributed labs is the connectivity between the lab nodes running on different computes.</p> <p>Thanks to Kubernetes and its services, the management network access is taken care of. You get access to the management interfaces of each pod out of the box. But what about the non-management/datapath links we have in the original topology file?</p> <p>In containerlab the links defined in the topology are often represented by the veth pairs between the containers running on a single host, but things are a bit more complicated in the distributed environments.</p> <p>If you rewind it back to the beginning of the quickstart where we looked at the Topology CR you would notice that it has the familiar <code>links</code> section in the <code>spec.definition.containerlab</code> field:</p> <pre><code># snip\nlinks:\n  - endpoints: [\"srl1:e1-10\", \"srl2:e1-10\"]\n</code></pre> <p>This link connects <code>srl1</code> node with <code>srl2</code>, and as we saw these nodes are running on different worker nodes in the k8s cluster. How does clabernetes lays out this link? Well, clabernetes takes the original link definition as provided by a user and transforms it into a set of point-to-point VXLAN tunnels<sup>4</sup> that stitch the nodes together.</p> <p>Two nodes appear to be connected to each other as if they were connected with a veth pair. We can check that LLDP neighbors are discovered on either other side of the link:</p> <pre><code>NS=c9s-vlan POD=srl1; \\\nkubectl -n $NS exec -it \\\n  $(kubectl -n $NS get pods | grep ^$POD | awk '{print $1}') -- \\\n    docker exec $POD sr_cli show system lldp neighbor\n</code></pre> <pre><code>+---------------+-------------------+----------------------+---------------------+------------------------+----------------------+---------------+\n|     Name      |     Neighbor      | Neighbor System Name | Neighbor Chassis ID | Neighbor First Message | Neighbor Last Update | Neighbor Port |\n+===============+===================+======================+=====================+========================+======================+===============+\n| ethernet-1/10 | 1A:C5:00:FF:00:00 | srl2                 | 1A:C5:00:FF:00:00   | an hour ago            | now                  | ethernet-1/10 |\n+---------------+-------------------+----------------------+---------------------+------------------------+----------------------+---------------+\n</code></pre> <p>We can also make sure that our startup-configuration that was provided in external files in original topology is applied in good order and we can perform the ping between two clients</p> fancy commandregular command <pre><code>NS=c9s-vlan POD=client1; \\\nkubectl -n $NS exec -it \\\n  $(kubectl -n $NS get pods | grep ^$POD | awk '{print $1}') -- \\\n    docker exec -it $POD ping -c 2 10.1.0.2\n</code></pre> <pre><code>kubectl exec -it -n c9s-vlan pod/vlan-client1-699dbcfd8b-r2fgc -- \\\ndocker exec -it client1 ping -c 2 10.1.0.2\n</code></pre> <pre><code>PING 10.1.0.2 (10.1.0.2) 56(84) bytes of data.\n64 bytes from 10.1.0.2: icmp_seq=1 ttl=64 time=2.08 ms\n64 bytes from 10.1.0.2: icmp_seq=2 ttl=64 time=1.04 ms\n\n--- 10.1.0.2 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1001ms\nrtt min/avg/max/mdev = 1.040/1.557/2.075/0.517 ms\n</code></pre> <p>With the command above we:</p> <ol> <li>connected to the <code>client1</code> pod that runs the <code>client1</code> lab node</li> <li>executed <code>ping</code> command inside the <code>client1</code> node to ping the <code>client2</code> node</li> <li>Ensured that the datapath stitching is working as expected</li> </ol>"},{"location":"manual/clabernetes/quickstart/#vm-based-nodes","title":"VM-based nodes?","text":"<p>In this quickstart we used native containerized Network OS - SR Linux - as it is lightweight and publicly available. But what if you want to use a VM-based Network OS like Nokia SR OS, Cisco IOS-XRv or Juniper vMX? Can you do that with clabernetes?</p> <p>Short answer is yes. Clabernetes should be able to run VM-based nodes as well, but your cluster nodes must support nested virtualization, same as you would need to run VM-based nodes in containerlab.</p> <p>Also you need to ensure that your VM-based container image is accessible to your cluster nodes, either via a public registry or a private one.</p> <p>When these considerations are taken care of, you can use the same topology file as you would use with containerlab. The only difference is that you need to specify the image in the topology file as a fully qualified image name, including the registry name.</p>"},{"location":"manual/clabernetes/quickstart/#cleaning-up","title":"Cleaning up","text":"<p>When you are done with the lab, you may free resources using: <pre><code>kind delete clusters c9s\n</code></pre></p> <ol> <li> <p>In general there are no requirements for clabernetes from a kubernetes cluster perspective, however, many device types may have requirements for nested virtualization or specific CPU flags that your nodes would need to support in order to run the device.\u00a0\u21a9</p> </li> <li> <p>They may run on the same node, this is up to the kubernetes scheduler whose job it is to schedule pods on the nodes it deems most appropriate.\u00a0\u21a9</p> </li> <li> <p>Default exposed ports can be overwritten by a user via Topology CR.\u00a0\u21a9</p> </li> <li> <p>Using containerlab's vxlan tunneling workflow to create tunnels.\u00a0\u21a9</p> </li> <li> <p>The namespace name is derived from the name of the lab in the <code>.clab.yml</code> file.\u00a0\u21a9</p> </li> <li> <p>VXLAN services are used for datapath stitching and are not meant to be accessed from outside the cluster.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/dev/","title":"Developers Guide","text":"<p>Containerlab relies on contributions from the community to improve existing functionality and add new features. The developers guide provides information on how to contribute to various bits of the project.</p> <p>Remember, that all contributions are welcome and important, no matter how big or small they are. Spot a typo in the documentation? Found a bug? Have an idea for a new feature? Want to improve the performance of the code? All of these are valuable contributions to the project.</p> <p>Thank you for considering contributing to containerlab!</p>"},{"location":"manual/dev/debug/","title":"Debugging","text":"<p>There is a point where you realized that putting another <code>fmt.Println(\"here100500\")</code> is not enough. You need to have a debugger to inspect the state of your program.</p>"},{"location":"manual/dev/debug/#debugging-in-vscode","title":"Debugging in VSCode","text":"<p>Debugging containerlab in VSCode relies on the Go Dlv integration with VSCode and can be split into two categories:</p> <ol> <li>Debugging using the root user</li> <li>Debugging using the non-<code>root</code> user</li> </ol> <p>Since containerlab requires the superuser privileges to run, the workflow will be slightly different depending on if operate as a <code>root</code> user already or not.</p> <p>We will document the workflow for the latter (non-root user) case, as this is the most common scenario. In the non-root user case a developer should create a debug configuration file and optionally a task file to build the binary. The reason for the build step is rooted in a fact that we would need to build the binary first as our user, and then the debugger will be called as a <code>root</code> user and execute the binary with the debug mode.</p>"},{"location":"manual/dev/debug/#create-a-debug-configuration","title":"Create a debug configuration","text":"<p>The debug configuration defined in the <code>launch.json</code> file will contain the important fields such as <code>asRoot</code> and <code>console</code>, both needed for the debugging as a root user.</p> <p>Here is an example of a configuration file that launches <code>containerlab tools vxlan create</code> command in the debug mode:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"tools vxlan create\",\n            \"type\": \"go\",\n            \"request\": \"launch\",\n            \"mode\": \"exec\",\n            \"console\": \"integratedTerminal\",\n            \"asRoot\": true,\n            \"program\": \"${workspaceFolder}/bin/containerlab\",\n            \"args\": [\n                \"tools\",\n                \"vxlan\",\n                \"create\",\n                \"--remote\",\n                \"10.0.0.20\",\n                \"-l\",\n                \"ens3\"\n            ],\n            \"preLaunchTask\": \"delete vx-ens3 interface\",\n        }\n    ]\n}\n</code></pre> <p>The debug config is run in the <code>exec</code> mode, which means that the debugger expects a program to be built first. This is why we need to create a task file to build the binary first.</p> <p>The build happens via the <code>preLaunchTask</code> field, that references the task in a <code>tasks.json</code> file.</p>"},{"location":"manual/dev/debug/#create-a-task-file-to-build-the-binary","title":"Create a task file to build the binary","text":"<p>The task file provides the means to define arbitrary tasks that can be executed via VSCode UI and as well hooked up in the debug configuration.</p> <p>Here is a simple task file that contains two tasks - one is building the binary with the debug information, and the other is a simple command that removes a test interface that the <code>tools vxlan create</code> command creates. The only task you need is the build task, but we wanted to show you how to define additional tasks that might be required before your run containerlab in the debug mode to cleanup from the previous execution.</p> <p>The dependencies between the tasks are defined in the <code>dependsOn</code> field, and this is how you can first build the binary and then run the preparation step.</p> <pre><code>{\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n        {\n            \"label\": \"delete vx-ens3 interface\",\n            \"type\": \"shell\",\n            \"command\": \"sudo ip link delete vx-ens3\",\n            \"presentation\": {\n                \"reveal\": \"always\",\n                \"panel\": \"new\"\n            },\n            \"dependsOn\": \"make build-dlv-debug\",\n            \"problemMatcher\": []\n        },\n        {\n            \"label\": \"make build-dlv-debug\",\n            \"type\": \"shell\",\n            \"command\": \"make\",\n            \"args\": [\n                \"build-dlv-debug\"\n            ],\n        }\n    ]\n}\n</code></pre> <p>Reach out via Discord to get help if you get stuck.</p>"},{"location":"manual/dev/doc/","title":"Containerlab Documentation","text":"<p>Contributing documentation is as valuable as contributing code; it is also a great way to start contributing to the project. The containerlab documentation is part of the same repo that contains the code.</p> <p>We tried to make it as easy as possible to contribute to the documentation. Starting from small edits that can be solely done in the browser, to more complex and thorough changes with a live dev server running on your machine to control the development process.</p>"},{"location":"manual/dev/doc/#online-editing","title":"Online editing","text":"<p>If you found a typo, or want to add a little piece of documentation you can do this all in your browser! On each documentation page you will find an \"Edit this page\" icon in the top right corner. Clicking on it will take you to the markdown file in the GitHub repository where you can make your changes. Once you are done, you can submit a pull request with your changes.</p> <p>Sometimes you might want to have a change that spans more than one file, in that case you use GitHub's online VS Code experience and opening the repo in your browser by following the <code>github.dev/srl-labs/containerlab</code> link.</p>"},{"location":"manual/dev/doc/#offline-editing","title":"Offline editing","text":"<p>While online editing makes it easy to make small changes, it doesn't offer you a preview of the changes you're making, and this might be a bit cumbersome for larger changes. For this reason, we recommend setting up a local development environment to preview your changes when you feel like your changes are more substantial than a typo fix.</p> <p>To setup the dev environment you have to have Docker installed, which is a requirement for containerlab anyway. Once you have Docker installed, you can run the following command to start the development server:</p> <ol> <li>Fork the srl-labs/containerlab repo</li> <li>Clone your fork of the repo locally</li> <li>Change to the local repo top level directory - <code>cd containerlab</code></li> <li>Run <code>make serve-docs-full PUBLIC=yes</code></li> </ol> <p>You can access the local website content from your browser at http://localhost:8001</p> <p>Look at the <code>nav</code> key in the <code>mkdocs.yml</code> file to identify the markdown file that corresponds to the page you want to edit.</p> <p>Any new content page should be added as a markdown file at a suitable location in the <code>docs</code> hierarchy and added under the <code>nav</code> key in <code>mkdocs.yml</code> to be reflected in the website documentation.</p> <p>Consult the mkdocs and mkdocs-material for more information.</p> <p>Once the documentation changes are complete, commit the changes and raise a pull request.</p>"},{"location":"manual/dev/doc/#diagrams","title":"Diagrams","text":"<p>We prefer scalable vector diagrams for crisp sharp images checked into the repository. But you probably already noticed that.</p> <p>Containerlab drawings are made using the free online diagramming tool called diagrams.net. If you have multiple diagrams, create multiple pages in your diagram file - one per diagram.</p> <p>Diagrams are stored in the diagrams  branch, not the <code>main</code> branch. Create a fork of the <code>diagrams</code> branch to commit your draw.io file and raise a pull request targeting the <code>srl-labs/containerlab:diagrams</code> branch.</p> <p>Once your diagram PR is merged, you can embed it in any markdown document using the below markup -</p> <pre><code>&lt;div class='mxgraph'\n  style='max-width:100%;border:1px solid transparent;margin:0 auto; display:block;'\n  data-mxgraph='{\"page\":0,\"zoom\":1,\"highlight\":\"#0000ff\",\"nav\":true,\"resize\":true,\"edit\":\"_blank\",\n      \"url\":\"https://raw.githubusercontent.com/srl-labs/containerlab/diagrams/YOUR-DIAGRAM.drawio\"}'&gt;\n&lt;/div&gt;\n</code></pre> <p>Replace <code>YOUR-DIAGRAM</code> with the name of your diagram file in the above markup. If your file has multiple pages, you can specify the required page number in the above markup. Each diagram will need a markup like the above. You can add multiple markups for different diagrams in the same markdown file.</p> <p>You MUST also add the below HTML markup at the bottom of your markdown file so that the diagrams are viewable -</p> <pre><code>&lt;script type=\"text/javascript\" src=\"https://viewer.diagrams.net/js/viewer-static.min.js\" async&gt;&lt;/script&gt;\n</code></pre>"},{"location":"manual/dev/test/","title":"Testing Containerlab","text":"<p>Containerlab's test program largely consists of:</p> <ul> <li>Go-based unit tests</li> <li>RobotFramework-based integration tests</li> </ul>"},{"location":"manual/dev/test/#integration-tests","title":"Integration Tests","text":"<p>The integration tests are written in RobotFramework and are located in the <code>tests</code> directory. The tests are run using the <code>rf-run</code> command that wraps <code>robot</code> command. The tests are run in a Docker container, so you don't need to install RobotFramework on your local machine.</p>"},{"location":"manual/dev/test/#local-execution","title":"Local execution","text":"<p>To execute the integration tests locally you have to install the python environment with the required dependencies. If you're using <code>pyenv</code> you can use the following commands:</p> <ol> <li> <p>Create a venv and activate it</p> <pre><code>pyenv virtualenv 3.11 clab-rf\npyenv shell clab-rf\n</code></pre> </li> <li> <p>Install the dependencies</p> <pre><code>pip install -r tests/requirements.txt\n</code></pre> </li> </ol> <p>Usually you would run the tests using the locally built containerlab binary that contains the unreleased changes. The typical workflow then starts with building the containerlab binary:</p> <pre><code>make build\n</code></pre> <p>The newly built binary is located in the <code>bin</code> directory. In order to let the test runner script know where to find the binary, you have to set the <code>CLAB_BIN</code> environment variable before calling the <code>rf-run</code> script:</p> <pre><code>CLAB_BIN=$(pwd)/bin/containerlab ./tests/rf-run.sh &lt;runtime&gt; &lt;test suite&gt;\n</code></pre> <p>Note</p> <p>The test runner script requires you to specify the runtime as its first argument. The runtime can be either <code>docker</code> or <code>podman</code>. Containerlab primarily uses Docker as the default runtime, hence the number of tests written for docker outnumber the podman tests.</p>"},{"location":"manual/dev/test/#selecting-the-test-suite","title":"Selecting the test suite","text":"<p>Containerlab's integration tests are grouped by a topic, and each topic is mapped to a directory under the <code>tests</code> directory and RobotFramework allows for a flexible selection of tests/test suites to run. For example, to run all the smoke test cases, you can use the following command:</p> <pre><code>CLAB_BIN=$(pwd)/bin/containerlab ./tests/rf-run.sh docker tests/01-smoke\n</code></pre> <p>since <code>01-smoke</code> is a directory containing all the smoke test suites.</p> <p>Consequently, in order to run a specific test suite you just need to provide a path to it. E.g. running the <code>01-basic-flow.robot</code> test suite from the <code>01-smoke</code> directory:</p> <pre><code>CLAB_BIN=$(pwd)/bin/containerlab ./tests/rf-run.sh docker tests/01-smoke/01-basic-flow.robot\n</code></pre> <p>Note</p> <p>Selecting a specific test case in a test suite is not supported, since test suites are written in a way that test cases depend on previous ones.</p>"},{"location":"manual/dev/test/#inspecting-the-test-results","title":"Inspecting the test results","text":"<p>RobotFramework generates a detailed report in HTML and XML formats that can be found in the <code>tests/out</code> directory. The exact paths to the reports are printed to the console after the test run.</p>"},{"location":"manual/kinds/","title":"Kinds","text":"<p>Containerlab launches, wires up and manages container-based labs. The steps required to launch a vanilla <code>debian</code> or <code>centos</code> container image aren't at all different. On the other hand, Nokia SR Linux launching procedure is nothing like the one for Arista cEOS.</p> <p>Things like required syscalls, mounted directories, entrypoint and commands to execute are all different for the containerized NOS'es. To let containerlab understand which launching sequence to use, the notion of a <code>kind</code> was introduced. Essentially <code>kinds</code> abstract away the need to understand certain setup peculiarities of different NOS'es.</p> <p>Given the following topology definition file, containerlab is able to know how to launch <code>node1</code> as an SR Linux container and <code>node2</code> as a cEOS one because they are associated with the kinds:</p> <pre><code>name: srlceos01\n\ntopology:\n  nodes:\n    node1:\n      # node1 is of nokia_srlinux kind\n      kind: nokia_srlinux\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n    node2:\n      # node2 is of ceos kind\n      kind: arista_ceos\n      image: ceos:4.32.0F\n\n  links:\n    - endpoints: [\"node1:e1-1\", \"node2:eth1\"]\n</code></pre> <p>Containerlab supports a fixed number of platforms. Most platforms are identified with both a short and a long <code>kind</code> name; these names can be used interchangeably.</p> <p>Within each predefined kind, we store the necessary information that is used to successfully launch the container. The following kinds are supported by containerlab:</p> Name Short/Long kind name Status Packaging Nokia SR Linux <code>nokia_srlinux</code> supported container Nokia SR OS <code>nokia_sros</code> supported VM Arista cEOS <code>arista_ceos</code> supported container Arista vEOS <code>arista_veos</code> supported VM Juniper cRPD <code>juniper_crpd</code> supported container Juniper vMX <code>juniper_vmx</code> supported VM Juniper vQFX <code>juniper_vqfx</code> supported VM Juniper vSRX <code>juniper_vsrx</code> supported VM Juniper vJunos-router <code>juniper_vjunosrouter</code> supported VM Juniper vJunos-switch <code>juniper_vjunosswitch</code> supported VM Juniper vJunosEvolved <code>juniper_vjunosevolved</code> supported VM Cisco XRd <code>cisco_xrd</code> supported container Cisco XRv9k <code>cisco_xrv9k</code> supported VM Cisco XRv <code>cisco_xrv</code> supported VM Cisco CSR1000v <code>cisco_csr1000v</code> supported VM Cisco Nexus 9000v <code>cisco_n9kv</code> supported VM Cisco 8000 <code>cisco_c8000</code> supported VM+ Cisco Catalyst 9000v <code>cisco_cat9kv</code> supported VM Cisco IOL <code>cisco_iol</code> supported container Cisco FTDv <code>cisco_ftdv</code> supported VM Cumulus VX <code>cumulus_cvx</code> supported container Aruba ArubaOS-CX <code>aruba_aoscx</code> supported VM SONiC <code>sonic</code> supported container SONiC VM <code>sonic_vm</code> supported VM Dell FTOS10v <code>dell_ftos</code> supported VM Dell SONiC <code>dell_sonic</code> supported VM Mikrotik RouterOS <code>mikrotik_ros</code> supported VM Huawei VRP <code>huawei_vrp</code> supported VM IPInfusion OcNOS <code>ipinfusion_ocnos</code> supported VM OpenBSD <code>openbsd</code> supported VM Keysight ixia-c-one <code>keysight_ixia-c-one</code> supported container Ostinato <code>linux</code> supported container Check Point Cloudguard <code>checkpoint_cloudguard</code> supported VM Fortinet Fortigate <code>fortinet_fortigate</code> supported VM Palo Alto PAN <code>paloalto_panos</code> supported VM Linux bridge <code>bridge</code> supported N/A Linux container <code>linux</code> supported container RARE/freeRtr <code>rare</code> supported container Openvswitch bridge <code>ovs-bridge</code> supported N/A External container <code>ext-container</code> supported container Host <code>host</code> supported N/A <p>Refer to a specific kind documentation article for kind-specific details.</p>","boost":4},{"location":"manual/kinds/bridge/","title":"Linux bridge","text":"","boost":4},{"location":"manual/kinds/bridge/#linux-bridge","title":"Linux bridge","text":"<p>Containerlab can connect its nodes to a Linux bridge instead of interconnecting the nodes directly. This connectivity option is enabled with <code>bridge</code> kind and opens a variety of integrations that containerlab labs can have with workloads of other types.</p> <p>For example, by connecting a lab node to a bridge we can:</p> <ol> <li>allow a node to talk to any workload (VM, container, baremetal) which are connected to that bridge</li> <li>let a node to reach networks which are available via that bridge</li> <li>scale out containerlab labs by running separate labs in different hosts and get network reachability between them</li> <li>wiring nodes' data interfaces via a broadcast domain (linux bridge) and use vlans to making dynamic connections</li> </ol> Using bridges","boost":4},{"location":"manual/kinds/bridge/#using-bridge-kind","title":"Using bridge kind","text":"<p>Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file, the bridge needs to be created and enabled first.</p> <p>Once the bridge is created, it needs to be referenced as a node inside the topology file:</p> <pre><code># topology documentation: http://containerlab.dev/lab-examples/ext-bridge/\nname: br01\n\ntopology:\n  kinds:\n    nokia_srlinux:\n      type: ixrd2l\n      image: ghcr.io/nokia/srlinux\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n    srl2:\n      kind: nokia_srlinux\n    srl3:\n      kind: nokia_srlinux\n    # note, that the bridge br-clab must be created manually\n    br-clab:\n      kind: bridge\n\n  links:\n    - endpoints: [\"srl1:e1-1\", \"br-clab:eth1\"]\n    - endpoints: [\"srl2:e1-1\", \"br-clab:eth2\"]\n    - endpoints: [\"srl3:e1-1\", \"br-clab:eth3\"]\n</code></pre> <p>In the example above, node <code>br-clab</code> of kind <code>bridge</code> tells containerlab to identify it as a linux bridge and look for a bridge named <code>br-clab</code>.</p> <p>When connecting other nodes to a bridge, the bridge endpoint must be present in the <code>links</code> section.</p> <p>Subtle Note</p> <p>When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we named interfaces <code>eth1</code>, <code>eth2</code>, <code>eth3</code> accordingly and ensured that none of these interfaces existed before in the root netns.  </p> <p>As a result of such topology definition, you will see bridge <code>br-clab</code> with three interfaces attached to it:</p> <pre><code>bridge name     bridge id               STP enabled     interfaces\nbr-clab         8000.6281eb7133d2       no              eth1\n                                                        eth2\n                                                        eth3\n</code></pre> <p>Containerlab automatically adds iptables rules for the referenced bridges (v4 and v6) to allow traffic ingressing to the bridges. Namely, for a given bridge named <code>br-clab</code> containerlab will attempt to create the allowing rule in the filter table, FORWARD chain like this:</p> <pre><code>iptables -I FORWARD -i br-clab -j ACCEPT\n</code></pre> <p>This will ensure that traffic is forwarded when passing this particular bridge. Note, that once you destroy the lab, the rule will stay, if you wish to remove it, you will have to do it manually.</p> <p>Check out \"External bridge\" lab for a ready-made example on how to use bridges.</p>","boost":4},{"location":"manual/kinds/c8000/","title":"Cisco 8000","text":"<p>Cisco 8000 platform emulator is identified with <code>c8000</code> or <code>cisco_c8000</code> kind in the topology file.</p> <p>The 8000 emulator is an enhanced KVM hypervisor that emulates Cisco boards and chassis. The 8000 emulator VM is launched inside a container for ease of integration with ContainerLab.</p>","boost":4},{"location":"manual/kinds/c8000/#getting-cisco-8000-containerlab-docker-images","title":"Getting Cisco 8000 ContainerLab docker images","text":"<p>Cisco customers can contact their Cisco account team to get access to Cisco 8000 ContainerLab docker images.</p> <p>The obtained image archive can be loaded to local docker image store with (example):</p> <pre><code>docker image load -i 8201-32fh-clab_7.9.1.tar.gz\n</code></pre>","boost":4},{"location":"manual/kinds/c8000/#supported-platforms","title":"Supported platforms","text":"<ul> <li>Fixed form platforms   8101-32H, 8102-64H, 8201-32FH, 8201, 8202-32FH-M</li> <li>Modular chassis (8808 and 8804)   8800-LC-36FH, 8800-LC-48H, 8800-LC-36FH-M</li> </ul> <p>Additional platforms will be supported on \"as needed\" basis.</p>","boost":4},{"location":"manual/kinds/c8000/#host-server-requirements","title":"Host server requirements","text":"<ul> <li>Open vSwitch</li> <li>KVM</li> <li>Limit <code>/proc/sys/kernel/pid_max</code> to 1048575 <code>sysctl -w kernel.pid_max=1048575</code></li> </ul>","boost":4},{"location":"manual/kinds/c8000/#hardware-resource-requirements","title":"Hardware resource requirements","text":"<p>Memory and cpu usage depends on XR features enabled and control/data plane traffic</p> <ul> <li> <p>Fixed form platforms   Recommended 20GB memory and 4 cores per router</p> </li> <li> <p>Modular chassis   Recommended 64GB memory and 8 cores per router</p> </li> </ul>","boost":4},{"location":"manual/kinds/c8000/#managing-c8000-nodes","title":"Managing c8000 nodes","text":"<p>Info</p> <p>Cisco 8000 nodes may take a few minutes to come to XR prompt. To monitor boot progress: <pre><code>docker logs &lt;container-name/id&gt; -f\n</code></pre> Wait for <code>Router up</code> message.</p> SSHXR consolebashNetconf <p><code>ssh cisco@&lt;node-mgmt-address&gt;</code> Password: <code>cisco123</code></p> <p>to connect to a XR console (via telnet): <pre><code>docker exec -it &lt;container-name/id&gt; telnet 0 60000\n</code></pre></p> <p>to connect to a <code>bash</code> shell of a running c8000 container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>Netconf server runs on <code>830</code> port: <pre><code>ssh cisco@&lt;node-mgmt-address&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default credentials: <code>cisco:cisco123</code></p>","boost":4},{"location":"manual/kinds/c8000/#interface-naming-convention","title":"Interface naming convention","text":"<p>c8000 container uses the following naming convention for its management and data interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>Hu0_0_0_X</code> - 100G data interface mapped to <code>HundredGigE0/0/0/X</code> internal interface.</li> <li><code>FH0_0_0_X</code> - 400G data interface mapped to <code>FourHundredGigE0/0/0/X</code> internal interface.</li> </ul> <p>When containerlab launches c8000 node, it will set IPv4 address as assigned by docker to the <code>eth0</code> interface and c8000 node will boot with this address configured for its <code>MgmtEth0</code>.</p> <pre><code>RP/0/RP0/CPU0:r1#sh ip int br\nWed Dec 21 12:04:13.049 UTC\n\nInterface                      IP-Address      Status          Protocol Vrf-Name\nMgmtEth0/RP0/CPU0/0            172.20.20.5     Up              Up       default\n</code></pre>","boost":4},{"location":"manual/kinds/c8000/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/c8000/#node-configuration","title":"Node configuration","text":"","boost":4},{"location":"manual/kinds/c8000/#default-node-configuration","title":"Default node configuration","text":"<p>It is possible to launch nodes of <code>cisco_c8000</code> kind with a basic config or to provide a custom config file that will be used as a startup config instead.</p> <p>When a node is defined without <code>startup-config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p>","boost":4},{"location":"manual/kinds/c8000/#user-defined-config","title":"User defined config","text":"<p>With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>name: c8201\ntopology:\n  nodes:\n    c8000:\n      kind: cisco_c8000\n      startup-config: r1.cfg\n</code></pre> <p>When a config file is passed via <code>startup-config</code> parameter it will be used during an initial lab deployment. However, a config file that might be in the lab directory of a node takes precedence over the startup-config<sup>1</sup>.</p> <p>With such topology file containerlab is instructed to take a file <code>r1.cfg</code> from the current working directory and copy it to the lab directory for that specific node under the <code>/first-boot.cfg</code> name. This will result in this config acting as a startup-config for the node.</p> <p>To provide a user-defined config, take the default configuration template and add the necessary configuration commands without changing the rest of the file. This will result in proper automatic assignment of IP addresses to the management interface, as well as applying user-defined commands.</p>","boost":4},{"location":"manual/kinds/c8000/#lab-examples","title":"Lab examples","text":"<pre><code>name: test\ntopology:\n  nodes:\n    Cisco8201-1:\n      kind: cisco_c8000\n      image: 8201-32fh-clab:7.9.1\n      image-pull-policy: Never\n\n    Cisco8201-2:\n      kind: cisco_c8000\n      image: 8201-32fh-clab:7.9.1\n      image-pull-policy: Never\n\n  links:\n    - endpoints: [\"Cisco8201-1:FH0_0_0_0\", \"Cisco8201-2:FH0_0_0_0\"]\n</code></pre> <ol> <li> <p>if startup config needs to be enforced, either deploy a lab with <code>--reconfigure</code> flag, or use <code>enforce-startup-config</code> setting.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/ceos/","title":"Arista cEOS","text":"<p>Arista cEOS is identified with <code>ceos</code> or <code>arista_ceos</code> kind in the topology file. The <code>ceos</code> kind defines a supported feature set and a startup procedure of a <code>ceos</code> node.</p> <p>cEOS nodes launched with containerlab comes up with</p> <ul> <li>their management interface <code>eth0</code> configured with IPv4/6 addresses as assigned by docker</li> <li>hostname assigned to the node name</li> <li>gNMI, Netconf and eAPI services enabled</li> <li><code>admin</code> user created with password <code>admin</code></li> </ul>","boost":4},{"location":"manual/kinds/ceos/#getting-ceos-image","title":"Getting cEOS image","text":"<p>Arista requires its users to register with arista.com before downloading any images. Once you created an account and logged in, go to the software downloads section and download ceos64 tar archive for a given release.</p> <p>Once downloaded, import the archive with docker:</p> <pre><code># import container image and save it under ceos:4.32.0F name\ndocker import cEOS64-lab-4.32.0F.tar.xz ceos:4.32.0F\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#managing-ceos-nodes","title":"Managing ceos nodes","text":"<p>Arista cEOS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgNMI <p>to connect to a <code>bash</code> shell of a running ceos container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the ceos CLI <pre><code>docker exec -it &lt;container-name/id&gt; Cli\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh root@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>gNMI server is running over port 6030 in non-secure mode using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt;:6030 --insecure \\\n-u admin -p admin \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/ceos/#interfaces-mapping","title":"Interfaces mapping","text":"<p>ceos container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code><sup>4</sup> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface</li> </ul> <p>When containerlab launches ceos node, it will set IPv4/6 addresses as assigned by docker to the <code>eth0</code> interface and ceos node will boot with that addresses configured. Data interfaces <code>eth1+</code> need to be configured with IP addressing manually.</p> ceos interfaces output <p>This output demonstrates the IP addressing of the linux interfaces of ceos node. <pre><code>bash-4.2# ip address\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/24 scope host lo\n    valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n    valid_lft forever preferred_lft forever\n&lt;SNIP&gt;\n5877: eth0@if5878: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether 02:42:ac:14:14:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 172.20.20.2/24 brd 172.20.20.255 scope global eth0\n    valid_lft forever preferred_lft forever\n    inet6 3fff:172:20:20::2/80 scope global\n    valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe14:1402/64 scope link\n    valid_lft forever preferred_lft forever\n</code></pre> This output shows how the linux interfaces are mapped into the ceos OS. <pre><code>ceos&gt;sh ip int br\n                                                                            Address\nInterface         IP Address           Status       Protocol           MTU    Owner\n----------------- -------------------- ------------ -------------- ---------- -------\nManagement0       172.20.20.2/24       up           up                1500\n\nceos&gt;sh ipv6 int br\nInterface       Status        MTU       IPv6 Address                     Addr State    Addr Source\n--------------- ------------ ---------- -------------------------------- ---------------- -----------\nMa0             up           1500       fe80::42:acff:fe14:1402/64       up            link local\n                                        3fff:172:20:20::2/80             up            config\n</code></pre> As you see, the management interface <code>Ma0</code> inherits the IP address that docker assigned to ceos container management interface.</p>","boost":4},{"location":"manual/kinds/ceos/#user-defined-interface-mapping","title":"User-defined interface mapping","text":"<p>Note</p> <p>Supported in cEOS &gt;= 4.28.0F</p> <p>It is possible to make ceos nodes boot up with a user-defined interface layout. With the <code>binds</code> property, a user sets the path to the interface mapping file that will be mounted to a container and used during bootup. The underlying linux <code>eth</code> interfaces (used in the containerlab topology file) are mapped to cEOS interfaces in this file. The following shows an example of how this mapping file is structured:</p> <pre><code>{\n  \"ManagementIntf\": {\n    \"eth0\": \"Management1\"\n  },\n  \"EthernetIntf\": {\n    \"eth1\": \"Ethernet1/1\",\n    \"eth2\": \"Ethernet2/1\",\n    \"eth3\": \"Ethernet27/1\",\n    \"eth4\": \"Ethernet28/1\",\n    \"eth5\": \"Ethernet3/1/1\",\n    \"eth6\": \"Ethernet5/2/1\"\n  }\n}\n</code></pre> <p>Linux's <code>eth0</code> interface is always used to map the management interface.</p> <p>With the following topology file, containerlab is instructed to take a <code>mymapping.json</code> file located in the same directory as the topology and mount that to the container as <code>/mnt/flash/EosIntfMapping.json</code>. This will result in this interface mapping being considered during the bootup of the node. The destination for that bind has to be <code>/mnt/flash/EosIntfMapping.json</code>.</p> <ol> <li>Craft a valid interface mapping file.</li> <li> <p>Use <code>binds</code> config option for a ceos node/kind to make this file available in the container's filesystem:</p> <pre><code>name: ceos\n\ntopology:\n  nodes:\n    ceos1:\n      kind: ceos\n      image: ceos:4.32.0F\n      binds:\n        - mymapping.json:/mnt/flash/EosIntfMapping.json:ro # (1)!\n    ceos2:\n      kind: ceos\n      image: ceos:4.32.0F\n      binds:\n        - mymapping.json:/mnt/flash/EosIntfMapping.json:ro\n  links:\n    - endpoints: [\"ceos1:eth1\", \"ceos2:eth1\"]\n</code></pre> <ol> <li>If all ceos nodes use the same interface mapping file, it is easier to set the bind instruction on a kind level</li> </ol> <pre><code>    topology:\n      kinds:\n        ceos:\n          binds:\n            - mymapping.json:/mnt/flash/EosIntfMapping.json:ro\n      nodes:\n        ceos1:\n          kind: ceos\n          image: ceos:4.32.0F\n        ceos2:\n          kind: ceos\n          image: ceos:4.32.0F\n</code></pre> <p>This way the bind is set only once, and nodes of <code>ceos</code> kind will have these binds applied.</p> </li> </ol>","boost":4},{"location":"manual/kinds/ceos/#additional-interface-naming-considerations","title":"Additional interface naming considerations","text":"<p>While many users will be fine with the default ceos naming of <code>eth</code>, some ceos users may find that they need to name their interfaces <code>et</code>. Interfaces named <code>et</code> provide consistency with the underlying interface mappings within ceos. This enables the correct operation of commands/features which depend on <code>et</code> format interface naming.</p> <p>In order to align interfaces in this manner, the <code>INTFTYPE</code> environment variable must be set to <code>et</code> in the topology definition file and the links which are defined must be named <code>et</code>, as opposed to <code>eth</code>. This naming requirement does not apply to the <code>eth0</code> interface automatically created by containerlab. This is only required for links that are used for interconnection with other elements in a topology.</p> <p>example:</p> <pre><code>topology:\n  defaults:\n    env:\n      INTFTYPE: et\n  nodes:\n  # --snip--\n  links:\n    - endpoints: [\"ceos_rtr1:et1\", \"ceos_rtr2:et1\"]\n    - endpoints: [\"ceos_rtr1:et2\", \"ceos_rtr3:et1\"]\n</code></pre> <p>If the only purpose of renaming the interfaces is to add breakouts (\"/1\", etc.) to the interface naming to match the future physical setup, it is possible to use underscores (\"_\") in the interface names.</p> <pre><code>name: ceos\n\ntopology:\n  nodes:\n    ceos1:\n      kind: ceos\n      image: ceos:4.32.0F\n    ceos2:\n      kind: ceos\n      image: ceos:4.32.0F\n  links:\n    - endpoints: [\"ceos1:eth1_1\", \"ceos2:eth2_1_1\"]\n</code></pre> <p>This topology will be equivalent to <code>ceos1:Ethernet1/1</code> connected to <code>ceos2:Ethernet2/1/1</code>.</p> <p>Note</p> <p>This feature can not be used together with interface mapping. If the interface mapping is in use, all names must be redefined in the map and the underscore naming option will not work. Also, it's only possible to rename Ethernet interfaces this way, not management ports.</p>","boost":4},{"location":"manual/kinds/ceos/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/ceos/#node-configuration","title":"Node configuration","text":"<p>cEOS nodes have a dedicated <code>config</code> directory that is used to persist the configuration of the node. It is possible to launch nodes of <code>ceos</code> kind with a basic config or to provide a custom config file that will be used as a startup config instead.</p>","boost":4},{"location":"manual/kinds/ceos/#default-node-configuration","title":"Default node configuration","text":"<p>When a node is defined without <code>startup-config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p> <pre><code># example of a topo file that does not define a custom config\n# as a result, the config will be generated from a template\n# and used by this node\nname: ceos\ntopology:\n  nodes:\n    ceos:\n      kind: ceos\n</code></pre> <p>The generated config will be saved by the path <code>clab-&lt;lab_name&gt;/&lt;node-name&gt;/flash/startup-config</code>. Using the example topology presented above, the exact path to the config will be <code>clab-ceos/ceos/flash/startup-config</code>.</p> <p>cEOS Ma0 interface will be configured with a random MAC address with <code>00:1c:73</code> OUI part. Containerlab will also create a <code>system_mac_address</code> file in the node's lab directory with the value of a System MAC address. The System MAC address value is calculated as <code>Ma0-MAC-addr + 1</code>.</p> <p>A default ipv4 route is also created with a next-hop of the management network to allow for outgoing connections.</p>","boost":4},{"location":"manual/kinds/ceos/#mgmt-vrf","title":"MGMT VRF","text":"<p>The default empty configuration supports placing the management interface into a VRF to isolate it from the main device routing table.  Passing the environment variable <code>CLAB_MGMT_VRF</code> in either the kind or node definition will activate this behavior, and alter the management services configuration to also reflect the management VRF.  You can duplicate this when using the <code>startup-config</code> by starting from the linked template below.</p> <pre><code># example topo file with management VRF\n# node1 will have vrf MGMT\n# node2 will have vrf FOO\nname: ceos_vrf\ntopology:\n  kinds:\n    ceos:\n      env:\n        CLAB_MGMT_VRF: MGMT\n  nodes:\n    node1:\n      kind: ceos\n    node2:\n      kind: ceos\n      env:\n        CLAB_MGMT_VRF: FOO\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#user-defined-config","title":"User defined config","text":"<p>It is possible to make ceos nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup config:</p> <pre><code>name: ceos_lab\ntopology:\n  nodes:\n    ceos:\n      kind: ceos\n      startup-config: myconfig.conf\n</code></pre> <p>When a config file is passed via <code>startup-config</code> parameter it will be used during an initial lab deployment. However, a config file that might be in the lab directory of a node takes precedence over the startup-config<sup>2</sup>.</p> <p>With such topology file containerlab is instructed to take a file <code>myconfig.conf</code> from the current working directory, copy it to the lab directory for that specific node under the <code>/flash/startup-config</code> name and mount that dir to the container. This will result in this config to act as a startup config for the node.</p> <p>It is possible to change the default config which every ceos node will start with with the following steps:</p> <ol> <li>Craft a valid startup configuration file<sup>1</sup>.</li> <li> <p>Use this file as a startup-config for ceos kind:</p> <pre><code>name: ceos\n\ntopology:\n  kinds:\n    ceos:\n    startup-config: ceos-custom-startup.cfg\n  nodes:\n    # ceos1 will boot with ceos-custom-startup.cfg as set in the kind parameters\n    ceos1:\n      kind: ceos\n      image: ceos:4.32.0F\n    # ceos2 will boot with its own specific startup config, as it overrides the kind variables\n    ceos2:\n      kind: ceos\n      image: ceos:4.32.0F\n      startup-config: node-specific-startup.cfg\n  links:\n    - endpoints: [\"ceos1:eth1\", \"ceos2:eth1\"]\n</code></pre> </li> </ol>","boost":4},{"location":"manual/kinds/ceos/#saving-configuration","title":"Saving configuration","text":"<p>In addition to cli commands such as <code>write memory</code> user can take advantage of the <code>containerlab save</code> command. It saves running cEOS configuration into a startup config file effectively calling the <code>write</code> CLI command.</p>","boost":4},{"location":"manual/kinds/ceos/#container-configuration","title":"Container configuration","text":"<p>To start an Arista cEOS node containerlab uses the following configuration:</p> Startup commandEnvironment variables <p><code>/sbin/init systemd.setenv=INTFTYPE=eth systemd.setenv=ETBA=1 systemd.setenv=SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT=1 systemd.setenv=CEOS=1 systemd.setenv=EOS_PLATFORM=ceoslab systemd.setenv=container=docker systemd.setenv=MAPETH0=1 systemd.setenv=MGMT_INTF=eth0</code></p> <p><code>CEOS:1</code> <code>EOS_PLATFORM\":ceoslab</code> <code>container:docker</code> <code>ETBA:1</code> <code>SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT:1</code> <code>INTFTYPE:eth</code> <code>MAPETH0:1</code> <code>MGMT_INTF:eth0</code></p>","boost":4},{"location":"manual/kinds/ceos/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For <code>ceos</code> kind containerlab creates <code>flash</code> directory for each ceos node and mounts these folders by <code>/mnt/flash</code> paths.</p> <pre><code>\u276f tree clab-srlceos01/ceos\nclab-srlceos01/ceos\n\u2514\u2500\u2500 flash\n    \u251c\u2500\u2500 AsuFastPktTransmit.log\n    \u251c\u2500\u2500 debug\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 proc\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 modules\n    \u251c\u2500\u2500 fastpkttx.backup\n    \u251c\u2500\u2500 Fossil\n    \u251c\u2500\u2500 kickstart-config\n    \u251c\u2500\u2500 persist\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 local\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 messages\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 persistentRestartLog\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 secure\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 sys\n    \u251c\u2500\u2500 schedule\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tech-support\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 ceos_tech-support_2021-01-14.0907.log.gz\n    \u251c\u2500\u2500 SsuRestoreLegacy.log\n    \u251c\u2500\u2500 SsuRestore.log\n    \u251c\u2500\u2500 system_mac_address\n    \u2514\u2500\u2500 startup-config\n\n9 directories, 11 files\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#copy-to-flash","title":"Copy to <code>flash</code>","text":"<p>If there is a need to copy ceos-specific configuration or override files to the ceos node in the topology use <code>.extras.ceos-copy-to-flash</code> config option. These files will be copied to the node's flash directory and evaluated on startup.</p> <pre><code>name: ceos\ntopology:\n  nodes:\n    ceos1:\n      kind: ceos\n      ...\n      extras:\n        ceos-copy-to-flash:\n        - ceos-config # (1)!\n        - toggle_override\n</code></pre> <ol> <li>Paths are relative to the topology file. Absolute paths like <code>~/some/path</code> or <code>/some/path</code> are also possible.</li> </ol>","boost":4},{"location":"manual/kinds/ceos/#lab-examples","title":"Lab examples","text":"<p>The following labs feature a cEOS node:</p> <ul> <li>SR Linux and cEOS</li> </ul>","boost":4},{"location":"manual/kinds/ceos/#known-issues-or-limitations","title":"Known issues or limitations","text":"","boost":4},{"location":"manual/kinds/ceos/#cgroups-v1","title":"cgroups v1","text":"<p>In versions prior to EOS-4.32.0F, the ceos-lab image requires a cgroups v1 environment. For many users, this should not require any changes to the runtime environment. However, some Linux distributions (ref: #467) may be configured to use cgroups v2 out-of-the-box<sup>3</sup>, which will prevent ceos-lab image from booting. In such cases, the users will need to configure their system to utilize a cgroups v1 environment.</p> <p>Consult your distribution's documentation for details regarding configuring cgroups v1 in case you see similar startup issues as indicated in #467.</p> <p>Starting with EOS-4.32.0F, ceos-lab will automatically determine whether the container host is using cgroups v1 or cgroups v2 and act appropriately. No configuration is required.</p> Switching to cgroup v1 in Ubuntu 21.04 <p>To switch back to cgroup v1 in Ubuntu 21+ users need to add a kernel parameter <code>systemd.unified_cgroup_hierarchy=0</code> to GRUB config. Below is a snippet of <code>/etc/default/grub</code> file with the added <code>systemd.unified_cgroup_hierarchy=0</code> parameter.</p> <p>Note that <code>sudo update-grub</code> is needed once changes are made to the file.</p> <pre><code># If you change this file, run 'update-grub' afterwards to update\n# /boot/grub/grub.cfg.\n# For full documentation of the options in this file, see:\n#   info -f grub -n 'Simple configuration'\n\nGRUB_DEFAULT=0\nGRUB_TIMEOUT_STYLE=hidden\nGRUB_TIMEOUT=0\nGRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`\nGRUB_CMDLINE_LINUX_DEFAULT=\"transparent_hugepage=never quiet splash systemd.unified_cgroup_hierarchy=0\"\nGRUB_CMDLINE_LINUX=\"\"\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#wsl","title":"WSL","text":"<p>When running under WSL2 ceos datapath might appear not working. As of Feb 2022 users would need to manually enter the following iptables rules inside ceos container:</p> <pre><code>sudo iptables -P INPUT ACCEPT\nsudo ip6tables -P INPUT ACCEPT\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#scale","title":"Scale","text":"<p>From version 4.32.0F, the ceos-lab image supports up to 50 nodes per host. On previous releases and/or with higher scale there might be issues cores inside the ceos-lab nodes and errors like <code>Error: Too many open files</code>.</p> <p>Example solution for 60 ceos-lab nodes:</p> <ol> <li>On the host run:</li> </ol> <pre><code>sudo sh -c 'echo \"fs.inotify.max_user_instances = 75000\" &gt; /etc/sysctl.d/99-zceoslab.conf'\nsudo sysctl --load /etc/sysctl.d/99-zceoslab.conf\n</code></pre> <p>where 75000 is <code>60 (# of nodes) * 1250</code>.</p> <ol> <li>Bind newly created file into the ceos-lab containers:</li> </ol> <pre><code>...\ntopology:\n  kinds:\n    ceos:\n      ...\n      binds:\n        - /etc/sysctl.d/99-zceoslab.conf:/etc/sysctl.d/99-zceoslab.conf:ro\n...\n</code></pre> <ol> <li> <p>feel free to omit the IP addressing for Management interface, as it will be configured by containerlab when ceos node boots.\u00a0\u21a9</p> </li> <li> <p>if startup config needs to be enforced, either deploy a lab with <code>--reconfigure</code> flag, or use <code>enforce-startup-config</code> setting.\u00a0\u21a9</p> </li> <li> <p>for example, Ubuntu 21.04 comes with cgroup v2 by default.\u00a0\u21a9</p> </li> <li> <p>interface name can also be <code>et</code> instead of <code>eth</code>.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/","title":"Check Point Cloudguard","text":"<p>Check Point Cloudguard virtualized security appliance is identified with <code>checkpoint_cloudguard</code> kind in the topology file. It is built using boxen project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/#getting-cloudguard-image","title":"Getting Cloudguard image","text":"<p>Users can obtain the qcow2 disk image for Check Point Cloudguard VM from the official download site. To build a containerlab-compatible container use boxen project.</p>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/#managing-check-point-cloudguard-nodes","title":"Managing Check Point Cloudguard nodes","text":"<p>Note</p> <p>Containers with Check Point Cloudguard VM inside will take ~5min to fully boot. You can monitor the progress with</p> <ul> <li><code>docker logs -f &lt;container-name&gt;</code> for boxen status reports</li> <li>and <code>docker exec -it &lt;container-name&gt; tail -f /console.log</code> to see the boot log messages.</li> </ul> <p>Check Point Cloudguard node launched with containerlab can be managed via the following interfaces:</p> bashCLIHTTPS <p>to connect to a <code>bash</code> shell of a running checkpoint_cloudguard container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>Note</p> <p>The shell access gives you access to the container that hosts the Qemu VM.</p> <p>to connect to the Cloudguard CLI <pre><code>ssh admin@&lt;container-name/id/IP-addr&gt;\n</code></pre></p> <p>Cloudguard OS comes with HTTPS server running on boot. You can access the Web UI using https schema <pre><code>curl https://&lt;container-name/id/IP-addr&gt;\n</code></pre></p> <p>You can expose container's 443 port with <code>ports</code> setting in containerlab and get access to the Web UI using your containerlab host IP.</p> <p>Info</p> <p>Default login credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/#interfaces-mapping","title":"Interfaces mapping","text":"<p>Check Point Cloudgard starts up with 8 available interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Cloudguard node, it assigns a static <code>10.0.0.5</code> IPv4 address to the VM's <code>eth0</code> interface. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cloudguard using containerlab's assigned IP.</p> <p>Data interfaces <code>eth1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/cisco_iol/","title":"Cisco IOL","text":"<p>Cisco IOL (IOS On Linux or IOL for short) is a version of Cisco IOS/IOS-XE software which is packaged as binary, in other words it does not require a virtual machine, hence the name IOS On Linux.</p> <p>It is identified with <code>cisco_iol</code> kind in the topology file and built using vrnetlab project and essentially is the IOL binary packaged into a docker container.</p>","boost":4},{"location":"manual/kinds/cisco_iol/#getting-and-building-cisco-iol","title":"Getting and building Cisco IOL","text":"<p>You can get Cisco IOL from Cisco's CML refplat .iso. It is identified by the <code>iol</code> or <code>ioll2</code> prefix.</p> <p>From the IOL binary you are required to build a container using the vrnetlab project.</p> <p>IOL is distributed as two versions:</p> <ul> <li>IOL - For usage as an L3 router, lacks L2 switching functionality.</li> <li>IOL-L2 - For usage as a virtual version of an IOS-XE switch. Still has support for some L3 features. See usage information.</li> </ul>","boost":4},{"location":"manual/kinds/cisco_iol/#resource-requirements","title":"Resource requirements","text":"<p>Cisco IOL is very light on resources compared to VM-based Cisco products. Each IOL node requires at minimum:</p> <ul> <li>1vCPU per node, you are able oversubscribe and run many IOL nodes per vCPU.</li> <li>768Mb of RAM.</li> <li>1Mb of disk space for the NVRAM (where configuration is saved).</li> </ul> <p>Using KSM you can achieve a higher density of IOL nodes per GB of RAM.</p>","boost":4},{"location":"manual/kinds/cisco_iol/#managing-cisco-iol-nodes","title":"Managing Cisco IOL nodes","text":"<p>You can manage the Cisco IOL with containerlab via the following interfaces:</p> CLIbash <p>to connect to the IOL CLI</p> <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre> <p>to connect to a <code>bash</code> shell of a running IOL container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>Note</p> <p>Default credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/cisco_iol/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in the Cisco IOL CLI.</p> <p>The interface naming convention is: <code>Ethernet0/X</code> (or <code>e0/X</code>), where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>e0/1</code> - First data-plane interface available</li> <li><code>e0/2</code> - Second data-plane interface, and so on...</li> </ul> <p>Keep in mind IOL defines interfaces in groups of 4. Every four interfaces (zero-indexed), the slot index increments by one.</p> <ul> <li><code>e0/3</code> - Third data-plane interface</li> <li><code>e1/0</code> - Fourth data-plane interface</li> <li><code>e1/1</code> - Fifth data-plane interface</li> </ul> <p>The example ports above would be mapped to the following Linux interfaces inside the container running Cisco IOL:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network. Mapped to <code>Ethernet0/0</code>.</li> <li><code>eth1</code> - First data-plane interface. Mapped to <code>Ethernet0/1</code> interface.</li> <li><code>eth2</code> - Second data-plane interface. Mapped to <code>Ethernet0/2</code> interface</li> <li><code>eth3</code> - Third data-plane interface. Mapped to <code>Ethernet0/3</code> interface</li> <li><code>eth4</code> - Fourth data-plane interface. Mapped to <code>Ethernet1/0</code> interface</li> <li><code>eth5</code> - Fifth data-plane interface. Mapped to <code>Ethernet1/1</code> interface and so on...</li> </ul> <p>When containerlab launches Cisco IOL, the <code>Ethernet0/0</code> interface of the container gets assigned management IPv4 and IPv6 addresses from docker. The <code>Ethernet0/0</code> interface is in it's own management VRF so that configuration in the global context will not affect the management interface.</p> <p>Interfaces can be defined in a non-contigous manner in your toplogy file. See the example below.</p> <pre><code>name: my-iol-lab\ntopology:\n  nodes:\n    iol-1:\n      kind: cisco_iol\n      image: vrnetlab/cisco_iol:17.12.01\n    iol-2:\n      kind: cisco_iol\n      image: vrnetlab/cisco_iol:17.12.01\n\n  links:\n    - endpoints: [\"iol-1:Ethernet0/1\",\"iol-2:Ethernet0/1\"] \n    - endpoints: [\"iol-1:Ethernet1/3\", \"iol-2:Ethernet1/0\"]\n</code></pre> <p>Warning</p> <p>When defining interfaces non-contigiously you may see more interfaces than you have defined in the IOL CLI, this is because interfaces are provisioned in groups.</p> <p>At minimum you will see all numerically-lower indexed interfaces in the CLI compared to the interface you have defined, you may also see interfaces with a higher numerical index.</p> <p>Links/interfaces that you did not define in your containerlab topology will not pass any traffic.</p> <p>Data interfaces <code>Ethernet0/1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces and will appear <code>unset</code> in the CLI:</p> <pre><code>iol#sh ip int br\nInterface              IP-Address      OK? Method Status                Protocol\nEthernet0/0            172.20.20.5     YES TFTP   up                    up\nEthernet0/1            unassigned      YES unset  administratively down down\nEthernet0/2            unassigned      YES unset  administratively down down\nEthernet0/3            unassigned      YES unset  administratively down down\n</code></pre>","boost":4},{"location":"manual/kinds/cisco_iol/#startup-configuration","title":"Startup configuration","text":"<p>When Cisco IOL is booted, it will start with a basic configuration which configures the following:</p> <ul> <li>IP addressing for the Ethernet0/0 (management) interface.</li> <li>Management VRF for the Ethernet0/0 interface.</li> <li>Default route(s) in the management VRF context for the management network.</li> <li>SSH server.</li> <li>Sets all user defined interfaces into 'up' state.</li> </ul> <p>On subsequent boots (deployments which are not the first boot of IOL), IOL will take a few extra seconds to come up, this is because Containerlab must update the management interface IP addressing and default routes for the management network.</p>","boost":4},{"location":"manual/kinds/cisco_iol/#user-defined-config","title":"User-defined config","text":"<p>Cisco IOL supports user defined startup configurations in two forms:</p> <ul> <li>Full startup configuration.</li> <li>Partial startup configuration.</li> </ul> <p>Both types of startup configurations are only be applied on the first boot of IOL. When you save configuration in IOL to the NVRAM (using <code>write memory</code> or <code>copy run start</code> commands), the NVRAM configuration will override the startup configuration.</p>","boost":4},{"location":"manual/kinds/cisco_iol/#full-startup-configuration","title":"Full startup configuration","text":"<p>The full startup configuration is used to fully replace/override the default startup configuration that is applied. This means you must define IP addressing and the SSH server in your configuration to access IOL.</p> <p>You can use the template variables that are defined in the default startup confguration. On lab deployment the template variables will be replaced/substituted.</p> <pre><code>name: iol_full_startup_cfg\ntopology:\n  nodes:\n    sros:\n      kind: cisco_iol\n      startup-config: configuration.txt\n</code></pre>","boost":4},{"location":"manual/kinds/cisco_iol/#partial-startup-configuration","title":"Partial  startup configuration","text":"<p>The partial startup configuration is appended to the default startup configuration. This is useful to preconfigure certain things like loopback interfaces or IGP, while also taking advantage of the startup configuration that containerlab applies by default for management interface IP addressing and SSH access.</p> <p>The partial startup configuration must contain <code>.partial</code> in the filename. For example: <code>config.partial.txt</code> or <code>config.partial</code></p> <pre><code>name: iol_partial_startup_cfg\ntopology:\n  nodes:\n    sros:\n      kind: cisco_iol\n      startup-config: configuration.txt.partial\n</code></pre>","boost":4},{"location":"manual/kinds/cisco_iol/#usage-and-sample-topology","title":"Usage and sample topology","text":"<p>IOL-L2 requires a different startup configuration compared to the regular IOL. You can tell containerlab you are using the L2 image by supplying the <code>type</code> field in your topology.</p> <p>See the sample topology below</p> <pre><code>name: iol\ntopology:\n  nodes:\n    router1:\n      kind: cisco_iol\n      image: vrnetlab/cisco_iol:17.12.01\n    router2:\n      kind: cisco_iol\n      image: vrnetlab/cisco_iol:17.12.01\n    switch:\n      kind: cisco_iol\n      image: vrnetlab/cisco_iol:L2-17.12.01\n      type: L2\n  links:\n    - endpoints: [\"router1:Ethernet0/1\",\"switch:Ethernet0/1\"]\n    - endpoints: [\"router2:Ethernet0/1\",\"switch:e0/2\"]\n</code></pre>","boost":4},{"location":"manual/kinds/crpd/","title":"Juniper cRPD","text":"<p>Juniper cRPD is identified with <code>crpd</code> or <code>juniper_crpd</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a <code>crpd</code> node.</p> <p>cRPD nodes launched with containerlab comes up pre-provisioned with SSH service enabled, <code>root</code> user created and NETCONF enabled.</p>","boost":4},{"location":"manual/kinds/crpd/#managing-crpd-nodes","title":"Managing cRPD nodes","text":"<p>Juniper cRPD node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONF <p>to connect to a <code>bash</code> shell of a running cRPD container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the cRPD CLI <pre><code>docker exec -it &lt;container-name/id&gt; cli\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh root@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>root:clab123</code></p>","boost":4},{"location":"manual/kinds/crpd/#interfaces-mapping","title":"Interfaces mapping","text":"<p>cRPD container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface</li> </ul> <p>When containerlab launches cRPD node, it will assign IPv4/6 address to the <code>eth0</code> interface. Data interface <code>eth1</code> needs to be configured with IP addressing manually.</p> cRPD interfaces output <p>This output demonstrates the IP addressing of the linux interfaces of cRPD node. <pre><code>\u276f docker exec -it clab-crpd-crpd bash\n\n===&gt;\n        Containerized Routing Protocols Daemon (CRPD)\nCopyright (C) 2020, Juniper Networks, Inc. All rights reserved.\n                                                                    &lt;===\n\nroot@crpd:/# ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n    valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n    valid_lft forever preferred_lft forever\n\n&lt;SNIP&gt;\n\n5767: eth0@if5768: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default\n    link/ether 02:42:ac:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 172.20.20.3/24 brd 172.20.20.255 scope global eth0\n    valid_lft forever preferred_lft forever\n    inet6 3fff:172:20:20::3/80 scope global nodad\n    valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe14:1403/64 scope link\n    valid_lft forever preferred_lft forever\n5770: eth1@if5769: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether b6:d3:63:f1:cb:7b brd ff:ff:ff:ff:ff:ff link-netnsid 1\n    inet6 fe80::b4d3:63ff:fef1:cb7b/64 scope link\n    valid_lft forever preferred_lft forever\n</code></pre> This output shows how the linux interfaces are mapped into the cRPD OS. <pre><code>root@crpd&gt; show interfaces routing\nInterface        State Addresses\nlsi              Up\ntunl0            Up    ISO   enabled\nsit0             Up    ISO   enabled\n                    INET6 ::172.20.20.3\n                    INET6 ::127.0.0.1\nlo.0             Up    ISO   enabled\n                    INET6 fe80::1\nip6tnl0          Up    ISO   enabled\n                    INET6 fe80::42a:e9ff:fede:a0e3\ngretap0          Down  ISO   enabled\ngre0             Up    ISO   enabled\neth1             Up    ISO   enabled\n                    INET6 fe80::b4d3:63ff:fef1:cb7b\neth0             Up    ISO   enabled\n                    INET  172.20.20.3\n                    INET6 3fff:172:20:20::3\n                    INET6 fe80::42:acff:fe14:1403\n</code></pre> As you see, the management interface <code>eth0</code> inherits the IP address that docker assigned to cRPD container.</p>","boost":4},{"location":"manual/kinds/crpd/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/crpd/#node-configuration","title":"Node configuration","text":"<p>cRPD nodes have a dedicated <code>config</code> directory that is used to persist the configuration of the node. It is possible to launch nodes of <code>crpd</code> kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead.</p>","boost":4},{"location":"manual/kinds/crpd/#default-node-configuration","title":"Default node configuration","text":"<p>When a node is defined without <code>config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p> <pre><code># example of a topo file that does not define a custom config\n# as a result, the config will be generated from a template\n# and used by this node\nname: crpd\ntopology:\n  nodes:\n    crpd:\n      kind: crpd\n</code></pre> <p>The generated config will be saved by the path <code>clab-&lt;lab_name&gt;/&lt;node-name&gt;/config/juniper.conf</code>. Using the example topology presented above, the exact path to the config will be <code>clab-crpd/crpd/config/juniper.conf</code>.</p>","boost":4},{"location":"manual/kinds/crpd/#user-defined-config","title":"User defined config","text":"<p>It is possible to make cRPD nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property of the node/kind a user sets the path to the config file that will be mounted to a container:</p> <pre><code>name: crpd_lab\ntopology:\n  nodes:\n    crpd:\n      kind: crpd\n      startup-config: myconfig.conf\n</code></pre> <p>With such topology file containerlab is instructed to take a file <code>myconfig.conf</code> from the current working directory, copy it to the lab directory for that specific node under the <code>/config/juniper.conf</code> name and mount that dir to the container. This will result in this config to act as a startup config for the node.</p>","boost":4},{"location":"manual/kinds/crpd/#saving-configuration","title":"Saving configuration","text":"<p>With <code>containerlab save</code> command it's possible to save running cRPD configuration into a file. The configuration will be saved by <code>/config/juniper.conf</code> path in the relevant node directory.</p>","boost":4},{"location":"manual/kinds/crpd/#license","title":"License","text":"<p>cRPD containers require a license file to have some features to be activated. With a <code>license</code> directive it's possible to provide a path to a license file that will be copied over to the nodes configuration directory by the <code>/config/license/safenet/junos_sfnt.lic</code> path and will get applied automatically on boot.</p>","boost":4},{"location":"manual/kinds/crpd/#container-configuration","title":"Container configuration","text":"<p>To launch cRPD, containerlab uses the deployment instructions that are provided in the TechLibrary as well as leveraging some setup steps outlined by Matt Oswalt in this blog post.</p> <p>The SSH service is already enabled for root login, so nothing is needed to be done additionally.</p> <p>The <code>root</code> user is created already with the <code>clab123</code> password.</p>","boost":4},{"location":"manual/kinds/crpd/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For <code>crpd</code> kind containerlab creates <code>config</code> and <code>log</code> directories for each crpd node and mounts these folders by <code>/config</code> and <code>/var/log</code> paths accordingly.</p> <pre><code>\u276f tree clab-crpd/crpd\nclab-crpd/crpd\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 juniper.conf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 license\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 safenet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sshd_config\n\u2514\u2500\u2500 log\n    \u251c\u2500\u2500 cscript.log\n    \u251c\u2500\u2500 license\n    \u251c\u2500\u2500 messages\n    \u251c\u2500\u2500 mgd-api\n    \u251c\u2500\u2500 na-grpcd\n    \u251c\u2500\u2500 __policy_names_rpdc__\n    \u2514\u2500\u2500 __policy_names_rpdn__\n\n4 directories, 9 files\n</code></pre>","boost":4},{"location":"manual/kinds/crpd/#lab-examples","title":"Lab examples","text":"<p>The following labs feature cRPD node:</p> <ul> <li>SR Linux and cRPD</li> </ul>","boost":4},{"location":"manual/kinds/cvx/","title":"Cumulus VX","text":"<p>Cumulus VX is identified with <code>cvx</code> or <code>cumulus_cvx</code> kind in the topology file. The <code>cvx</code> kind defines a supported feature set and a startup procedure of a <code>cvx</code> node.</p> <p>CVX nodes launched with containerlab come up with:</p> <ul> <li>the management interface <code>eth0</code> is configured with IPv4/6 addresses as assigned by either the container runtime or DHCP</li> <li><code>root</code> user created with password <code>root</code></li> </ul>","boost":4},{"location":"manual/kinds/cvx/#mode-of-operation","title":"Mode of operation","text":"<p>CVX supports two modes of operation:</p> <ul> <li>Using only the container runtime -- this mode runs Cumulus VX container image directly inside the container runtime (e.g. Docker). Due to the lack of Cumulus VX kernel modules, some features are not supported, most notable one being MLAG. In order to use this mode, add <code>runtime: docker</code> under the cvx node definition (see also this example).</li> <li> <p>Using Firecracker micro-VMs -- this mode runs Cumulus VX inside a micro-VM on top of the native Cumulus kernel. This mode uses <code>ignite</code> runtime and is the default way of running CVX nodes.</p> <p>Warning</p> <p>This mode was broken in containerlab between v0.27.1 and v0.32.1 due to dependencies issues in ignite<sup>2</sup>.</p> </li> </ul> <p>Note</p> <p>When running in the default <code>ignite</code> runtime mode, the only host OS dependency is <code>/dev/kvm</code><sup>1</sup> required to support hardware-assisted virtualisation. Firecracker VMs are spun up inside a special \"sandbox\" container that has all the right tools and dependencies required to run micro-VMs.</p> <p>Additionally, containerlab creates a number of directories under <code>/var/lib/firecracker</code> for nodes running in <code>ignite</code> runtime to store runtime metadata; these directories are managed by containerlab.</p>","boost":4},{"location":"manual/kinds/cvx/#managing-cvx-nodes","title":"Managing cvx nodes","text":"<p>Cumulus VX node launched with containerlab can be managed via the following interfaces:</p> bashSSHgNMI <p>to attach to a <code>bash</code> shell of a running cvx container (only container ID is supported): <pre><code>docker attach &lt;container-id&gt;\n</code></pre> Use Docker's detach sequence (Ctrl+P+Q) to disconnect.</p> <p>SSH server is running on port 22 <pre><code>ssh root@&lt;container-name&gt;\n</code></pre></p> <p>gNMI server will be added in future releases.</p> <p>Info</p> <p>Default user credentials: <code>root:root</code></p>","boost":4},{"location":"manual/kinds/cvx/#user-defined-config","title":"User-defined config","text":"<p>It is possible to make cvx nodes to boot up with a user-defined config by passing any number of files along with their desired mount path:</p> <pre><code>name: cvx_lab\ntopology:\n  nodes:\n    cvx:\n      kind: cvx\n      binds:\n        - cvx/interfaces:/etc/network/interfaces\n        - cvx/daemons:/etc/frr/daemons\n        - cvx/frr.conf:/etc/frr/frr.conf\n</code></pre>","boost":4},{"location":"manual/kinds/cvx/#configuration-persistency","title":"Configuration persistency","text":"<p>When running inside the <code>ignite</code> runtime, all mount binds work one way -- from host OS to the cvx node, but not the other way around. Currently, it's up to a user to manually update individual files if configuration updates need to be persisted. This will be addressed in the future releases.</p>","boost":4},{"location":"manual/kinds/cvx/#lab-examples","title":"Lab examples","text":"<p>The following labs feature CVX node:</p> <ul> <li>Cumulus and FRR</li> <li>Cumulus in Docker runtime and Host</li> <li>Cumulus Linux Test Drive</li> <li>EVPN with MLAG and multi-homing scenarios</li> </ul>","boost":4},{"location":"manual/kinds/cvx/#known-issues-or-limitations","title":"Known issues or limitations","text":"<ul> <li>CVX in Ignite is always attached to the default docker bridge network</li> </ul> <ol> <li> <p>this device is already part of the linux kernel, therefore this can be read as \"no external dependencies are needed for running cvx with <code>ignite</code> runtime\".\u00a0\u21a9</p> </li> <li> <p>see https://github.com/srl-labs/containerlab/pull/1037 and https://github.com/srl-labs/containerlab/issues/1039 \u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/dell_sonic/","title":"Dell Enterprise SONiC","text":"<p>Dell Enterprise SONiC VM is identified with <code>dell_sonic</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/dell_sonic/#managing-dell-sonic-nodes","title":"Managing Dell SONiC nodes","text":"<p>Dell SONiC node launched with containerlab can be managed via the following interfaces:</p> <p>Note</p> <ol> <li> <p>Dell SONiC node will take ~2min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> </li> <li> <p>Default credentials are <code>admin:admin</code></p> </li> </ol> SSHTelnet <p>To open a linux shell simply type in</p> <pre><code>ssh &lt;node-name&gt;\n</code></pre> <p>You will enter the bash shell of the VM:</p> <pre><code>\u276f ssh &lt;node name&gt;\nDebian GNU/Linux 10\nadmin@clab-dell_sonic-ds's password: \nLinux ds 5.10.0-21-amd64 #1 SMP Debian 5.10.162-1 (2023-01-21) x86_64\nYou are on\n  ____   ___  _   _ _  ____\n / ___| / _ \\| \\ | (_)/ ___|\n \\___ \\| | | |  \\| | | |\n  ___) | |_| | |\\  | | |___\n |____/ \\___/|_| \\_|_|\\____|\n\n-- Software for Open Networking in the Cloud --\n\nUnauthorized access and/or use are prohibited.\nAll access and/or use are subject to monitoring.\n\nHelp:    http://azure.github.io/SONiC/\nadmin@sonic:~$\n</code></pre> <p>From within the Linux shell users can perform system configuration using linux utilities, or connect to the SONiC CLI using <code>vtysh</code> command.</p> <pre><code>admin@sonic:~$ vtysh\n\nHello, this is FRRouting (version 8.2.2).\nCopyright 1996-2005 Kunihiro Ishiguro, et al.\n\nsonic#\n</code></pre> <p>to connect to sonic-vm CLI via telnet</p> <pre><code>telnet &lt;container-name/id&gt; 5000\n</code></pre>","boost":4},{"location":"manual/kinds/dell_sonic/#interfaces-mapping","title":"Interfaces mapping","text":"<p>Dell SONiC container uses the following mapping rules for its interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data (front-panel port) interface that is mapped to Ethernet0 port</li> <li><code>eth2</code> - second data interface that is mapped to Ethernet4 port. Any new port will result in a \"previous interface + 4\" (Ethernet4) mapping.</li> </ul> <p>When containerlab launches sonic-vs node, it will assign IPv4/6 address to the <code>eth0</code> interface. Data interface <code>eth1</code> mapped to <code>Ethernet0</code> port and needs to be configured with IP addressing manually.</p>","boost":4},{"location":"manual/kinds/dell_sonic/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/dell_sonic/#startup-configuration","title":"Startup configuration","text":"<p>VM-based Dell SONiC supports the <code>startup-config</code> feature. The startup configuration must be provided in a form of a json file extracted from the VM's <code>/etc/sonic/config_db.json</code> path. Consequently, the startup config must be provided in full, partial configuration is not supported.</p> <p>When the startup config is provided, the default containerlab config is overridden and the startup config is used instead. The user-provided startup config file is copied over to the VM's <code>/etc/sonic/config_db.json</code> path and <code>sudo config load -y</code> command is executed to apply it.</p>","boost":4},{"location":"manual/kinds/dell_sonic/#saving-configuration","title":"Saving configuration","text":"<p>Extracting the config from a running node is possible with the <code>containerlab save</code> command. The config will be available in the lab directory under the node's subdirectory.</p>","boost":4},{"location":"manual/kinds/ext-container/","title":"External Container","text":"<p>Regular containerlab-managed nodes can be connected to externally managed containers. For instance, users may want to connect Network OS nodes launched by containerlab to some containers that are managed by other container orchestration tools to create advanced topologies.</p> <p>This connectivity option is enabled by adding nodes of <code>ext-container</code> kind to the topology.</p>","boost":4},{"location":"manual/kinds/ext-container/#using-ext-container-kind","title":"Using <code>ext-container</code> kind","text":"<p>Containerlab doesn't create nodes of type <code>ext-container</code>, it uses those nodes to let users create links to those externally managed containers from the nodes scheduled by containerlab.</p> <p>The below topology demonstrates how the node named <code>srl</code> created by containerlab can be connected to the external container named <code>external-node1</code> that is created by some other tool.</p> <pre><code>name: ext-cont\n\ntopology:\n  nodes:\n    srl:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    external-node1: #(1)!\n      kind: ext-container\n\n  links:\n    - endpoints: [\"srl:e1-1\", \"external-node1:eth1\"]\n</code></pre> <ol> <li>The name of the node of <code>ext-container</code> kind should match the container name as displayed by the <code>docker ps</code> command.</li> </ol> <p>By specifying the node <code>external-node1</code> as part of the containerlab topology, users can use this node name in the links section of the file and create links between containerlab-managed and externally-managed nodes.</p>","boost":4},{"location":"manual/kinds/ext-container/#interacting-with-external-container-nodes","title":"Interacting with External Container nodes","text":"<p>Even though External Container nodes are not scheduled by containerlab, it is possible to configure or interact with them using containerlab topology definition options.</p> <p>For example, when deploying containerlab topology, users can execute commands in the external containers using <code>exec</code> configuration option:</p> <pre><code>topology:\n  nodes:\n    srl:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    external-node1: #(1)!\n      kind: ext-container\n      exec:\n        - ip address add 192.168.0.1/24 dev eth1\n</code></pre> <ol> <li><code>external-node1</code> is the name of a container launched outside of containerlab. In the case of a Docker runtime, this is a name displayed by <code>docker ps</code> command.</li> </ol>","boost":4},{"location":"manual/kinds/fortinet_fortigate/","title":"Fortinet Fortigate","text":"<p>Fortinet Fortigate virtualized security appliance is identified with the <code>fortinet_fortigate</code> kind in the topology file. It is built using the hellt/vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>The integration of Fortinet Fortigate has been tested with v7.0.14 release. Note, that releases &gt;= 7.2.0 would require a valid license and internet access to activate the Fortinet Fortigate VM.</p>","boost":4},{"location":"manual/kinds/fortinet_fortigate/#getting-fortinet-fortigate-disk-image","title":"Getting Fortinet Fortigate disk image","text":"<p>Users can obtain the qcow2 disk image for Fortinet Fortigate VM from the official support site; a free account required. Download the \"New deployment\" variant of the FGVM64 VM for the KVM platform.</p> <p>Extract the downloaded zip file and rename the <code>fortios.qcow2</code> to <code>fortios-vX.Y.Z.qcow2</code> where <code>X.Y.Z</code> is the version of the Fortigate VM. Put the renamed file in the <code>fortigate</code> directory of the cloned hellt/vrnetlab project and run <code>make</code> to build the container image.</p>","boost":4},{"location":"manual/kinds/fortinet_fortigate/#managing-fortinet-fortigate-nodes","title":"Managing Fortinet Fortigate nodes","text":"<p>Note</p> <p>Containers with Fortinet Fortigate VM inside will take ~2min to fully boot. You can monitor the progress with the <code>docker logs -f &lt;container-name&gt;</code> command.</p> <p>Fortinet Fortigate node launched with containerlab can be managed via the following interfaces:</p> bashCLIWeb UI (HTTP) <p>to connect to a <code>bash</code> shell of a running fortigate container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>to connect to the Fortigate CLI</p> <pre><code>ssh admin@&lt;container-name/id/IP-addr&gt;\n</code></pre> <p>Fortigate VM comes with HTTP(S) server with a GUI manager app. You can access the Web UI using http schema.</p> <pre><code>http://&lt;container-name/id/IP-addr&gt;\n</code></pre> <p>You can expose container's port 80 with the <code>ports</code> setting in containerlab and get access to the Web UI using your containerlab host IP.</p> <p>Note</p> <p>Default login credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/fortinet_fortigate/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Fortinet Fortigate.</p> <p>The interface naming convention is: <code>portX</code>, where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>port2</code> - first data port available</li> <li><code>port3</code> - second data port, and so on...</li> </ul> <p>Warning</p> <p>Data port numbering starts at <code>2</code>, as <code>port1</code> is reserved for management connectivity. Attempting to use <code>port1</code> in a containerlab topology will result in an error.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Fortinet Fortigate VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>port1</code> in the CLI)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>port2</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>port3</code> and so on)</li> </ul> <p>When containerlab launches Fortinet Fortigate node the <code>port1</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Fortinet Fortigate using containerlab's assigned IP.</p> <p>Data interfaces <code>port2+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/freebsd/","title":"FreeBSD","text":"<p>FreeBSD is identified with <code>freebsd</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/freebsd/#getting-freebsd-image","title":"Getting FreeBSD image","text":"<p>To build FreeBSD docker container image you will need to download a custom-built <code>qcow2</code> VM image with pre-installed cloud-init from https://bsd-cloud-image.org/.</p> <p>If, for some reason, you're unable to obtain an image from https://bsd-cloud-image.org/, you can build it yourself with the script from this repository.</p>","boost":4},{"location":"manual/kinds/freebsd/#managing-freebsd-nodes","title":"Managing FreeBSD nodes","text":"<p>Note</p> <p>Containers with FreeBSD inside will take ~1-2 min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>FreeBSD node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHTelnet <p>to connect to a <code>bash</code> shell of a running FreeBSD container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the FreeBSD shell (password <code>admin</code>) <pre><code>ssh admin@&lt;container-name&gt;\n</code></pre></p> <p>serial port (console) is exposed over TCP port 5000: <pre><code># from container host\ntelnet &lt;container-name&gt; 5000\n</code></pre> You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/freebsd/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in FreeBSD.</p> <p>The interface naming convention is: <code>vtnetX</code>, where <code>X</code> denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>vtnet1</code> - first data port available</li> <li><code>vtnet2</code> - second data port, and so on...</li> </ul> <p>Warning</p> <p>Data port numbering starts at <code>1</code>, as <code>vtnet0</code> is reserved for management connectivity. Attempting to use <code>vtnet0</code> in a containerlab topology will result in an error.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the FreeBSD VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>vtnet0</code> in the CLI)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>vtnet1</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>vtnet2</code> and so on)</li> </ul> <p>When containerlab launches FreeBSD node the <code>vtnet0</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the FreeBSD using containerlab's assigned IP.</p> <p>Data interfaces <code>vtnet1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/freebsd/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/freebsd/#node-configuration","title":"Node configuration","text":"<p>FreeBSD nodes come up with a basic configuration where only the management interface and a default user are provisioned.</p>","boost":4},{"location":"manual/kinds/freebsd/#configuration-save","title":"Configuration save","text":"<p>Containerlab's <code>save</code> command will perform a configuration backup for <code>FreeBSD</code> nodes via SCP. The entire <code>/etc</code> directory of each node will be archived and saved under <code>backup.tar.gz</code> file and can be found at the node's directory inside the lab parent directory:</p> <pre><code># assuming the lab name is \"freebsd01\"\n# and node name is \"fbsd1\"\nls clab-freebsd01/fbsd1/config/\nbackup.tar.gz\n</code></pre> <p>If the backup file is present upon the node's boot, it will be transferred to the node and extracted. The node will then reboot to apply the restored configuration.</p>","boost":4},{"location":"manual/kinds/freebsd/#lab-examples","title":"Lab examples","text":"<p>The following simple lab consists of two Linux hosts connected via one FreeBSD host:</p> <ul> <li>FreeBSD</li> </ul>","boost":4},{"location":"manual/kinds/generic_vm/","title":"Generic VM","text":"<p>Generic VM is identified with <code>generic_vm</code> kind in the topology file. It is built using vrnetlab project and offers containerlab users to launch arbitrary VMs that are packaged in a container using vrnetlab.</p> <p>A typical use case for this kind is to launch a regular Linux VM such as Ubuntu, AlmaLinux, Redhat, etc. The term generic here means that containerlab does not provide any specific configuration for the VM, it just launches the VM and it is up to a user to confiugre it further.</p>","boost":4},{"location":"manual/kinds/generic_vm/#generic-vm-images","title":"Generic VM images","text":"<p>To build a docker container for a generic VM you will need to download a <code>qcow2</code> VM image for your distribution.</p> <p>For Ubuntu images use hellt/vrnetlab/ubuntu repository and the associated build instructions.</p>","boost":4},{"location":"manual/kinds/generic_vm/#managing-linux-vm-nodes","title":"Managing Linux VM nodes","text":"<p>Note</p> <p>Boot time depends on a linux distrubutive in use as well as the hardware resources. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Ubuntu 22.04 takes about 1 minute to complete its start up.</p> <p>A Linux VM node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHTelnet <p>to connect to a <code>bash</code> shell of a running linux container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>Connect to the VM Guest via SSH. Default password see below the Credentials.</p> <pre><code>ssh clab@&lt;container-name&gt;\n</code></pre> <p>serial port (console) is exposed over TCP port 5000:</p> <pre><code># from container host\ntelnet &lt;container-name&gt; 5000\n</code></pre> <p>You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p>","boost":4},{"location":"manual/kinds/generic_vm/#credentials","title":"Credentials","text":"<p>Default credentials for the Generic VM nodes are <code>clab:clab@123</code>.</p>","boost":4},{"location":"manual/kinds/generic_vm/#interfaces-mapping","title":"Interfaces mapping","text":"<ul> <li><code>eth0</code> - management interface (maps to <code>enp1s0</code> in the case of Ubuntu) connected to the containerlab management network. Should not be provisioned in the topology file as it is handled by containerlab.</li> <li><code>eth1+</code> - second and subsequent interfaces. Only <code>ethX</code> interface names are allowed (where X &gt; 0). These interfaces must be provided in the topology file's links section.</li> </ul> <p>When containerlab launches a Linux VM node, it will assign IPv4/6 address to the <code>enp1s0</code> (or whatever the name is assigned to the management interface by the OS) interface which connects to the containerlab management network.</p> <p>Data interfaces need to be configured with IP addressing manually using CLI.</p>","boost":4},{"location":"manual/kinds/generic_vm/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/generic_vm/#node-configuration","title":"Node configuration","text":"<p>Linux vm nodes come up with a basic configuration where only the management interface and a default user are provisioned.</p>","boost":4},{"location":"manual/kinds/generic_vm/#dns","title":"DNS","text":"","boost":4},{"location":"manual/kinds/generic_vm/#ubuntu","title":"Ubuntu","text":"<p>The Ubuntu VM node comes with <code>9.9.9.9</code> configured as the DNS resolver. Change it with <code>resolvectl</code> if required.</p>","boost":4},{"location":"manual/kinds/generic_vm/#lab-examples","title":"Lab examples","text":"<p>The following simple lab consists of two vr_linux hosts connected via one cEOS host:</p> <ul> <li>generic_vm</li> </ul>","boost":4},{"location":"manual/kinds/host/","title":"Host","text":"<p>A node of kind <code>host</code> represents the containerlab host the labs are running on. It is a special node that is implicitly used when nodes have links connected to the host - see host links.</p> <p>But there is a use case when users might want to define the node of kind <code>host</code> explicitly in the topology. For example, when some commands need to be executed on the host for the lab to function.</p> <p>In such case, the following topology definition can be used:</p> <pre><code>h1:\n  kind: host\n  exec:\n    - ip link set dev enp0s3 up\n</code></pre> <p>In the above example, the node <code>h1</code> is defined as a node of kind <code>host</code> and the <code>exec</code> option is used to run the command <code>ip link set dev enp0s3 up</code> in the containerlab host. Of course, the command can be any other command that is required for the lab to function.</p>","boost":4},{"location":"manual/kinds/huawei_vrp/","title":"Huawei VRP","text":"<p>Huawei VRP virtualized router is identified with <code>huawei_vrp</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Huawei VRP currently supports Huawei N40e and CE12800 variants, the same kind value - <code>huawei_vrp</code> - is used for both.</p> <p>Huawei VRP nodes launched with containerlab comes up pre-provisioned with SSH, NETCONF services enabled.</p>","boost":4},{"location":"manual/kinds/huawei_vrp/#managing-huawei-vrp-nodes","title":"Managing Huawei VRP nodes","text":"<p>Note</p> <p>Containers with Huawei VRP inside will take ~3min to fully boot without a startup config file. And ~5-7 minute if the startup config file is provided, since a node will undergo a reboot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Huawei VRP node launched with containerlab can be managed via the following interfaces:</p> CLIbashNETCONF <p>to connect to the Huawei VRP CLI</p> <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre> <p>to connect to a <code>bash</code> shell of a running Huawei VRP container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>NETCONF server is running over port 830</p> <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre>","boost":4},{"location":"manual/kinds/huawei_vrp/#credentials","title":"Credentials","text":"<p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/huawei_vrp/#interface-naming","title":"Interface naming","text":"<p>The example ports above would be mapped to the following Linux interfaces inside the container running the Huawei VRP VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>GigabitEthernet0/0/0</code> in the VRP config)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>Ethernet1/0/0</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>Ethernet1/0/1</code> and so on)</li> </ul> <p>When containerlab launches Huawei VRP node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Huawei VRP using containerlab's assigned IP.</p> <p>Data interfaces <code>Ethernet1/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/huawei_vrp/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/huawei_vrp/#node-configuration","title":"Node configuration","text":"<p>Huawei VRP nodes come up with a basic configuration where only <code>admin</code> user and management interfaces such as SSH and NETCONF provisioned.</p>","boost":4},{"location":"manual/kinds/huawei_vrp/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make Huawei VRP nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: huawei_vrp\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain both partial configuration snippets that you desire to add on top of the default config that a node boots up with as well as the full configuration extracted from the VRP.</p> <p>When startup config is provided the node will undergo a reboot cycle after applying the bootstrap config, thus the startup time will be twice as long as the node boots up without a config.</p>","boost":4},{"location":"manual/kinds/ipinfusion-ocnos/","title":"IPInfusion OcNOS","text":"<p>IPInfusion OcNOS virtualized router is identified with <code>ipinfusion_ocnos</code> kind in the topology file. It is built using hellt/vrnetlab and essentially is a Qemu VM packaged in a docker container format.</p> <p>ipinfusion_ocnos nodes launched with containerlab come up pre-provisioned with SSH, and NETCONF services enabled.</p> <p>Warning</p> <p>OcNOS VM disk images need to be altered to support telnet serial access and ethX interfaces name style. This can be done by modifying the grub config file, as shown here.</p>","boost":4},{"location":"manual/kinds/ipinfusion-ocnos/#managing-ipinfusion_ocnos-nodes","title":"Managing ipinfusion_ocnos nodes","text":"<p>Note</p> <p>Containers with OcNOS inside will take ~3min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code> and <code>docker exec -it &lt;container-name&gt; tail -f /console.log</code>.</p> <p>IPInfusion OcNOS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONF <p>to connect to a <code>bash</code> shell of a running ipinfusion_ocnos container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the OcNOS CLI <pre><code>ssh ocnos@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh ocnos@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/ipinfusion-ocnos/#interfaces-mapping","title":"Interfaces mapping","text":"<p>ipinfusion_ocnos container can have up to 144 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of OcNOS line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches ipinfusion_ocnos node, it will assign IPv4 address to the <code>eth0</code> interface. This address can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> need to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/k8s-kind/","title":"Kubernetes in docker (kind) cluster","text":"<p>Since more and more applications (including network management systems and network functions) are being deployed in the k8s clusters, it is important to be able to test the network connectivity between the k8s workloads and the underlay network.</p> <p>Kind is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. By integrating kind clusters via a new kind <code>k8s-kind</code> with containerlab, it is possible to spin-up kind clusters as part of the containerlab topology.</p> <p>This deployment model unlocks the possibility to integrate network underlay created by containerlab with the workloads running in the kind clusters in a single YAML file. The integration between kind clusters and containerlab topology makes it easy to deploy and interconnect k8s clusters and the underlay network.</p>","boost":4},{"location":"manual/kinds/k8s-kind/#using-k8s-kind","title":"Using <code>k8s-kind</code>","text":"<p>Integration between Kind and Containerlab is a mix of two kinds:</p> <ol> <li><code>k8s-kind</code> - to manage the creation of the kind clusters</li> <li><code>ext-container</code> - to allow for the interconnection between the nodes of a kind cluster and the network nodes that are part of the same containerlab topology</li> </ol> <p>The lab depicted below incorporates two kind clusters, one with a control plane and a worker node, and the other with an all-in-one node.</p> <p>By defining the clusters with <code>k8s-kind</code> nodes we let containerlab manage the lifecycle (deployment/destroy) of the kind clusters. But this is not all. We can use the <code>ext-container</code> nodes to define actual kind cluster containers that run the control plane and worker nodes.</p> <p>The name of the <code>ext-container</code> node is known upfront as it is computed as <code>&lt;k8s-kind-node-name&gt;-control-plane</code> for the control plane node and <code>&lt;k8s-kind-node-name&gt;-worker[worker-node-index]</code> for the worker nodes.</p> <p>By defining the <code>ext-container</code> nodes we unlock the possibility to define the links between the kind cluster nodes and the network nodes that are part of the same containerlab topology.</p> <pre><code>name: kind01\n\ntopology:\n  kinds:\n    nokia_srlinux:\n      image: ghcr.io/nokia/srlinux:23.10.1\n  nodes:\n    srl01:\n      kind: nokia_srlinux\n    k01:\n      kind: k8s-kind\n      startup-config: k01-config.yaml\n    k02:\n      kind: k8s-kind\n\n    # k01 cluster contains 2 nodes: a control-plane and a worker\n    # the cluster config is defined in k01-config.yaml\n    k01-control-plane:\n      kind: ext-container\n      exec:\n        - \"ip addr add dev eth1 192.168.10.1/24\"\n    k01-worker:\n      kind: ext-container\n      exec:\n        - \"ip addr add dev eth1 192.168.11.1/24\"\n\n    # k02 cluster has an all-in-one node\n    k02-control-plane:\n      kind: ext-container\n      exec:\n        - \"ip addr add dev eth1 192.168.20.1/24\"\n\n  links:\n    - endpoints: [\"srl01:e1-1\", \"k01-control-plane:eth1\"]\n    - endpoints: [\"srl01:e1-2\", \"k01-worker:eth1\"]\n\n    - endpoints: [\"srl01:e1-4\", \"k02-control-plane:eth1\"]\n</code></pre> <p>This is exactly how you use the integration between containerlab and kind to create a topology that includes kind clusters and network nodes.</p> <p>Once the lab pictured above is deployed, we can see the two clusters created:</p> get clustersk01 nodesk02 nodes <pre><code>\u276f kind get clusters\nk01\nk02\n</code></pre> <pre><code>\u276f kind get nodes --name k01\nk01-worker\nk01-control-plane\n</code></pre> <pre><code>\u276f kind get nodes --name k02\nk02-control-plane\n</code></pre>","boost":4},{"location":"manual/kinds/k8s-kind/#cluster-config","title":"Cluster config","text":"<p>It is possible to provide original kind cluster configuration via <code>startup-configuration</code> parameter of the <code>k8s-kind</code> node. Due to the kind cluster config provided to <code>k01</code> node above, kind will spin up 2 containers, one control-plane and one worker node. <code>k02</code> cluster that doesn't have a <code>startup-configuration</code> defined will spin up a single container with an all-in-one control-plane and a worker node.</p> <p>Contents of <code>k01-config.yaml</code>:</p> <pre><code>apiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n  - role: control-plane\n  - role: worker\n</code></pre>","boost":4},{"location":"manual/kinds/k8s-kind/#cluster-interfaces","title":"Cluster interfaces","text":"<p>When containerlab orchestrates the kind clusters creation it relies on kind API to handle the actual deployment process. When kind creates a cluster it uses a docker network to connect the kind cluster nodes together.</p> <p>In order to connect cluster nodes to the network underlay created by containerlab, we use <code>ext-container</code> kind of nodes per each control-plane and worker node of a cluster and connect them with their <code>eth1+</code> interfaces to the network nodes.</p> <p>Since the <code>eth1+</code> interfaces come up unconfigured, we may configure them using the <code>exec</code> property and set the IP addresses.</p> <p>Given the lab above, we configure <code>eth1</code> interface on all nodes. For example, we can check that worker node of cluster <code>k01</code> got its <code>eth1</code> inteface configured with the IP address:</p> <pre><code>\u276f docker exec -it k01-worker ip a show eth1\n12229: eth1@if12230: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9500 qdisc noqueue state UP group default \n    link/ether aa:c1:ab:7e:22:6f brd ff:ff:ff:ff:ff:ff link-netnsid 1\n    inet 192.168.11.1/24 scope global eth1\n       valid_lft forever preferred_lft forever\n</code></pre>","boost":4},{"location":"manual/kinds/k8s-kind/#node-parameters","title":"Node parameters","text":"<p>With `k8s-kind`` nodes it is possible to use the following configuration parameters:</p> <ul> <li>image - to define the kind container image to use for the kind cluster</li> <li>startup-config - to provide a kind cluster configuration (optional, kind defaults apply otherwise)</li> </ul>","boost":4},{"location":"manual/kinds/k8s-kind/#extra-parameters","title":"Extra parameters","text":"<p>In addition to the generic node parameters, <code>k8s-kind</code> can take following extra parameters from <code>extras</code> field.</p> <pre><code>topology:\n  nodes:\n    kind0:\n      kind: k8s_kind\n      extras:\n        k8s_kind:\n          deploy:\n            # Corresponds to --wait option. Wait given duration until the cluster becomes ready.\n            wait: 0s\n</code></pre>","boost":4},{"location":"manual/kinds/k8s-kind/#known-issues","title":"Known issues","text":"","boost":4},{"location":"manual/kinds/k8s-kind/#duplication-of-nodes-in-the-output","title":"Duplication of nodes in the output","text":"<p>When you deploy a lab with <code>k8s-kind</code> nodes you may notice that the output of the <code>deploy</code> command contains more nodes than you have defined in the topology file. This is a known visual issue that is caused by the fact that <code>k8s-kind</code> nodes are merely a placeholder for a kind cluster configuration, and the actual nodes of the kind cluster are defined by the <code>ext-container</code> nodes.</p>","boost":4},{"location":"manual/kinds/k8s-kind/#inspect-all-command-output","title":"<code>inspect --all</code> command output","text":"<p>When you run <code>clab inspect --all</code> command you may notice that the output doesn't list the <code>k8s-kind</code> nodes nor the <code>ext-container</code> nodes.</p> <p>For now, use <code>clab inspect -t &lt;topology-file&gt;</code> to see the full topology output.</p>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/","title":"Keysight Ixia-c-one","text":"<p>Keysight Ixia-c-one is a single-container distribution of Ixia-c, a software traffic generator and protocol emulator with Open Traffic Generator (OTG) API.</p> <p>What is Ixia-c?</p> <p>Ixia-c is an agile and composable network test system designed for continuous integration. It is provides a modern, powerful and API-driven traffic generator designed to cater to the needs of network operators, vendors and hobbyists alike.</p> <p>Ixia-c Community Edition is available for free with limitations. Commercially licensed editions are also available.</p> <p>Users can pull Ixia-c-one container image from Github Container Registry.</p> <p>The corresponding node in containerlab is identified with <code>keysight_ixia-c-one</code> kind in the topology file. Upon boot up, it comes up with:</p> <ul> <li>management interface <code>eth0</code> configured with IPv4/6 addresses as assigned by the container runtime</li> <li>hostname assigned to the node name</li> <li>HTTPS service enabled on port TCP/8443 (for client SDK to push configuration and fetch metrics)</li> </ul>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#managing-ixia-c-one-nodes","title":"Managing Ixia-c-one nodes","text":"<p>Ixia-c-one provides an API endpoint that manages configuration across multiple test ports. Requests and responses to the API endpoint are defined by the Open Traffic Generator API and can be exercised in the following two ways:</p> Using SDKUsing <code>curl</code> <p>Using SDK is the preferred way of interacting with OTG devices. Implementations listed in the SDK chapter below provide references to SDK clients in different languages along with examples.</p> <p>Test case designers create test cases using SDK in one of the supported languages and leverage native language toolchain to test/execute the tests. Being API-first, Open Traffic Generator compliant implementations provide full configuration flexibility over the API.</p> <p>SDK clients use HTTPS to interface with the OTG API.</p> <pre><code># fetch configuration that was last pushed to ixia-c-one\n# assuming 'clab-ixiac01-ixia-c' is a container name allocated by containerlab for the Ixia-c node\ncurl -kL https://clab-ixiac01-ixia-c:8443/config\n\n# fetch flow metrics\ncurl -kL https://clab-ixiac01-ixia-c:8443/monitor/metrics -d '{\"choice\": \"flow\"}'\n</code></pre>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#sdk","title":"SDK","text":"<p>Client SDK for Open Traffic Generator API is available in various languages, most prevalent being gosnappi for Go and snappi for Python.</p>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#lab-examples","title":"Lab examples","text":"<p>The following labs feature Keysight ixia-c-one node:</p> <ul> <li>Keysight Ixia-c and Nokia SR Linux</li> </ul>","boost":4},{"location":"manual/kinds/linux/","title":"Linux container","text":"","boost":4},{"location":"manual/kinds/linux/#linux-container","title":"Linux container","text":"<p>Labs deployed with containerlab are endlessly flexible, mostly because containerlab can spin up and wire regular containers as part of the lab topology.</p> <p>Nowadays more and more workloads are packaged into containers, and containerlab users can nicely integrate them in their labs following a familiar docker' compose-like syntax. As long as the networking domain is considered, the most common use case for bare linux containers is to introduce \"clients\" or traffic generators which are connected to the network nodes or host telemetry/monitoring stacks.</p> <p>But, of course, you are free to choose which container to add into your lab, there is not restriction to that!</p>","boost":4},{"location":"manual/kinds/linux/#using-linux-containers","title":"Using linux containers","text":"<p>As with any other node, the linux container is a node of a specific kind, <code>linux</code> in this case.</p> <pre><code># a simple topo of two alpine containers connected with each other\nname: demo\n\ntopology:\n  nodes:\n    n1:\n      kind: linux\n      image: alpine:latest\n    n2:\n      kind: linux\n      image: alpine:latest\n  links:\n    - endpoints: [\"n1:eth1\",\"n2:eth1\"]\n</code></pre> <p>With a topology file like that, the nodes will start and both containers will have <code>eth1</code> link available.</p> <p>Containerlab tries to deliver the same level of flexibility in container configuration as docker-compose has. With linux containers it is possible to use the following node configuration parameters:</p> <ul> <li>image - to set an image source for the container</li> <li>binds - to mount files from the host to a container</li> <li>ports - to expose services running in the container to a host</li> <li>env - to set environment variables</li> <li>user - to set a user that will be used inside the container system</li> <li>cmd - to provide a command that will be executed when the container is started</li> </ul> <p>Note</p> <p>Nodes of <code>linux</code> kind will have a <code>on-failure</code> restart policy when run with docker runtime. This means that if container fails/exits with a non zero return code, docker will restart this container automatically. When restarted, the container will loose all non-<code>eth0</code> interfaces. These can be re-added manually with tools veth command.</p>","boost":4},{"location":"manual/kinds/openbsd/","title":"OpenBSD","text":"<p>OpenBSD is identified with <code>openbsd</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/openbsd/#getting-openbsd-image","title":"Getting OpenBSD image","text":"<p>To build OpenBSD docker container image you will need to download a custom-built <code>qcow2</code> VM image with pre-installed cloud-init from https://bsd-cloud-image.org/.</p> <p>If, for some reason, you're unable to obtain an image from https://bsd-cloud-image.org/, you can build it yourself with the script from this repository.</p>","boost":4},{"location":"manual/kinds/openbsd/#managing-openbsd-nodes","title":"Managing OpenBSD nodes","text":"<p>Note</p> <p>Containers with OpenBSD inside will take ~1-2 min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>OpenBSD node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHTelnet <p>to connect to a <code>bash</code> shell of a running OpenBSD container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the OpenBSD shell (password <code>admin</code>) <pre><code>ssh admin@&lt;container-name&gt;\n</code></pre></p> <p>serial port (console) is exposed over TCP port 5000: <pre><code># from container host\ntelnet &lt;container-name&gt; 5000\n</code></pre> You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/openbsd/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in OpenBSD.</p> <p>The interface naming convention is: <code>vioX</code>, where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>vio1</code> - first data port available</li> <li><code>vio2</code> - second data port, and so on...</li> </ul> <p>Warning</p> <p>Data port numbering starts at <code>1</code>, as <code>vio0</code> is reserved for management connectivity. Attempting to use <code>vio0</code> in a containerlab topology will result in an error.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the OpenBSD VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>vio0</code> in the CLI)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>vio1</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>vio2</code> and so on)</li> </ul> <p>When containerlab launches OpenBSD node the <code>vio0</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the OpenBSD using containerlab's assigned IP.</p> <p>Data interfaces <code>vio1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/openbsd/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/openbsd/#node-configuration","title":"Node configuration","text":"<p>OpenBSD nodes come up with a basic configuration where only the management interface and a default user are provisioned.</p>","boost":4},{"location":"manual/kinds/openbsd/#configuration-save","title":"Configuration save","text":"<p>Containerlab's <code>save</code> command will perform a configuration backup for <code>OpenBSD</code> nodes via SCP. The entire <code>/etc</code> directory of each node will be archived and saved under <code>backup.tar.gz</code> file and can be found at the node's directory inside the lab parent directory:</p> <pre><code># assuming the lab name is \"openbsd01\"\n# and node name is \"obsd1\"\nls clab-openbsd01/obsd1/config/\nbackup.tar.gz\n</code></pre> <p>If the backup file is present upon the node's boot, it will be transferred to the node and extracted.The node will then reboot to apply the restored configuration.</p>","boost":4},{"location":"manual/kinds/openbsd/#lab-examples","title":"Lab examples","text":"<p>The following simple lab consists of two Linux hosts connected via one OpenBSD host:</p> <ul> <li>Openbsd</li> </ul>","boost":4},{"location":"manual/kinds/ostinato/","title":"Ostinato","text":"<p>Ostinato network traffic generator is currently identified with <code>linux</code> kind in the topology file. This will change to its own kind in the future.</p>","boost":4},{"location":"manual/kinds/ostinato/#getting-ostinato-image","title":"Getting Ostinato image","text":"<p>Ostinato for containerlab image is a paid (but inexpensive) offering and can be obtained from the Ostinato for Containerlab section of the Ostinato website. Follow the instructions on the page to install the image.</p>","boost":4},{"location":"manual/kinds/ostinato/#topology-definition","title":"Topology definition","text":"<p>Add a topology definition for the Ostinato node in <code>.clab.yml</code> as shown below -</p> <pre><code>topology:\n  nodes:\n    ost:\n      kind: linux\n      image: ostinato/ostinato:{tag}\n      ports:\n        - 5900:5900/tcp\n        - 7878:7878/tcp\n</code></pre> <p>Replace <code>{tag}</code> above with the tag shown in the output of <code>docker images</code></p>","boost":4},{"location":"manual/kinds/ostinato/#managing-ostinato-nodes","title":"Managing Ostinato nodes","text":"<p>Ostinato has a GUI and a Python API. The Ostinato image includes both the Ostinato agent (called Drone) which does the actual traffic generation and the Ostinato GUI that is used to configure and monitor the Drone agent.</p> <p>The GUI is the primary way to work with Ostinato and is accessible over VNC. To use the Ostinato API, you will need Ostinato PyApi.</p> Using GUIUsing Python APIshell <p>Once the lab is deployed, connect any VNC client to <code>&lt;host-ip&gt;:5900</code> - this will bring up the Ostinato GUI.</p> <p></p> <p>The GUI only lists the data interfaces and hence, <code>eth0</code> will not be included</p> <p>To manage the Ostinato node using the Ostinato Python API, you will typically write a script using the API and run it on the same or different host connecting to the Drone agent via the management interface</p> <p>You can login to the shell of the Ostinato node for any troubleshooting, if required. Ostinato does not have any CLI or commands to generate traffic - use the GUI (or API).</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre>","boost":4},{"location":"manual/kinds/ostinato/#interfaces-mapping","title":"Interfaces mapping","text":"<p>Ostinato container includes the following interfaces -</p> <ul> <li><code>eth0</code> - management interface (should NOT be used for data traffic)</li> <li><code>eth1+</code> - data interfaces to connect to other nodes for traffic generation</li> </ul>","boost":4},{"location":"manual/kinds/ostinato/#file-mounts","title":"File mounts","text":"<p>Ostinato allows you to save and load files - traffic streams, pcaps and session files. To ensure persistence of these files (after a lab is destroyed), mount a directory on the host to a directory inside the container as shown in the example below -</p> <pre><code>topology:\n  nodes:\n    ost:\n      kind: linux\n      image: ostinato/ostinato:{tag}\n      ports:\n        - 5900:5900/tcp\n        - 7878:7878/tcp\n      binds:\n        - /some/dir/on/host:/some/path/in/container\n</code></pre> <p>You can read more about node binds</p>","boost":4},{"location":"manual/kinds/ostinato/#lab-examples","title":"Lab examples","text":"<p>A simple Ostinato and Nokia SR Linux lab demonstrates a simple test topology, IPv4 traffic streams to verify L3 forwarding and a short video clip that shows it in action.</p>","boost":4},{"location":"manual/kinds/ostinato/#more","title":"More","text":"<p>You can find more information including how to run the Ostinato GUI natively on your Windows/MacOS laptop to manage the Ostinato agent running inside containterlab on the Ostinato for Containerlab page.</p>","boost":4},{"location":"manual/kinds/ovs-bridge/","title":"Openvswitch bridge","text":"<p>Similar to linux bridge capability, containerlab allows to connect nodes to an Openvswitch (Ovs) bridge. Ovs bridges offers even more connectivity options compared to classic Linux bridge, as well as it allows to create stretched L2 domain by means of tunneled interfaces (vxlan).</p>","boost":4},{"location":"manual/kinds/ovs-bridge/#using-ovs-bridge-kind","title":"Using ovs-bridge kind","text":"<p>Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file, the Ovs bridge needs to be created first.</p> <p>Once the bridge is created, it has to be referenced as a node inside the topology file:</p> <pre><code>name: ovs\n\ntopology:\n  nodes:\n    myovs:\n      kind: ovs-bridge\n    ceos:\n      kind: ceos\n      image: ceos:latest\n  links:\n    - endpoints: [\"myovs:ovsp1\", \"srl:eth1\"]\n</code></pre> <p>In the example above, node <code>myovs</code> of kind <code>ovs-bridge</code> tells containerlab to identify it as a Ovs bridge and look for a bridge named <code>myovs</code>.</p> <p>When connecting other nodes to a bridge, the bridge endpoint must be present in the <code>links</code> section.</p> <p>Note</p> <p>When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we attach a single interfaces named <code>ovsp1</code> to the Ovs bridge with a name <code>myovs</code>. Before that, we ensured that no other interfaces named <code>ovsp1</code> existed.</p> <p>As a result of such topology definition, you will see the Ovs bridge <code>br-clab</code> with three interfaces attached to it:</p> <pre><code>\u276f ovs-vsctl show\n918a3466-4368-4167-9162-f2cf80a0c106\n    Bridge myovs\n        Port myovs\n            Interface myovs\n                type: internal\n        Port eth1\n            Interface ovsp1\n    ovs_version: \"2.13.1\"\n</code></pre>","boost":4},{"location":"manual/kinds/rare-freertr/","title":"RARE/freeRtr","text":"<p>RARE stands for Router for Academia, Research &amp; Education. It is an open source routing platform, used to create a network operating system (NOS) on commodity hardware (a white box switch). RARE uses FreeRtr as a control plane software  and is thus often referred to as RARE/freeRtr.</p> <p>RARE nodes are identified by the <code>rare</code> kind in the topology file.</p> what is RARE? <p>RARE/freeRtr has the particularity to run interchangeably different dataplanes such P4 INTEL TOFINO, P4 BMv2, DPDK, XDP, libpcap or UNIX UDP raw sockets. This inherent property allows RARE/freeRtr to run multiple use cases requiring different bandwidth capability.</p> <p>It can be used as:</p> <ul> <li>a full featured versatile DPDK SOHO router able to handle nx1GE, nx10GE and a couple of 100GE.</li> <li>a service provider Metropolitan Arean Network IPv4/IPv6 MPLS router</li> <li>a full featured BGP Route Reflector</li> </ul> <p>More information here and here.</p>","boost":4},{"location":"manual/kinds/rare-freertr/#getting-rare-image","title":"Getting RARE image","text":"<p>RARE/freeRtr container image is freely available on GitHub Container Registry.</p> <p>The container image is nightly build of a RARE/freeRtr control plane off the <code>master</code> branch.</p>","boost":4},{"location":"manual/kinds/rare-freertr/#managing-rarefreertr-nodes","title":"Managing RARE/freeRtr nodes","text":"<p>RARE/freeRtr node launched with containerlab can be managed via the following interfaces:</p> bashCLI <p>to connect to a <code>bash</code> shell of a running RARE/freeRtr container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to RARE/freeRtr CLI directly <pre><code>telnet &lt;container-name/id&gt;\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>rare:rare</code></p>","boost":4},{"location":"manual/kinds/rare-freertr/#interfaces-mapping","title":"Interfaces mapping","text":"<p>RARE/freeRtr container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface</li> <li><code>eth2</code> - second data interface</li> <li><code>eth&lt;n&gt;</code> - n<sup>th</sup> data interface</li> </ul> <p>When containerlab launches RARE/freeRtr node:</p> <ul> <li>It will assign IPv4/6 address to the <code>eth0</code> interface.</li> <li>Data interface <code>eth1</code>, <code>eth2</code>, <code>eth&lt;n&gt;</code> need to be configured with IP addressing manually.</li> </ul>","boost":4},{"location":"manual/kinds/rare-freertr/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/rare-freertr/#node-configuration","title":"Node configuration","text":"<p>RARE/freeRtr nodes have a dedicated <code>/run</code><sup>1</sup> directory that is used to persist the configuration of the node which consists of 2 files:</p> <ul> <li><code>rtr-hw.txt</code> also called freeRtr hardware file</li> <li><code>rtr-sw.txt</code> also called freeRtr software file</li> </ul>","boost":4},{"location":"manual/kinds/rare-freertr/#user-defined-config","title":"User defined config","text":"<p>It is possible to make RARE/freeRtr nodes to boot up with a user-defined config instead of a default one. In this case you'd have to create <code>rtr-hw.txt</code> and <code>rtr-sw.txt</code> files and bind mount them to the <code>/rtr/run/conf</code> dir:</p> <pre><code>nodes:\n  rtr1:\n    kind: rare\n    image: ghcr.io/rare-freertr/freertr-containerlab:latest\n    binds:\n      - rtr-hw.txt:/rtr/run/conf/rtr-hw.txt\n      - rtr-sw.txt:/rtr/run/conf/rtr-sw.txt\n</code></pre>","boost":4},{"location":"manual/kinds/rare-freertr/#saving-configuration","title":"Saving configuration","text":"<p>Configuration is saved using <code>write</code> command using RARE/freeRtr CLI. The router configuration will be saved at <code>&lt;lab_dir&gt;/&lt;node_name&gt;/run/</code></p>","boost":4},{"location":"manual/kinds/rare-freertr/#license","title":"License","text":"<p>As an open source software, RARE/freeRtr does not require any license file.</p>","boost":4},{"location":"manual/kinds/rare-freertr/#build-rarefreertr-container","title":"Build RARE/freeRtr Container","text":"<p>RARE/freeRTr container can be built:</p> <pre><code>git clone https://github.com/rare-freertr/freeRtr-containerlab.git\ncd freeRtr-containerlab\ndocker build --no-cache -t freertr-containerlab:latest .\n</code></pre>","boost":4},{"location":"manual/kinds/rare-freertr/#file-mounts","title":"File mounts","text":"<p>During lab initialisation, each node will have their <code>run</code> folder created.</p> <p>In the lab example above:</p> <pre><code>cd freeRtr-containerlab\n$ ls clab-rtr000/rtr1/run/\nconf  logs  mrt  ntfw  pcap\n\n$ ls clab-rtr000/rtr2/run/\nconf  logs  mrt  ntfw  pcap\n</code></pre> <pre><code>root@debian:~/development/testclab/freeRtr-containerlab# tree clab-rtr000/\nclab-rtr000/\n\u251c\u2500\u2500 ansible-inventory.yml\n\u251c\u2500\u2500 authorized_keys\n\u251c\u2500\u2500 rtr1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 run\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 conf\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet-all.sh\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet.eth\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet.mac\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet-main.sh\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet.ser\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 pcapInt.bin -&gt; /rtr/pcapInt.bin\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 rtr-hw.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 rtr-sw.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 freertr.log\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mrt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ntfw\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pcap\n\u251c\u2500\u2500 rtr2\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 run\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 conf\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet-all.sh\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet.eth\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet.mac\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet-main.sh\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hwdet.ser\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 pcapInt.bin -&gt; /rtr/pcapInt.bin\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 rtr-hw.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 rtr-sw.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 freertr.log\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mrt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ntfw\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pcap\n\u2514\u2500\u2500 topology-data.json\n</code></pre> <ul> <li><code>conf</code> folder is where RARE/freeRtr configuration files are located</li> <li><code>logs</code> folder is where RARE/freeRtr logs files are located (output of <code>show logging</code>)</li> <li><code>pcap</code> folder is where <code>pcap</code> files are located (<code>packet capture eth1</code>)</li> <li><code>ntfw</code> folder is where netflow files are stored (future use - not configured currently in default config)</li> <li><code>mrt</code> folder is where <code>bmp</code> output files are stored (future use - not configured currently in default config)</li> </ul>","boost":4},{"location":"manual/kinds/rare-freertr/#lab-example","title":"Lab example","text":"<p>The following labs feature RARE/freeRtr node:</p> <ul> <li>RARE/freeRtr</li> </ul> <ol> <li> <p><code>/run</code> directory is created in the Lab directory for each RARE/freeRtr node.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/sonic-vm/","title":"SONiC (VM)","text":"<p>SONiC Network OS is distributed in two formats suitable for testing with containerlab</p> <ol> <li>Containerized SONiC (<code>sonic-vs</code> kind)</li> <li>Virtual Machine SONiC (<code>sonic-vm</code> kind; the topic of this document)</li> </ol> <p>This document covers the VM flavor of the upstream SONiC that is identified with <code>sonic-vm</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a <code>sonic-vm</code> node.</p> <p>The VM-based image of SONiC is built with the <code>hellt/vrnetlab</code> project.</p>","boost":4},{"location":"manual/kinds/sonic-vm/#getting-sonic-images","title":"Getting Sonic images","text":"<p>Getting SONiC images is possible via two resources:</p> <ol> <li>Sonic.software -- an unofficial repo with SONiC images (may be down sometimes, uses Azure pipeline as a source)</li> <li>Azure pipeline -- an official source of SONiC images (may also be down eventually), and finding the right one there is a pita.</li> <li>Another pipeline view -- may not contain recent pipelines.</li> </ol> <p>When https://sonic.software is down, you can follow the following procedure to find the SONiC image in the Azure pipeline artifacts maze:</p> <ol> <li>Go to the piplines list: https://sonic-build.azurewebsites.net/ui/sonic/pipelines</li> <li>Scroll all the way to the bottom where <code>vs</code> platform is listed</li> <li>Pick a branch name that you want to use (e.g. <code>202405</code>) and click on the \"Build History\".</li> <li>On the build history page choose the latest build that has succeeded (check the Result column) and click on the \"Artifacts\" link</li> <li>In the new window, you will see a list with a single artifact, click on it</li> <li>One more long scroll down until you see <code>target/sonic-vs.img.gz</code> name (or Ctrl+F for it), click on it to start the download or copy the download link.</li> <li>Here you go, you managed to download a SONiC image from a mysteriosly named branch for a build that probably means nothing to you. This Sonic experience for ya...</li> </ol> How to download SONiC image from Azure pipeline (video)","boost":4},{"location":"manual/kinds/sonic-vm/#managing-sonic-vm-nodes","title":"Managing sonic-vm nodes","text":"<p>SONiC node launched with containerlab takes approximately 1 minute to boot up and can be managed via the following interfaces:</p> <p>Note</p> <p>The default login credentials for the SONiC VM are <code>admin:admin</code></p> SSHTelnet <p>To open a linux shell simply type in</p> <pre><code>ssh &lt;node-name&gt;\n</code></pre> <p>You will enter the bash shell of the VM:</p> <pre><code>\u276f ssh clab-sonic-sonic \nWarning: Permanently added 'clab-sonic-sonic' (RSA) to the list of known hosts.\nDebian GNU/Linux 12 \\n \\l\n\nadmin@clab-sonic-sonic's password: \nLinux sonic 6.1.0-11-2-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.38-4 (2023-08-08) x86_64\nYou are on\n  ____   ___  _   _ _  ____\n / ___| / _ \\| \\ | (_)/ ___|\n \\___ \\| | | |  \\| | | |\n  ___) | |_| | |\\  | | |___\n |____/ \\___/|_| \\_|_|\\____|\n\n-- Software for Open Networking in the Cloud --\n\nUnauthorized access and/or use are prohibited.\nAll access and/or use are subject to monitoring.\n\nHelp:    https://sonic-net.github.io/SONiC/\n\nLast login: Wed Jul  3 09:45:35 2024\nadmin@sonic:~$\n</code></pre> <p>From within the Linux shell users can perform system configuration using linux utilities, or connect to the SONiC CLI using <code>vtysh</code> command.</p> <pre><code>admin@sonic:~$ vtysh\n\nHello, this is FRRouting (version 8.5.4).\nCopyright 1996-2005 Kunihiro Ishiguro, et al.\n\nsonic#\n</code></pre> <p>to connect to sonic-vm CLI via telnet</p> <pre><code>telnet &lt;container-name/id&gt; 5000\n</code></pre>","boost":4},{"location":"manual/kinds/sonic-vm/#interfaces-mapping","title":"Interfaces mapping","text":"<p>sonic-vm container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data (front-panel port) interface that is mapped to Ethernet0 port</li> <li><code>eth2</code> - second data interface that is mapped to Ethernet4 port. Any new port will result in a \"previous interface + 4\" (Ethernet4) mapping.</li> </ul> <p>When containerlab launches sonic-vs node, it will assign IPv4/6 address to the <code>eth0</code> interface. Data interface <code>eth1</code> mapped to <code>Ethernet0</code> port and needs to be configured with IP addressing manually.</p>","boost":4},{"location":"manual/kinds/sonic-vm/#features-and-options","title":"Features and Options","text":"","boost":4},{"location":"manual/kinds/sonic-vm/#startup-configuration","title":"Startup configuration","text":"<p>VM-based SONiC supports the <code>startup-config</code> feature. The startup configuration file is a JSON file that is available in the VM's filesystem by the <code>/etc/sonic/config_db.json</code> path.</p> <p>Extracting the config from a running node is possible with <code>containerlab save</code> command. The config will be available in the lab directory.</p>","boost":4},{"location":"manual/kinds/sonic-vs/","title":"SONiC (container)","text":"<p>SONiC Network OS is distributed in two formats suitable for testing with containerlab</p> <ol> <li>Containerized SONiC (topic of this document)</li> <li>Virtual Machine SONiC</li> </ol> <p>This document covers the containerized SONiC that is identified with <code>sonic-vs</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a <code>sonic-vs</code> node.</p>","boost":4},{"location":"manual/kinds/sonic-vs/#getting-sonic-images","title":"Getting Sonic images","text":"<p>Getting SONiC images is possible via two resources:</p> <ol> <li>Sonic.software -- an unofficial repo with SONiC images (may be down sometimes, uses Azure pipeline as a source)</li> <li>Azure pipeline -- an official source of SONiC images, but finding the right one there is a pita.</li> </ol> <p>When https://sonic.software is down, you can follow the following procedure to find the SONiC image in the Azure pipeline artifacts maze:</p> <ol> <li>Go to the piplines list: https://sonic-build.azurewebsites.net/ui/sonic/pipelines</li> <li>Scroll all the way to the bottom where <code>vs</code> platform is listed</li> <li>Pick a branch name that you want to use (e.g. <code>202405</code>) and click on the \"Build History\".</li> <li>On the build history page choose the latest build that has succeeded (check the Result column) and click on the \"Artifacts\" link</li> <li>In the new window, you will see a list with a single artifact, click on it</li> <li>One more long scroll down until you see <code>target/docker-sonic-vs.gz</code> name (or Ctrl+F for it), click on it to start the download or copy the download link.</li> <li>Here you go, you managed to download a SONiC image from a mysteriosly named branch for a build that probably means nothing to you. This Sonic experience for ya...</li> </ol> How to download SONiC image from Azure pipeline (video)","boost":4},{"location":"manual/kinds/sonic-vs/#managing-sonic-vs-nodes","title":"Managing sonic-vs nodes","text":"<p>SONiC node launched with containerlab can be managed via the following interfaces:</p> bashCLI <p>to connect to a <code>bash</code> shell of a running sonic-vs container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>to connect to the sonic-vs CLI (vtysh)</p> <pre><code>docker exec -it &lt;container-name/id&gt; vtysh\n</code></pre>","boost":4},{"location":"manual/kinds/sonic-vs/#interfaces-mapping","title":"Interfaces mapping","text":"<p>sonic-vs container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data (front-panel port) interface</li> </ul> <p>When containerlab launches sonic-vs node, it will assign IPv4/6 address to the <code>eth0</code> interface. Data interface <code>eth1</code> mapped to <code>Ethernet0</code> port and needs to be configured with IP addressing manually. See Lab examples for exact configurations.</p>","boost":4},{"location":"manual/kinds/sonic-vs/#lab-examples","title":"Lab examples","text":"<p>The following labs feature sonic-vs node:</p> <ul> <li>SR Linux and sonic-vs</li> </ul>","boost":4},{"location":"manual/kinds/srl/","title":"Nokia SR Linux","text":"<p>Nokia SR Linux NOS is identified with <code>nokia_srlinux</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a node.</p>","boost":4},{"location":"manual/kinds/srl/#getting-sr-linux-image","title":"Getting SR Linux image","text":"<p>Nokia SR Linux is the first commercial Network OS with a free and open distribution model. Everyone can pull SR Linux container from a public registry:</p> <pre><code># pull latest available release\ndocker pull ghcr.io/nokia/srlinux\n</code></pre> <p>To pull a specific version, use tags that match the released version and are listed in the srlinux-container-image repo.</p> <p>ARM64-native SR Linux container image</p> <p>SR Linux Network OS is also available as an ARM64-native container image in a preview mode. The preview mode means that some issues may be present, as the image is not yet fully qualified.</p> <p>Starting with SR Linux 24.10.1 the container image is built using the manifest list, so when you pull the image, the correct architecture is selected automatically.</p> <p>ARM64 image unlocks running networking labs on Apple macOS with M-chips, as well as cloud instances with ARM64 architecture and on new Microsoft Surface laptops.</p>","boost":4},{"location":"manual/kinds/srl/#managing-sr-linux-nodes","title":"Managing SR Linux nodes","text":"<p>There are many ways to manage SR Linux nodes, ranging from classic CLI management all the way up to the gNMI programming.</p> bashCLI/SSHgNMIJSON-RPCSNMPNETCONF <p>to connect to a <code>bash</code> shell of a running SR Linux container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>to connect to the SR Linux CLI</p> <pre><code>docker exec -it &lt;container-name/id&gt; sr_cli\n</code></pre> <p>or with SSH <code>ssh admin@&lt;container-name&gt;</code></p> <p>using the best in class gnmic gNMI client as an example:</p> <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --skip-verify \\\n-u admin -p \"NokiaSrl1!\" \\\n-e json_ietf \\\nget --path /system/name/host-name\n</code></pre> <p>SR Linux has a JSON-RPC interface running over ports 80/443 for HTTP/HTTPS schemas.</p> <p>HTTPS server uses the same TLS certificate as gNMI server.</p> <p>Here is an example of getting version information with JSON-RPC:</p> <pre><code>curl http://admin:admin@clab-srl-srl/jsonrpc -d @- &lt;&lt; EOF\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\":\n    {\n        \"commands\":\n        [\n            {\n                \"path\": \"/system/information/version\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre> <p>SR Linux nodes come up with SNMPv2 server enabled and running on port 161. The default SNMP community is <code>public</code>.</p> <pre><code>docker run -i -t ghcr.io/hellt/net-snmp-tools:5.9.4-r0 \\\n  snmpwalk -v 2c -c public &lt;node-name&gt;\n</code></pre> <p>From SR Linux release 24.7.1 onwards, SR Linux comes with NETCONF server enabled and running on port 830.</p> <p>Using netconf-console2:</p> <pre><code>docker run --rm --network clab -i -t \\\nghcr.io/hellt/netconf-console2:3.0.1 \\\n--host &lt;node-name&gt; --port 830 -u admin -p 'NokiaSrl1!' \\\n--hello\n</code></pre>","boost":4},{"location":"manual/kinds/srl/#credentials","title":"Credentials","text":"<p>Default credentials<sup>1</sup>: <code>admin:NokiaSrl1!</code></p> <p>Containerlab will automatically enable public-key authentication for <code>root</code>, <code>admin</code> and <code>linuxadmin</code> users if public key files are found at <code>~/.ssh</code> directory<sup>2</sup>.</p>","boost":4},{"location":"manual/kinds/srl/#interfaces-naming","title":"Interfaces naming","text":"<p>You can use interfaces names in the topology file like they appear in SR Linux.</p> <p>The interface naming convention is: <code>ethernet-1/Y</code>, where <code>1</code> is the only available line card and <code>Y</code> is the port on the line card.</p> <p>With that naming convention in mind:</p> <ul> <li><code>ethernet-1/1</code> - first ethernet interface on line card 1</li> <li><code>ethernet-1/2</code> - second interface on line card 1</li> </ul> <p>As an example:</p> <pre><code>  links:\n    # srlinux port ethernet-1/3 is connected to vsrx port ge-0/0/3\n    - endpoints: [\"srlinux:ethernet-1/3\", \"vsrx:ge-0/0/3\"]\n    # srlinux port ethernet-1/5 is connected to sros port 2\n    - endpoints: [\"srlinux:ethernet-1/5\", \"sros:1/1/2\"]\n</code></pre> <p>SR Linux system expects interfaces inside the container to be named in a specific way - <code>e1-Y</code> - where <code>1</code> is the only available line card and <code>Y</code> is the port on the line card, however, it is optional (but still fully supported) to use this internal naming convention in Containerlab topologies.</p> <p>The example ports above would be mapped to the following Linux interfaces:</p> <ul> <li><code>e1-1</code> - first ethernet interface on line card 1</li> <li><code>e1-2</code> - second interface on line card 1</li> </ul>","boost":4},{"location":"manual/kinds/srl/#breakout-interfaces","title":"Breakout interfaces","text":"<p>You can also use breakout (or channelised) interfaces on SR Linux nodes.</p> <pre><code>  links:\n    # srlinux's first breakout port ethernet-1/3/1\n    # is connected to sros port 2\n    - endpoints: [\"srlinux:ethernet-1/3/1\", \"sros:1/1/2\"]\n    # srlinux's second breakout port ethernet-1/3/2\n    # is connected to vEOS port Et1/2\n    - endpoints: [\"srlinux:ethernet-1/3/2\", \"veos:Et1/2\"]\n</code></pre> <p>The breakout interfaces will have the mapped Linux interface name <code>eX-Y-Z</code> where <code>Z</code> is the breakout port number. For example, if interface <code>ethernet-1/3</code> on an IXR-D3 system is meant to act as a breakout 100Gb to 4x25Gb, and the first breakout port is used in the topology (<code>ethernet-1/3/1</code>), then the mapped interfaces in the container will be called <code>e1-3-1</code>.</p>","boost":4},{"location":"manual/kinds/srl/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/srl/#types","title":"Types","text":"<p>For SR Linux nodes <code>type</code> defines the hardware variant that this node will emulate.</p> <p>The available 7220 IXR models support the following types: <code>ixrd1</code>, <code>ixrd2</code>, <code>ixrd3</code>, <code>ixrd2l</code>, <code>ixrd3l</code>, <code>ixrd4</code>, <code>ixrd5</code>, <code>ixrh2</code>, <code>ixrh3</code> and <code>ixrh4</code>.</p> <p>Nokia 7250 IXR chassis identified with types <code>ixr6e</code>, <code>ixr10e</code>, <code>ixrx3b</code> and <code>ixrx1b</code> require a valid license to operate.</p> <p>If type is not set in the clab file <code>ixrd2</code> value will be used by containerlab.</p> <p>Based on the provided type, containerlab will generate the topology file that will be mounted to the SR Linux container and make it boot in a chosen HW variant.</p>","boost":4},{"location":"manual/kinds/srl/#node-configuration","title":"Node configuration","text":"<p>SR Linux uses a <code>/etc/opt/srlinux/config.json</code> file to persist its configuration. By default, containerlab starts nodes of <code>srl</code> kind with a basic \"default\" config, and with the <code>startup-config</code> parameter, it is possible to provide a custom config file that will be used as a startup one.</p>","boost":4},{"location":"manual/kinds/srl/#default-node-configuration","title":"Default node configuration","text":"<p>When a node is defined without the <code>startup-config</code> statement present, containerlab will make additional configurations on top of the factory config:</p> <pre><code># example of a topo file that does not define a custom startup-config\n# as a result, the default configuration will be used by this node\n\nname: srl_lab\ntopology:\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n      type: ixrd3\n</code></pre> <p>The rendered config can be found at <code>/tmp/clab-default-config</code> path on SR Linux filesystem and will be saved by the path <code>clab-&lt;lab_name&gt;/&lt;node-name&gt;/config/config.json</code>. Using the example topology presented above, the exact path to the config will be <code>clab-srl_lab/srl1/config/config.json</code>.</p> <p>Additional configurations that containerlab adds on top of the factory config:</p> <ul> <li>enabling interfaces (<code>admin-state enable</code>) referenced in the topology's <code>links</code> section</li> <li>enabling LLDP</li> <li>enabling gNMI/gNOI/JSON-RPC as well as enabling unix-socket access for gRPC services</li> <li>creating tls server certificate</li> <li>setting <code>mgmt0 subinterface 0 ip-mtu</code> to the MTU value of the underlying container runtime network</li> </ul> <p>A configuration checkpoint named <code>clab-initial</code> is generated by containerlab once default and user-provided configs are applied. The checkpoint may be used to quickly revert configuration changes made by a user to a state that was present after the node was started.</p>","boost":4},{"location":"manual/kinds/srl/#user-defined-startup-config","title":"User defined startup config","text":"<p>It is possible to make SR Linux nodes boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property of the node/kind a user sets the path to the local config file that will be used as a startup config.</p> <p>The startup configuration file can be provided in two formats:</p> <ul> <li>full SR Linux config in JSON format</li> <li>partial config in SR Linux CLI format</li> </ul>","boost":4},{"location":"manual/kinds/srl/#cli","title":"CLI","text":"<p>A typical lab scenario is to make nodes boot with a pre-configured use case. The easiest way to do that is to capture the intended changes as CLI commands.</p> <p>On SR Linux, users can configure the system and capture the changes in the form of CLI instructions using the <code>info</code> command. These CLI commands can be saved in a file<sup>3</sup> and used as a startup configuration.</p> CLI config examples <p>these snippets can be the contents of <code>myconfig.cli</code> file referenced in the topology below</p> Regular configFlat config <pre><code>network-instance default {\n    interface ethernet-1/1.0 {\n    }\n    interface ethernet-1/2.0 {\n    }\n    protocols {\n        bgp {\n            admin-state enable\n            autonomous-system 65001\n            router-id 10.0.0.1\n            group rs {\n                peer-as 65003\n                ipv4-unicast {\n                    admin-state enable\n                }\n            }\n            neighbor 192.168.13.2 {\n                peer-group rs\n            }\n        }\n    }\n}\n</code></pre> <pre><code>set / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp router-id 10.10.10.1\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp group ibgp export-policy export-lo\nset / network-instance default protocols bgp neighbor 192.168.1.2 admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-group ibgp\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-as 65001\n</code></pre> <pre><code>name: srl_lab\ntopology:\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n      type: ixrd3\n      image: ghcr.io/nokia/srlinux\n      # a path to the partial config in CLI format relative to the current working directory\n      startup-config: myconfig.cli\n</code></pre> <p>In that case, SR Linux will first boot with the default configuration, and then the CLI commands from the <code>myconfig.cli</code> will be applied. Note, that no entering into the candidate config, nor explicit commit is required to be part of the CLI configuration snippets.</p>","boost":4},{"location":"manual/kinds/srl/#json","title":"JSON","text":"<p>SR Linux persists its configuration as a JSON file that can be found by the <code>/etc/opt/srlinux/config.json</code> path. Users can use this file as a startup configuration like that:</p> <pre><code>name: srl_lab\ntopology:\n  nodes:\n    srl1:\n      kind: nokia_srlinux\n      type: ixrd3\n      image: ghcr.io/nokia/srlinux\n      # a path to the full config in JSON format relative to the current working directory\n      startup-config: myconfig.json\n</code></pre> <p>Containerlab will take the <code>myconfig.json</code> file, copy it to the lab directory for that specific node under the <code>config.json</code> name, and mount that directory to the container. This will result in this config acting as a startup-config for the node.</p>","boost":4},{"location":"manual/kinds/srl/#saving-configuration","title":"Saving configuration","text":"<p>As was explained in the Node configuration section, SR Linux containers can make their config persistent because config files are provided to the containers from the host via the bind mount.</p> <p>When a user configures the SR Linux node, the changes are saved into the running configuration stored in memory. To save the running configuration as a startup configuration, the user needs to execute the <code>tools system configuration save</code> CLI command. This command will write the config to the <code>/etc/opt/srlinux/config.json</code> file that holds the startup-config and is exposed to the host.</p> <p>SR Linux node also supports the <code>containerlab save -t &lt;topo-file&gt;</code> command, which will execute the command to save the running-config on all lab nodes. For SR Linux node, the <code>tools system configuration save</code> will be executed:</p> <pre><code>\u276f containerlab save -t quickstart.clab.yml\nINFO[0000] Parsing &amp; checking topology file: quickstart.clab.yml\nINFO[0001] saved SR Linux configuration from leaf1 node. Output:\n/system:\n    Saved current running configuration as initial (startup) configuration '/etc/opt/srlinux/config.json'\n\nINFO[0001] saved SR Linux configuration from leaf2 node. Output:\n/system:\n    Saved current running configuration as initial (startup) configuration '/etc/opt/srlinux/config.json'\n</code></pre>","boost":4},{"location":"manual/kinds/srl/#tls","title":"TLS","text":"<p>By default, containerlab will generate TLS certificates and keys for each SR Linux node of a lab. The TLS-related files that containerlab creates are located in the TLS directory, which can be found by the <code>&lt;lab-directory&gt;/.tls/</code> path. Here is a list of files that containerlab creates relative to the TLS directory:</p> <ol> <li>CA certificate - <code>./ca/ca.pem</code></li> <li>CA private key - <code>./ca/ca.key</code></li> <li>Node certificate - <code>./&lt;node-name&gt;/&lt;node-name&gt;.pem</code></li> <li>Node private key - <code>./&lt;node-name&gt;/&lt;node-name&gt;.key</code></li> </ol> <p>The generated TLS files will persist between lab deployments. This means that if you destroyed a lab and deployed it again, the TLS files from the initial lab deployment will be used.</p> <p>In case user-provided certificates/keys need to be used, the <code>ca.pem</code>, <code>&lt;node-name&gt;.pem</code> and <code>&lt;node-name&gt;.key</code> files must be copied by the paths outlined above for containerlab to take them into account when deploying a lab.</p> <p>In case only <code>ca.pem</code> and <code>ca.key</code> files are provided, the node certificates will be generated using these CA files.</p> <p>The certificate is generated for the following subjects (assuming node name is <code>srl</code>, lab name is <code>srl</code> and container runtime assigned the below listed IP addresses):</p> <pre><code>DNS:srl\nDNS:clab-srl-srl\nDNS:srl.srl.io\nIP Address:172.20.20.3, IP Address:3fff:172:20:20:0:0:0:3\n</code></pre> <p>Nokia SR Linux nodes support setting of SANs.</p>","boost":4},{"location":"manual/kinds/srl/#grpc-server","title":"gRPC server","text":"<p>Starting with SR Linux 24.3.1, the gRPC server config block is used to configure gRPC-based services such as gNMI, gNOI, gRIBI and P4RT. The factory configuration includes the <code>mgmt</code> gRPC server block to which containerlab adds all those services and:</p> <ul> <li>generated TLS profile - <code>clab-profile</code></li> <li>unix-socket access for gRPC services</li> <li>increased rate limit</li> <li>trace options</li> </ul> <p>These additions are meant to make all gRPC services available to the user out of the box with the enabled tracing and a custom TLS profile.</p> <p>Besides augmenting the factory-provided <code>mgmt</code> gRPC server block, containerlab also adds a new <code>insecure-mgmt</code> gRPC server that provides the same services as the <code>mgmt</code> server but without TLS. This server runs on port 57401 and is meant to be used for testing purposes as well as for local gNMI clients running as part of the NDK apps or local Event Handler scripts.</p>","boost":4},{"location":"manual/kinds/srl/#eda-support","title":"EDA support","text":"<p>To ensure that Containerlab-provisioned SR Linux nodes can be managed by Nokia EDA a set of gRPC servers is added:</p> <ul> <li><code>eda-discovery</code> - provides support for EDA discovery</li> <li><code>eda-mgmt</code> - gRPC server that references <code>EDA</code> TLS security profile that EDA setups with gNSI</li> <li><code>eda-insecure-mgmt</code> - insecure version of the <code>eda-mgmt</code> gRPC server</li> </ul>","boost":4},{"location":"manual/kinds/srl/#ssh-keys","title":"SSH Keys","text":"<p>Containerlab will read the public keys found in <code>~/.ssh</code> directory of a sudo user as well as the contents of a <code>~/.ssh/authorized_keys</code> file if it exists<sup>4</sup>. The public keys will be added to the startup configuration for <code>admin</code> and <code>linuxadmin</code> users to enable passwordless access.</p>","boost":4},{"location":"manual/kinds/srl/#netconf","title":"NETCONF","text":"<p>Containerlab will configure the <code>netconf-mgmt</code> ssh server running over port 830 and the netconf-server instance using this SSH server to enable NETCONF management.</p>","boost":4},{"location":"manual/kinds/srl/#license","title":"License","text":"<p>SR Linux container can run without a license emulating the datacenter types (7220 IXR) . In that license-less mode, the datapath is limited to 1000 PPS and the <code>sr_linux</code> process will restart once a week.</p> <p>The license file lifts these limitations as well as unlocks chassis-based platform variants and a path to it can be provided with <code>license</code> directive.</p>","boost":4},{"location":"manual/kinds/srl/#container-configuration","title":"Container configuration","text":"<p>To start an SR Linux NOS containerlab uses the configuration that is described in SR Linux Software Installation Guide</p> Startup commandSyscallsEnvironment variables <p><code>sudo bash -c /opt/srlinux/bin/sr_linux</code></p> <pre><code>net.ipv4.ip_forward = \"0\"\nnet.ipv6.conf.all.disable_ipv6 = \"0\"\nnet.ipv6.conf.all.accept_dad = \"0\"\nnet.ipv6.conf.default.accept_dad = \"0\"\nnet.ipv6.conf.all.autoconf = \"0\"\nnet.ipv6.conf.default.autoconf = \"0\"\n</code></pre> <p><code>SRLINUX=1</code></p>","boost":4},{"location":"manual/kinds/srl/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a lab directory for storing configuration artifacts. For <code>nokia_srlinux</code> kind, containerlab creates directories for each node of that kind.</p> <pre><code>~/clab/clab-srl02\n\u276f ls -lah srl1\ndrwxrwxrwx+ 6 1002 1002   87 Dec  1 22:11 config\n-rw-r--r--  1 root root  233 Dec  1 22:11 topology.clab.yml\n</code></pre> <p>The <code>config</code> directory is mounted to container's <code>/etc/opt/srlinux/</code> path in <code>rw</code> mode. It will contain configuration that SR Linux runs of as well as the files that SR Linux keeps in its <code>/etc/opt/srlinux/</code> directory:</p> <pre><code>\u276f ls srl1/config\nbanner  cli  config.json  devices  tls  ztp\n</code></pre> <p>The topology file that defines the emulated hardware type is driven by the value of the kinds <code>type</code> parameter. Depending on a specified <code>type</code>, the appropriate content will be populated into the <code>topology.yml</code> file that will get mounted to <code>/tmp/topology.yml</code> directory inside the container in <code>ro</code> mode.</p>","boost":4},{"location":"manual/kinds/srl/#yumapt-repositories","title":"YUM/APT repositories","text":"<p>Containerlab will create and mount repository files for YUM and APT to ensure that SR Linux users can install packages from the aforementioned repos.</p> <p>The repo files are mounted to the following paths:</p> <ul> <li><code>/etc/yum.repos.d/srlinux.repo</code> - for YUM package manager (used in SR Linux releases prior to 23.10)</li> <li><code>/etc/apt/sources.list.d/srlinux.list</code> - for APT package manager</li> </ul>","boost":4},{"location":"manual/kinds/srl/#dns-configuration","title":"DNS configuration","text":"<p>SR Linux's management stack lives in a separate network namespace <code>srbase-mgmt</code>. Due to this fact, the DNS resolver provided by Docker in the root network namespace is not available to the SR Linux management stack.</p> <p>To enable DNS resolution for SR Linux, containerlab will extract the DNS servers configured on the host system from</p> <ul> <li><code>/etc/resolv.conf</code></li> <li><code>run/systemd/resolve/resolv.conf</code></li> </ul> <p>files and configure IP addresses found there as DNS servers in the management network instance of SR Linux:</p> <pre><code>--{ running }--[  ]--\nA:srl# info system dns  \n    system {\n        dns {\n            network-instance mgmt\n            server-list [\n                # these servers were extracted from the host\n                # and provisioned by containerlab\n                10.171.10.1\n                10.171.10.2\n            ]\n        }\n    }\n</code></pre> <p>If you wish to turn off the automatic DNS provisioning, set the <code>servers</code> list to an empty value in the node configuration.</p>","boost":4},{"location":"manual/kinds/srl/#acl-configuration","title":"ACL configuration","text":"<p>Starting with SR Linux 24.3.1 release, containerlab adds CPM filter rules to the default factory configuration to allow the following traffic:</p> <ul> <li>HTTP access over port 80 for v4 and v6</li> <li>Telnet access over port 23 for v4 and v6</li> </ul> <p>These protocols were removed from the default factory configuration in SR Linux 24.3.1 as a security hardening measure, but they are valuable for lab environments, hence containerlab adds them back.</p>","boost":4},{"location":"manual/kinds/srl/#host-requirements","title":"Host Requirements","text":"<p>SR Linux is a containerized NOS, therefore it depends on the host's kernel and CPU. It is recommended to run a kernel v4 and newer, though it might also run on the older kernels.</p>","boost":4},{"location":"manual/kinds/srl/#ssse3-cpu-set","title":"SSSE3 CPU set","text":"<p>SR Linux XDP - the emulated datapath based on DPDK - requires SSSE3 instructions to be available. This instruction set is present on most modern CPUs, but it is missing in the basic emulated CPUs created by hypervisors like QEMU, Proxmox. When this instruction set is not present in the host CPU set, containerlab will abort the lab deployment if it has SR Linux nodes defined.</p> <p>The easiest way to enable SSSE3 instruction set is to configure the hypervisor to use the <code>host</code> CPU type, which exposes all available instructions to the guest. For Proxmox, this can be set in the GUI:</p> <p></p> <p>Or it's also possible via the proxmox configuration file <code>/etc/pve/qemu-server/vmid.conf</code>.</p> <ol> <li> <p>Prior to SR Linux 22.11.1, the default credentials were <code>admin:admin</code>.\u00a0\u21a9</p> </li> <li> <p>The <code>authorized_keys</code> file will be created with the content of all found public keys. This file will be bind-mounted using the respecting paths inside SR Linux to enable password-less access. Experimental feature.\u00a0\u21a9</p> </li> <li> <p>CLI configs can be saved also in the \"flat\" format using <code>info flat</code> command.\u00a0\u21a9</p> </li> <li> <p>If running with <code>sudo</code>, add <code>-E</code> flag to sudo to preserve user' home directory for this feature to work as expected.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/vr-aoscx/","title":"Aruba ArubaOS-CX","text":"<p>ArubaOS-CX virtualized switch is identified with <code>aruba_aoscx</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/vr-aoscx/#managing-vr-aoscx-nodes","title":"Managing vr-aoscx nodes","text":"<p>Note</p> <p>Containers with AOS-CX inside will take ~2min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Aruba AOS-CX node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSH <p>to connect to a <code>bash</code> shell of a running vr-aoscx container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the AOS-CX CLI (password <code>admin</code>) <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-aoscx/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in ArubaOS-CX.</p> <p>The interface naming convention is: <code>1/1/X</code>, where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>1/1/1</code> - first data port available</li> <li><code>1/1/2</code> - second data port, and so on...</li> </ul> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the ArubaOS-CX VM:</p> <ul> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>1/1/1</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>1/1/2</code> and so on)</li> </ul> <p>When containerlab launches ArubaOS-CX node the <code>1/1/1</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the ArubaOS-CX using containerlab's assigned IP.</p> <p>Data interfaces <code>1/1/2+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-aoscx/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-aoscx/#node-configuration","title":"Node configuration","text":"<p>ArubaOS-CX nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>admin</code> user with the provided password.</p>","boost":4},{"location":"manual/kinds/vr-aoscx/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make ArubaOS-CX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: aruba_aoscx\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-c8000v/","title":"Cisco c8000v","text":"<p>The Cisco Catalyst 8000V is identified with <code>cisco_c8000v</code> kind in the topology file.</p> <p>Cisco c8000v is a successor of Cisco CSR1000v and is a different product from Cisco 8000 platform emulator.</p>","boost":4},{"location":"manual/kinds/vr-c8000v/#hardware-resource-requirements","title":"Hardware resource requirements","text":"<p>Each c8000v node is started with 1vCPU and 4GB of RAM by default.</p>","boost":4},{"location":"manual/kinds/vr-c8000v/#managing-c8000v-nodes","title":"Managing c8000v nodes","text":"<p>Note</p> <p>Cisco c8000v boots process takes around 5 minutes. To monitor boot progress:</p> <pre><code>docker logs -f &lt;container-name/id&gt;\n</code></pre> <p>Wait for <code>Startup complete in: &lt;time&gt;</code> message.</p> SSHbash <p><code>ssh admin@&lt;node-name&gt;</code> Password: <code>admin</code></p> <p>to connect to a <code>bash</code> shell of a running c8000v container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>Note</p> <p>Default credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-c8000v/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Cisco Catalyst 8000V.</p> <p>The interface naming convention is: <code>GigabitEthernetX</code> (or <code>GiX</code>), where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Gi2</code> - first data port available</li> <li><code>Gi3</code> - second data port, and so on...</li> </ul> <p>Warning</p> <p>Data port numbering starts at <code>2</code>, as <code>Gi1</code> is reserved for management connectivity. Attempting to use <code>Gi1</code> in a containerlab topology will result in an error.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Cisco Catalyst 8000V VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>GigabitEthernet1</code> in the CLI)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>GigabitEthernet2</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>GigabitEthernet3</code> and so on)</li> </ul> <p>When containerlab launches Cisco Catalyst 8000V node the <code>GigabitEthernet1</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cisco Catalyst 8000V using containerlab's assigned IP.</p> <p>Data interfaces <code>GigabitEthernet2+</code> need to be configured with IP addressing manually using CLI or other available management interfaces and will appear <code>unset</code> in the CLI:</p> <pre><code>node1#sh ip int br\nInterface              IP-Address      OK? Method Status                Protocol\nGigabitEthernet1       10.0.0.15       YES manual up                    up      \nGigabitEthernet2       unassigned      YES unset  administratively down down\n</code></pre>","boost":4},{"location":"manual/kinds/vr-c8000v/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-c8000v/#default-node-configuration","title":"Default node configuration","text":"<p>It is possible to launch nodes of <code>cisco_c8000v</code> kind with a basic config or to provide a custom config file that will be used as a startup config instead.</p> <p>When a node is defined without <code>startup-config</code> statement present, the node will boot with a factory config</p>","boost":4},{"location":"manual/kinds/vr-c8000v/#user-defined-config","title":"User defined config","text":"<p>With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>name: c8000v\ntopology:\n  nodes:\n    c8000:\n      kind: cisco_c8000v\n      startup-config: r1.cfg\n</code></pre> <p>When a config file is passed via <code>startup-config</code> parameter it will be used during an initial lab deployment. However, a config file that might be in the lab directory of a node takes precedence over the startup-config<sup>1</sup>.</p>","boost":4},{"location":"manual/kinds/vr-c8000v/#lab-examples","title":"Lab examples","text":"<pre><code>name: c8000v\ntopology:\n  nodes:\n    node1:\n      kind: cisco_c8000v\n      image: vrnetlab/vr-c8000v:17.11.01a\n    node2:\n      kind: cisco_c8000v\n      image: vrnetlab/vr-c8000v:17.11.01a\n\n  links:\n    - endpoints: [\"node1:Gi2\", \"node2:Gi2\"]\n</code></pre> <ol> <li> <p>if startup config needs to be enforced, either deploy a lab with <code>--reconfigure</code> flag, or use <code>enforce-startup-config</code> setting.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/vr-cat9kv/","title":"Cisco Catalyst 9000v","text":"<p>The Cisco Catalyst 9000v (or Cat9kv for short) is a virtualised form of the Cisco Catalyst 9000 series switches. It is identified with <code>cisco_cat9kv</code> kind in the topology file and built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>The Cisco Catalyst 9000v performs simulation of the dataplane ASICs that are present in the physical hardware. The two simulated ASICs are:</p> <ul> <li>Cisco UADP (Unified Access Data-Plane). This is the default ASIC that's simulated.</li> <li>Silicon One Q200 (referred to as Q200).</li> </ul> <p>Note</p> <p>The Q200 simulation has a limited featureset compared to the UADP simulation.</p>","boost":4},{"location":"manual/kinds/vr-cat9kv/#resource-requirements","title":"Resource requirements","text":"<p>The Cisco Catalyst 9000v is a resource-hungry VM. When launched with the default settings, it requires the following resources:</p> UADP Q200 vCPU 4 4 RAM (MB) 18432 12288 Disk (GB) 4 4 <p>Users can adjust the CPU and memory resources by setting adding appropriate environment variables as explained in Tuning Qemu Parameters section.</p>","boost":4},{"location":"manual/kinds/vr-cat9kv/#managing-cisco-catalyst-9000v-nodes","title":"Managing Cisco Catalyst 9000v nodes","text":"<p>You can manage the Cisco Catalyst 9000v with containerlab via the following interfaces:</p> bashCLINETCONF <p>to connect to a <code>bash</code> shell of a running Cisco Catalyst 9000v container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>to connect to the Cisco Catalyst 9000v  CLI</p> <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre> <p>NETCONF server is running over port 830</p> <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre> <p>Note</p> <p>Default credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-cat9kv/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in the Cisco Catalyst 9000v CLI.</p> <p>The interface naming convention is: <code>GigabitEthernet1/0/X</code> (or <code>Gi1/0/X</code>), where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Gi1/0/1</code> - first data port available</li> <li><code>Gi1/0/2</code> - second data port, and so on...</li> </ul> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Cisco Catalyst 9000v VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network. Mapped to <code>GigabitEthernet0/0</code>.</li> <li><code>eth1</code> - First data-plane interface. Mapped to <code>GigabitEthernet1/0/1</code> interface.</li> <li><code>eth2</code> - Second data-plane interface. Mapped to <code>GigabitEthernet1/0/2</code> interface and so on.</li> </ul> <p>Note</p> <p>Data interfaces may take 5+ minutes to function correctly after the node boots.</p> <p>You must define interfaces in a contigous manner in your toplogy file. For example, if you want to use <code>Gi1/0/4</code> you must define <code>Gi1/0/1</code>, <code>Gi1/0/2</code> and <code>Gi1/0/3</code>. See the example below.</p> <pre><code>name: my-cat9kv-lab\ntopology:\n  nodes:\n    cat9kv1:\n      kind: cisco_cat9kv\n      image: vrnetlab/vr-cat9kv:17.12.01p\n    cat9kv2:\n      kind: cisco_cat9kv\n      image: vrnetlab/vr-cat9kv:17.12.01p\n\n  links:\n    - endpoints: [\"cat9kv1:Gi1/0/1\",\"cat9kv2:GigabitEthernet1/0/1\"] \n    - endpoints: [\"cat9kv1:Gi1/0/2\",\"cat9kv2:GigabitEthernet1/0/2\"]\n    - endpoints: [\"cat9kv1:Gi1/0/3\", \"cat9kv2:GigabitEthernet1/0/3\"]\n    - endpoints: [\"cat9kv1:Gi1/0/4\", \"cat9kv2:GigabitEthernet1/0/4\"]\n</code></pre> <p>Warning</p> <p>Regardless of how many links are defined in your containerlab topology, the Catalyst 9000v will always display 8 data-plane interfaces. Links/interfaces that you did not define in your containerlab topology will not pass any traffic.</p> <p>When containerlab launches Cisco Catalyst 9000v node the <code>GigabitEthernet0/0</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cisco Catalyst 9000v using containerlab's assigned IP.</p> <p>Data interfaces <code>GigabitEthernet1/0/1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces and will appear <code>unset</code> in the CLI:</p> <pre><code>c9kv(config-if)#do sh ip in br\nInterface              IP-Address      OK? Method Status                Protocol\nVlan1                  unassigned      YES unset  administratively down down    \nGigabitEthernet0/0     10.0.0.15       YES manual up                    up      \nGigabitEthernet1/0/1   unassigned      YES unset  up                    up      \nGigabitEthernet1/0/2   unassigned      YES unset  up                    up      \nGigabitEthernet1/0/3   unassigned      YES unset  up                    up      \nGigabitEthernet1/0/4   unassigned      YES unset  up                    up      \nGigabitEthernet1/0/5   unassigned      YES unset  up                    up      \nGigabitEthernet1/0/6   unassigned      YES unset  up                    up      \nGigabitEthernet1/0/7   unassigned      YES unset  up                    up      \nGigabitEthernet1/0/8   unassigned      YES unset  up                    up\n</code></pre>","boost":4},{"location":"manual/kinds/vr-cat9kv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-cat9kv/#asic-data-plane-simulation-configuration","title":"ASIC data-plane simulation configuration","text":"<p>The default ASIC simulation of the node will be UADP. To enable the Q200 simulation or to enable specific features for the UADP simulation, you must provide a <code>vswitch.xml</code> file (with the relevant configuration).</p> <p>You can do this when building the image with vrnetlab, Please refer to the README file in vrnetlab/cat9kv for more information.</p> <p>You can also use supply the vswitch.xml file via <code>binds</code> in the containerlab topology file. Refer to the example below.</p> <pre><code>name: my-cat9kv-lab\ntopology:\n  nodes:\n    node1:\n      kind: cisco_cat9kv\n      image: vrnetlab/vr-cat9kv:17.12.01p\n    binds:\n      - /path/to/vswitch.xml:/vswitch.xml\n</code></pre> <p>Note</p> <p>You can obtain a <code>vswitch.xml</code> file from the relevant Cisco CML node definitions.</p>","boost":4},{"location":"manual/kinds/vr-cat9kv/#environment-variables","title":"Environment variables","text":"<p>There are <code>VCPU</code> and <code>RAM</code> environment variables defined. It is not recommended reduce the resources below the required amount. The node will be unable to boot in this case.</p> <p>The example below assigns 6vCPUs and 20 gigabytes of RAM to the node.</p> <pre><code>name: my-cat9kv-lab\ntopology:\n  nodes:\n    node1:\n      kind: cisco_cat9kv\n      image: vrnetlab/vr-cat9kv:17.12.01p\n    env:\n     VCPU: 6\n     RAM: 20480\n</code></pre>","boost":4},{"location":"manual/kinds/vr-csr/","title":"Cisco CSR1000v","text":"<p>Cisco CSR1000v virtualized router is identified with <code>cisco_csr1000v</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Cisco CSR1000v nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-csr/#managing-cisco-csr1000v-nodes","title":"Managing Cisco CSR1000v nodes","text":"<p>Note</p> <p>Containers with CSR1000v inside will take ~6min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Cisco CSR1000v node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONF <p>to connect to a <code>bash</code> shell of a running Cisco CSR1000v container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the CSR1000v CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-csr/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Cisco CSR1000v.</p> <p>The interface naming convention is: <code>GigabitEthernetX</code> (or <code>GiX</code>), where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Gi2</code> - first data port available</li> <li><code>Gi3</code> - second data port, and so on...</li> </ul> <p>Warning</p> <p>Data port numbering starts at <code>2</code>, as <code>Gi1</code> is reserved for management connectivity. Attempting to use <code>Gi1</code> in a containerlab topology will result in an error.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Cisco CSR1000v VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>GigabitEthernet1</code> in the CLI)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>GigabitEthernet2</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>GigabitEthernet3</code> and so on)</li> </ul> <p>When containerlab launches Cisco CSR1000v node the <code>GigabitEthernet1</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cisco CSR1000v using containerlab's assigned IP.</p> <p>Data interfaces <code>GigabitEthernet2+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-csr/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-csr/#node-configuration","title":"Node configuration","text":"<p>Cisco CSR1000v nodes come up with a basic configuration where only <code>admin</code> user and management interfaces such as NETCONF provisioned.</p>","boost":4},{"location":"manual/kinds/vr-csr/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make CSR1000V nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: cisco_csr1000v\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-ftdv/","title":"Cisco FTDv","text":"<p>Cisco FTDv is identified with <code>cisco_ftdv</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/vr-ftdv/#managing-ftdv-nodes","title":"Managing FTDv nodes","text":"<p>Note</p> <p>Containers with Cisco FTDv inside will take ~1-2 min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Cisco FTDv node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHTelnetHTTPS <p>to connect to a <code>bash</code> shell of a running FTDv container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the FTDv shell (password <code>Admin@123</code>) <pre><code>ssh admin@&lt;container-name&gt;\n</code></pre></p> <p>serial port (console) is exposed over TCP port 5000: <pre><code># from container host\ntelnet &lt;container-name&gt; 5000\n</code></pre> You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p> <p>HTTPS server is running over port 443 -- connect with any browser normally.</p> <p>Info</p> <p>Default user credentials: <code>admin:Admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-ftdv/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Cisco FTDv.</p> <p>The interface naming convention is: <code>GigabitEthernet0/X</code> (or <code>GiX</code>), where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Gi0</code> - first data port available</li> <li><code>Gi1</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>0</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Cisco FTDv VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>Management0/0</code> in the CLI)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>GigabitEthernet0/0</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>GigabitEthernet0/1</code> and so on)</li> </ul> <p>When containerlab launches Cisco FTDv node the <code>Management0/0</code> interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cisco FTDv using containerlab's assigned IP.</p> <p>Data interfaces <code>GigabitEthernet2+</code> need to be configured with IP addressing manually using Web UI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-ftdv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-ftdv/#node-configuration","title":"Node configuration","text":"<p>Cisco FTDv nodes come up with a basic configuration where only the management interface and a default user are provisioned.</p> <p>Nodes are configured for local management with Firepower Device Management (FDM) On-Box management service. FDM is available via HTTPS and takes a few minutes to come up after node boot up.</p>","boost":4},{"location":"manual/kinds/vr-ftdv/#lab-examples","title":"Lab examples","text":"<p>The following simple lab consists of two Linux hosts connected via one FTDv node:</p> <ul> <li>Cisco FTDv</li> </ul>","boost":4},{"location":"manual/kinds/vr-ftosv/","title":"Dell FTOSv (OS10) / ftosv","text":"<p>Dell FTOSv (OS10) virtualized router/switch is identified with <code>dell_ftosv</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Dell FTOSv nodes launched with containerlab comes up pre-provisioned with SSH and SNMP services enabled.</p>","boost":4},{"location":"manual/kinds/vr-ftosv/#managing-dell-ftosv-nodes","title":"Managing Dell FTOSv nodes","text":"<p>Note</p> <p>Containers with FTOS10v inside will take ~2-4min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Dell FTOS10v node launched with containerlab can be managed via the following interfaces:</p> bashCLI <p>to connect to a <code>bash</code> shell of a running Dell FTOSv container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the Dell FTOSv CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-ftosv/#interfaces-mapping","title":"Interfaces mapping","text":"<p>Dell FTOSv container can have different number of available interfaces which depends on platform used under FTOS10 virtualization .qcow2 disk and container image built using vrnetlab project. Interfaces uses the following mapping rules (in topology file):</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of FTOS10v line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Dell FTOSv node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-ftosv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-ftosv/#node-configuration","title":"Node configuration","text":"<p>Dell FTOSv nodes come up with a basic configuration where only <code>admin</code> user and management interfaces such as SSH provisioned.</p>","boost":4},{"location":"manual/kinds/vr-ftosv/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vMX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: dell_ftosv\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/","title":"Cisco Nexus 9000v","text":"<p>Cisco Nexus9000v virtualized router is identified with <code>cisco_n9kv</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Cisco Nexus 9000v nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF, NXAPI and gRPC services enabled.</p> N9kv Lite <p>If you have a Nexus 9000v Lightweight variant, you can use the same <code>cisco_n9kv</code> to launch it</p> <p>By default, Nexus 9kv image with require 10GB memory and 4 CPU. However <code>n9kv-lite</code> VM requires less resources, so you would want to tune the defaults down.</p> <p>Following is sample for setting up lower memory and CPU for the <code>n9kv-lite</code>:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: cisco_n9kv\n      env:\n        QEMU_MEMORY: 6144 # N9kv-lite requires minimum 6GB memory\n        QEMU_SMP: 2 # N9kv-lite requires minimum 2 CPUs\n</code></pre> <p>Please refer to 'tuning qemu parameters' section for more details.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/#managing-cisco-nexus-9000v-nodes","title":"Managing Cisco Nexus 9000v nodes","text":"<p>Note</p> <p>Containers with Cisco Nexus 9000v inside will take ~5min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Cisco Nexus 9000v node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgRPC <p>to connect to a <code>bash</code> shell of a running Cisco Nexus 9000v container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>to connect to the Cisco Nexus 9000v CLI</p> <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre> <p>NETCONF server is running over port 830</p> <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre> <p>gRPC server is running over port 50051</p>","boost":4},{"location":"manual/kinds/vr-n9kv/#credentials","title":"Credentials","text":"<p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-n9kv/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Cisco Nexus 9000v.</p> <p>The interface naming convention is: <code>Ethernet1/X</code> (or <code>Et1/X</code>), where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Ethernet1/1</code> - first data port available</li> <li><code>Ethernet1/2</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>1</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Cisco Nexus 9000v VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>Ethernet1/1</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>Ethernet1/2</code> and so on)</li> </ul> <p>When containerlab launches Cisco Nexus 9000v node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cisco Nexus 9000v using containerlab's assigned IP.</p> <p>Data interfaces <code>Ethernet1/1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-n9kv/#node-configuration","title":"Node configuration","text":"<p>Cisco Nexus 9000v nodes come up with a basic configuration where only <code>admin</code> user and management interfaces such as NETCONF, NXAPI and GRPC provisioned.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make n9kv nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: cisco_n9kv\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-pan/","title":"Palo Alto PA-VM","text":"<p>Palo Alto PA-VM virtualized firewall is identified with <code>paloalto_panos</code> kind in the topology file. It is built using boxen project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Palo Alto PA-VM nodes launched with containerlab come up pre-provisioned with SSH, and HTTPS services enabled.</p>","boost":4},{"location":"manual/kinds/vr-pan/#managing-palo-alto-pa-vm-nodes","title":"Managing Palo Alto PA-VM nodes","text":"<p>Note</p> <p>Containers with Palo Alto PA-VM inside will take ~8min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Palo Alto PA-VM node launched with containerlab can be managed via the following interfaces:</p> bashCLIHTTPS <p>to connect to a <code>bash</code> shell of a running Palo Alto PA-VM container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the Palo Alto PA-VM CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>HTTPS server is running over port 443 -- connect with any browser normally.</p> <p>Info</p> <p>Default user credentials: <code>admin:Admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-pan/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Cisco Nexus9000v.</p> <p>The interface naming convention is: <code>Ethernet1/X</code>, where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Ethernet1/1</code> - first data port available</li> <li><code>Ethernet1/2</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>1</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Cisco Nexus9000v VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>Ethernet1/1</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>Ethernet1/2</code> and so on)</li> </ul> <p>When containerlab launches Cisco Nexus9000v node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cisco Nexus9000v using containerlab's assigned IP.</p> <p>Data interfaces <code>Ethernet1/1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p> <p>Note</p> <p>Palo Alto PA-VM container supports up to 24 interfaces (plus mgmt).</p> <p>Interfaces will not show up in the cli (<code>show interfaces all</code>) until some configuration is made to the interface!</p>","boost":4},{"location":"manual/kinds/vr-pan/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-pan/#node-configuration","title":"Node configuration","text":"<p>Palo Alto PA-VM nodes come up with a basic configuration where only <code>admin</code> user and management interface is provisioned.</p>","boost":4},{"location":"manual/kinds/vr-pan/#user-defined-config","title":"User defined config","text":"<p>It is possible to make Palo Alto PA-VM nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup config:</p> <pre><code>name: lab\ntopology:\n  nodes:\n    ceos:\n      kind: paloalto_panos\n      startup-config: myconfig.conf\n</code></pre>","boost":4},{"location":"manual/kinds/vr-ros/","title":"MikroTik RouterOS Cloud-hosted router","text":"<p>MikroTik RouterOS cloud hosted router is identified with <code>mikrotik_ros</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/vr-ros/#managing-mikrotik-routeros-nodes","title":"Managing MikroTik RouterOS nodes","text":"<p>MikroTik RouterOS node launched with containerlab can be managed via the following interfaces:</p> bashCLITelnet <p>to connect to a <code>bash</code> shell of a running MikroTik RouterOS container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the MikroTik RouterOS CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>serial port (console) is exposed over TCP port 5000: <pre><code># from container host\ntelnet &lt;node-name&gt; 5000\n</code></pre> You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-ros/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in MikroTik RouterOS.</p> <p>The interface naming convention is: <code>etherX</code>, where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>ether2</code> - first data port available</li> <li><code>ether3</code> - second data port, and so on...</li> </ul> <p>Warning</p> <p>Data port numbering starts at <code>2</code>, as <code>ether1</code> is reserved for management connectivity. Attempting to use <code>ether1</code> in a containerlab topology will result in an error.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the MikroTik RouterOS VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network (rendered as <code>ether1</code>)</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>ether2</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>ether3</code> and so on)</li> </ul> <p>When containerlab launches MikroTik RouterOS node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the MikroTik RouterOS using containerlab's assigned IP.</p> <p>Data interfaces <code>ether2+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-ros/#node-configuration","title":"Node configuration","text":"<p>MikroTik RouterOS nodes come up with a basic \"blank\" configuration where only the management interface and user is provisioned.</p>","boost":4},{"location":"manual/kinds/vr-ros/#user-defined-config","title":"User defined config","text":"<p>It is possible to make ROS nodes to boot up with a user-defined startup config instead of a built-in one. With a <code>startup-config</code> property of the node/kind a user sets the path to the config file that will be mounted to a container and used as a startup config:</p> <pre><code>name: ros_lab\ntopology:\n  nodes:\n    ros:\n      kind: mikrotik_ros\n      startup-config: myconfig.txt\n</code></pre> <p>With such topology file containerlab is instructed to take a file <code>myconfig.txt</code> from the current working directory, copy it to the lab directory for that specific node under the <code>/ftpboot/config.auto.rsc</code> name and mount that dir to the container. This will result in this config to act as a startup config for the node via FTP. Mikrotik will automatically import any file with the .auto.rsc suffix.</p>","boost":4},{"location":"manual/kinds/vr-ros/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For MikroTik RouterOS kind containerlab creates <code>ftpboot</code> directory where the config file will be copied as config.auto.rsc.</p>","boost":4},{"location":"manual/kinds/vr-sros/","title":"Nokia SR OS","text":"<p>Nokia SR OS virtualized router is identified with <code>nokia_sros</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Nokia SR OS nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-sros/#managing-nokia-sr-os-nodes","title":"Managing Nokia SR OS nodes","text":"<p>Note</p> <p>Containers with SR OS inside will take ~3min to fully boot. You can monitor the progress with <code>watch docker ps</code> waiting till the status will change to <code>healthy</code>.</p> <p>Nokia SR OS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgNMITelnet <p>to connect to a <code>bash</code> shell of a running Nokia SR OS container:</p> <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre> <p>to connect to the SR OS CLI</p> <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre> <p>NETCONF server is running over port 830</p> <pre><code>ssh admin@&lt;node-name&gt; -p 830 -s netconf\n</code></pre> <p>or using netconf-console2 container:</p> <pre><code>docker run --rm --network clab -i -t \\\nghcr.io/hellt/netconf-console2:3.0.1 \\\n--host &lt;node-name&gt; --port 830 -u admin -p 'admin' \\\n--hello\n</code></pre> <p>using the best in class gnmic gNMI client as an example:</p> <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u admin -p admin \\\ncapabilities\n</code></pre> <p>serial port (console) is exposed over TCP port 5000:</p> <pre><code># from container host\ntelnet &lt;node-name&gt; 5000\n</code></pre> <p>You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p> <p>Note</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-sros/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Nokia SR OS.</p> <p>The interface naming convention is: <code>1/1/X</code>, where <code>X</code> is the port number.</p> <p>Warning</p> <p>Nokia SR OS nodes currently only support the simplified interface alias <code>1/1/X</code>, where X denotes the port number. Multi-chassis, multi-linecard setups, and channelized interfaces are not supported by interface aliasing at the moment, and you must fall back to the old <code>ethX</code>-based naming scheme (see below) in these scenarios.</p> <p>Data port numbering starts at <code>1</code>, like one would normally expect in the NOS.</p> <p>With that naming convention in mind:</p> <ul> <li><code>1/1/1</code> - first data port available</li> <li><code>1/1/2</code> - second data port, and so on...</li> </ul> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Nokia SR OS VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>1/1/1</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>1/1/2</code> and so on)</li> </ul> <p>When containerlab launches Nokia SR OS node the primary BOF interface gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Nokia SR OS using containerlab's assigned IP.</p> <p>Data interfaces <code>1/1/1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p> <p>Nokia SR OS container uses the following mapping for its interfaces:</p> <p>Interfaces can be defined in a non-sequential way, for example:</p> <pre><code>  links:\n    # sr1 port 3 is connected to sr2 port 5\n    - endpoints: [\"sr1:1/1/3\", \"sr2:1/1/5\"] #(1)!\n</code></pre> <ol> <li>Or <code>endpoints: [\"sr1:eth3\", \"sr2:eth5\"]</code> in the Linux interface naming scheme.</li> </ol>","boost":4},{"location":"manual/kinds/vr-sros/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-sros/#variants","title":"Variants","text":"<p>Virtual SR OS simulator can be run in multiple HW variants as explained in the vSIM installation guide.</p> <p>Nokia SR OS container images come with pre-packaged SR OS variants as defined in the upstream repo as well as support custom variant definition. The pre-packaged variants are identified by the variant name and come up with cards and mda already configured. On the other hand, custom variants give users total flexibility in emulated hardware configuration, but cards and MDAs must be configured manually.</p> <p>To make Nokia SR OS to boot in one of the packaged variants, set the type to one of the predefined variant values:</p> <pre><code>topology:\n  nodes:\n    sros:\n      kind: nokia_sros\n      image: vrnetlab/nokia_sros:20.10.R1\n      type: sr-1s # if omitted, the default sr-1 variant will be used\n      license: license-sros20.txt\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#custom-variants","title":"Custom variants","text":"<p>A custom variant can be defined by specifying the TIMOS line for the control plane and line card components:</p> <pre><code>type: &gt;- # (1)!\n  cp: cpu=2 ram=4 chassis=ixr-e slot=A card=cpm-ixr-e ___\n  lc: cpu=2 ram=4 max_nics=34 chassis=ixr-e slot=1 card=imm24-sfp++8-sfp28+2-qsfp28 mda/1=m24-sfp++8-sfp28+2-qsfp28\n</code></pre> <ol> <li> <p>for distributed chassis CPM and IOM are indicated with markers <code>cp:</code> and <code>lc:</code>.</p> <p>notice the delimiter string <code>___</code> that must be present between CPM and IOM portions of a custom variant string.</p> <p><code>max_nics</code> value must be set in the <code>lc</code> part and specifies a maximum number of network interfaces this card will be equipped with.</p> <p>Memory <code>mem</code> is provided in GB.</p> </li> </ol> <p>It is possible to define a custom variant with multiple line cards; just repeat the <code>lc</code> portion of a type. Note that each line card is a separate VM, increasing pressure on the host running such a node. You may see some issues starting multi line card nodes due to the VMs being moved between CPU cores unless a cpu-set is used.</p> distributed chassis with multiple line cards<pre><code>type: &gt;-\n  cp: cpu=2 min_ram=4 chassis=sr-7 slot=A card=cpm5 ___\n  lc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=1 card=iom4-e mda/1=me6-10gb-sfp+ ___\n  lc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=2 card=iom4-e mda/1=me6-10gb-sfp+\n</code></pre> How to define links in a multi line card setup? <p>When a node uses multiple line cards users should pay special attention to the way links are defined in the topology file. As explained in the interface naming section, SR OS nodes use <code>ethX</code> notation for their interfaces, where <code>X</code> denotes a port number on a line card/MDA.</p> <p>Things get a little more tricky when multiple line cards are provided. First, every line card must be defined with a <code>max_nics</code> property that serves a simple purpose - identify how many ports at maximum this line card can bear. In the example above both line cards are equipped with the same IOM/MDA and can bear 6 ports at max. Thus, <code>max_nics</code> is set to 6.</p> <p>Another significant value of a line card definition is the <code>slot</code> position. Line cards are inserted into slots, and slot 1 comes before slot 2, and so on.</p> <p>Knowing the slot number and the maximum number of ports a line card has, users can identify which indexes they need to use in the <code>link</code> portion of a topology to address the right port of a chassis. Let's use the following example topology to explain how this all maps together:</p> <pre><code>topology:\n  nodes:\n    R1:\n      kind: nokia_sros\n      image: nokia_sros:22.7.R2\n      type: &gt;-\n        cp: cpu=2 min_ram=4 chassis=sr-7 slot=A card=cpm5 ___\n        lc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=1 card=iom4-e mda/1=me6-10gb-sfp+ ___\n        lc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=2 card=iom4-e mda/1=me6-10gb-sfp+\n    R2:\n      kind: nokia_sros\n      image: nokia_sros:22.7.R2\n      type: &gt;-\n        cp: cpu=2 min_ram=4 chassis=sr-7 slot=A card=cpm5 ___\n        lc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=1 card=iom4-e mda/1=me6-10gb-sfp+ ___\n        lc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=2 card=iom4-e mda/1=me6-10gb-sfp+\n\n  links:\n  - endpoints: [\"R1:eth1\", \"R2:eth3\"]\n  - endpoints: [\"R1:eth7\", \"R2:eth8\"]\n</code></pre> <p>Starting with the first pair of endpoints <code>R1:eth1 &lt;--&gt; eth3:R2</code>; we see that port1 of R1 is connected with port3 of R2. Looking at the slot information and <code>max_nics</code> value of 6 we see that the linecard in slot 1 can host maximum 6 ports. This means that ports from 1 till 6 belong to the line card equipped in slot=1. Consequently, links ranging from <code>eth1</code> to <code>eth6</code> will address the ports of that line card.</p> <p>The second pair of endpoints <code>R1:eth7 &lt;--&gt; eth8:R2</code> addresses the ports on a line card equipped in the slot 2. This is driven by the fact that the first six interfaces belong to line card in slot 1 as we just found out. This means that our second line card that sits in slot 2 and has as well six ports, will be addressed by the interfaces <code>eth7</code> till <code>eth12</code>, where <code>eth7</code> is port1 and <code>eth12</code> is port6.</p> <p>An integrated variant is provided with a simple TIMOS line:</p> <pre><code>type: \"cpu=2 ram=4 slot=A chassis=ixr-r6 card=cpiom-ixr-r6 mda/1=m6-10g-sfp++4-25g-sfp28\" # (1)!\n</code></pre> <ol> <li>No <code>cp</code> nor <code>lc</code> markers are needed to define an integrated variant.</li> </ol>","boost":4},{"location":"manual/kinds/vr-sros/#node-configuration","title":"Node configuration","text":"<p>Nokia SR OS nodes come up with a basic \"blank\" configuration where only the card/mda are provisioned, as well as the management interfaces such as Netconf, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-sros/#user-defined-config","title":"User-defined config","text":"<p>SR OS nodes launched with hellt/vrnetlab come up with some basic configuration that configures the management interfaces, line cards, mdas and power modules. This configuration is applied right after the node is booted.</p> <p>Since this initial configuration is meant to provide a bare minimum configuration to make the node operational, users will likely want to apply their own configuration to the node to enable some features or to configure some interfaces. This can be done by providing a user-defined configuration file using <code>startup-config</code> property of the node/kind.</p>","boost":4},{"location":"manual/kinds/vr-sros/#full-startup-config","title":"Full startup-config","text":"<p>When a user provides a path to a file that has a complete configuration for the node, containerlab will copy that file to the lab directory for that specific node under the <code>/tftpboot/config.txt</code> name and mount that dir to the container. This will result in this config to act as a startup-config for the node:</p> <pre><code>name: sros_lab\ntopology:\n  nodes:\n    sros:\n      kind: nokia_sros\n      startup-config: myconfig.txt\n</code></pre> <p>Note</p> <p>With the above configuration, the node will boot with the configuration specified in <code>myconfig.txt</code>, no other configuration will be applied. You have to provision interfaces, cards, power-shelves, etc. yourself.</p>","boost":4},{"location":"manual/kinds/vr-sros/#partial-startup-config","title":"Partial startup-config","text":"<p>Quite often it is beneficial to have a partial configuration that will be applied on top of the default configuration that containerlab applies. For example, users might want to add some services on top of the default configuration provided by containerlab and do not want to have the full configuration file.</p> <p>This can be done by providing a partial configuration file that will be applied on top of the default configuration. The partial configuration file must have <code>.partial</code> string in its name, for example, <code>myconfig.partial.txt</code>.</p> <pre><code>name: sros_lab\ntopology:\n  nodes:\n    sros:\n      kind: nokia_sros\n      startup-config: myconfig.partial.txt\n</code></pre> <p>The partial config can contain configuration in a MD-CLI syntax that is accepted in the configuration mode of the SR OS. The way partial config is applied is by sending lines from the partial config file to the SR OS via SSH. A few important things to note:</p> <ol> <li>Entering the configuration mode is not required, containerlab will do that for you. <code>edit-config exclusive</code> mode is used by containerlab.</li> <li><code>commit</code> command must not be included in the partial config file, containerlab will do that for you.</li> </ol> <p>Both <code>flat</code> and normal syntax can be used in the partial config file. For example, the following partial config file adds a static route to the node in the regular CLI syntax:</p> <pre><code>    configure {\n       router \"Base\" {\n           static-routes {\n               route 192.168.200.200/32 route-type unicast {\n                   next-hop \"192.168.0.1\" {\n                       admin-state enable\n                   }\n               }\n           }\n       }\n    }\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#remote-partial-files","title":"Remote partial files","text":"<p>It is possible to provide a partial config file that is located on a remote http(s) server. This can be done by providing a URL to the file. The URL must start with <code>http://</code> or <code>https://</code> and must point to a file that is accessible from the containerlab host.</p> <p>Note</p> <p>The URL must have <code>.partial</code> in its name:</p> <pre><code>name: sros_lab\ntopology:\n  nodes:\n    sros:\n      kind: nokia_sros\n      startup-config: https://gist.com/&lt;somehash&gt;/staticroute.partial.cfg\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#embedded-partial-files","title":"Embedded partial files","text":"<p>Users can also embed the partial config in the topology file itself, making it a hermetic artifact that can be shared with others. This can be done by using multiline string in YAML:</p> <pre><code>name: sros_lab\ntopology:\n  nodes:\n    sros:\n      kind: nokia_sros\n      startup-config: | #(1)!\n        /configure system location \"I am an embedded config\"\n</code></pre> <ol> <li>It is mandatory to use YAML's multiline string syntax to denote that the string below is a partial config and not a file.</li> </ol> <p>Embedded partial configs will persist on containerlab's host and use the same directory as the remote startup-config files.</p>","boost":4},{"location":"manual/kinds/vr-sros/#configuration-save","title":"Configuration save","text":"<p>Containerlab's <code>save</code> command will perform a configuration save for <code>Nokia SR OS</code> nodes via Netconf. The configuration will be saved under <code>config.txt</code> file and can be found at the node's directory inside the lab parent directory:</p> <pre><code># assuming the lab name is \"cert01\"\n# and node name is \"sr\"\ncat clab-cert01/sr/tftpboot/config.txt\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#boot-options-file","title":"Boot Options File","text":"<p>By default <code>nokia_sros</code> nodes boot up with a pre-defined \"Boot Options File\" (BOF). This file includes boot settings including:</p> <ul> <li>license file location</li> <li>config file location</li> </ul> <p>When the node is up and running you can make changes to this BOF. One popular example of such changes is the addition of static-routes to reach external networks from within the SR OS node. Although you can save the BOF from within the SROS system, the location the file is written to is not persistent across container restarts. It is also not possible to define a BOF target location. A workaround for this limitation is to automatically execute a CLI script that configures BOF once the system boots.</p> <p>SR OS has an option (introduced in SR OS 16.0.R1) to automatically execute a script upon successful boot. This option is accessible in SR OS by the <code>/configure system boot-good-exec</code> MD-CLI path:</p> <pre><code>[pr:/configure]\nA:admin@sros1# system boot-good-exec ?\n\n boot-good-exec &lt;string&gt;\n &lt;string&gt;  - &lt;1..180 characters&gt;\n\n    CLI script file to execute following successful boot-up\n</code></pre> <p>By mounting a script to SR OS container node and using the <code>boot-good-exec</code> option, users can make changes to the BOF the second the node boots and thus complete the task of having a somewhat persistent BOF.</p> <p>As an example the following SR OS MD-CLI script was created to persist custom static routes to the BOF:</p> <pre><code>########################################\n# Configuring static management routes\n########################################\n/bof private\nrouter \"management\" static-routes route 10.0.0.0/24 next-hop 172.31.255.29\nrouter \"management\" static-routes route 10.0.1.0/24 next-hop 172.31.255.29\nrouter \"management\" static-routes route 192.168.0.0/24 next-hop 172.31.255.29\nrouter \"management\" static-routes route 172.20.20.0/24 next-hop 172.31.255.29\ncommit\nexit all\n</code></pre> <p>This script is then placed somewhere on the disk, for example in the containerlab's topology root directory, and mounted to <code>nokia_sros</code> node tftpboot directory using binds property:</p> <pre><code>  nodes:\n    sros1:\n      mgmt-ipv4: [mgmt-ip]\n      kind: nokia_sros\n      image: [container-image-repo]\n      type: sr-1s\n      license: license-sros.txt\n      binds:\n        - post-boot-exec.cfg:/tftpboot/post-boot-exec.cfg #(1)!\n</code></pre> <ol> <li><code>post-boot-exec.cfg</code> file contains the script referenced above and it is mounted to <code>/tftpboot</code> directory that is available in SR OS node.</li> </ol> <p>Once the script is mounted to the node, users need to instruct SR OS to execute the script upon successful boot. This is done by adding the following configuration line on SR OS MD-CLI:</p> <pre><code>[pr:/configure system]\nA:admin@sros1# info | match boot-goo\n    boot-good-exec \"tftp://172.31.255.29/post-boot-exec.cfg\" #(1)!\n</code></pre> <ol> <li>The tftpboot location is always at <code>tftp://172.31.255.29/</code> address and the name of the file needs to match the file you used in the binds instruction.</li> </ol> <p>By combining file bindings and the automatic script execution of SROS it is possible to create a workaround for persistent BOF settings.</p>","boost":4},{"location":"manual/kinds/vr-sros/#ssh-keys","title":"SSH keys","text":"<p>Containerlab v0.48.0+ supports SSH key injection into the Nokia SR OS nodes. First containerlab retrieves all public keys from <code>~/.ssh</code><sup>1</sup> directory and <code>~/.ssh/authorizde_keys</code> file, then it retrieves public keys from the ssh agent if one is running.</p> <p>Next it will filter out public keys that are not of RSA/ECDSA type. The remaining valid public keys will be configured for the admin user of the Nokia SR OS node using key IDs from 32 downwards<sup>2</sup>. This will enable key-based authentication next time you connect to the node.</p> Skipping keys injection <p>If you want to disable this feature (e.g. when using classic CLI mode), you can do so by setting the <code>CLAB_SKIP_SROS_SSH_KEY_CONFIG=true</code> env variable:</p> <pre><code>sudo CLAB_SKIP_SROS_SSH_KEY_CONFIG=true -E clab deploy -t &lt;topo-file&gt;\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#license","title":"License","text":"<p>Path to a valid license must be provided for all Nokia SR OS nodes with a <code>license</code> directive.</p> <p>If your SR OS license file is issued for a specific UUID, you can define it with custom type definition:</p> <pre><code># note, typically only the cp needs the UUID defined.\ntype: \"cp: uuid=00001234-5678-9abc-def1-000012345678 cpu=4 ram=6 slot=A chassis=SR-12 card=cpm5 ___ lc: cpu=4 ram=6 max_nics=36 slot=1 chassis=SR-12 card=iom3-xp-c mda/1=m10-1gb+1-10gb\"\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For Nokia SR OS kind containerlab creates <code>tftpboot</code> directory where the license file will be copied.</p>","boost":4},{"location":"manual/kinds/vr-sros/#lab-examples","title":"Lab examples","text":"<p>The following labs feature Nokia SR OS node:</p> <ul> <li>SR Linux and vr-sros</li> </ul> <ol> <li> <p><code>~</code> is the home directory of the user that runs containerlab.\u00a0\u21a9</p> </li> <li> <p>If a user wishes to provide a custom startup-config with public keys defined, then they should use key IDs from 1 onwards. This will minimize chances of key ID collision causing containerlab to overwrite user-defined keys.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/vr-veos/","title":"Arista vEOS","text":"<p>Arista vEOS virtualized router is identified with <code>arista_veos</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Arista vEOS nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-veos/#managing-arista-veos-nodes","title":"Managing Arista vEOS nodes","text":"<p>Note</p> <p>Containers with vEOS inside will take ~4min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Arista vEOS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgNMI <p>to connect to a <code>bash</code> shell of a running Arista vEOS container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vEOS CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt;:6030 --insecure \\\n-u admin -p admin \\\ncapabilities\n</code></pre> Note, gNMI service runs over 6030 port.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-veos/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Arista vEOS.</p> <p>The interface naming convention is: <code>Ethernet1/X</code> (or <code>Et1/X</code>), where <code>X</code> is the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Ethernet1/1</code> - first data port available</li> <li><code>Ethernet1/2</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>1</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Arista vEOS VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>Ethernet1/1</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>Ethernet1/2</code> and so on)</li> </ul> <p>When containerlab launches Arista vEOS node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Arista vEOS using containerlab's assigned IP.</p> <p>Data interfaces <code>Ethernet1/1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-veos/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-veos/#node-configuration","title":"Node configuration","text":"<p>Arista vEOS nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>admin</code> user and management interfaces such as NETCONF, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-veos/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vEOS nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: arista_veos\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/","title":"Juniper vJunosEvolved","text":"<p>Juniper vJunosEvolved is a virtualized PTX10001 router identified with <code>juniper_vjunosevolved</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Juniper vJunosEvolved nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#how-to-obtain-the-image","title":"How to obtain the image","text":"<p>The qcow2 image can be freely downloaded from the Juniper support portal without a Juniper account and built with vrnetlab.</p>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#managing-juniper-vjunosevolved-nodes","title":"Managing Juniper vJunosEvolved nodes","text":"<p>Note</p> <p>Containers with vJunosEvolved inside will take ~15min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vJunosEvolved node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONF <p>to connect to a <code>bash</code> shell of a running Juniper vJunosEvolved container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vJunosEvolved CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Juniper vJunosEvolved.</p> <p>The interface naming convention is: <code>et-0/0/X</code> (or <code>ge-0/0/X</code>, <code>xe-0/0/X</code>, all are accepted), where X denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>et-0/0/0</code> - first data port available</li> <li><code>et-0/0/1</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>0</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Juniper vJunosEvolved VM:</p> <p>Juniper vJunosEvolved container can have up to 17 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to a first data port of vJunosEvolved VM, which is <code>et-0/0/0</code> and not <code>et-0/0/1</code>.</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Juniper vJunosEvolved node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Juniper vJunosEvolved using containerlab's assigned IP.</p> <p>Data interfaces <code>et-0/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#node-configuration","title":"Node configuration","text":"<p>Juniper vJunosEvolved nodes come up with a basic configuration supplied by a mountable configuration disk to the main VM image. Users, management interfaces, and protocols such as SSH and NETCONF are configured.</p>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vJunosEvolved nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: juniper_vjunosevolved\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#lab-examples","title":"Lab examples","text":"<p>The following labs feature the Juniper vJunosEvolved node:</p> <ul> <li>SR Linux and Juniper vJunosEvolved</li> </ul>","boost":4},{"location":"manual/kinds/vr-vjunosevolved/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ul> <li>To check the boot log, use <code>docker logs -f &lt;node-name&gt;</code>.</li> </ul>","boost":4},{"location":"manual/kinds/vr-vjunosrouter/","title":"Juniper vJunos-router","text":"<p>Juniper vJunos-router is a virtualized MX router, a single-VM version of the vMX that requires no feature licenses and is meant for lab/testing use. It is identified with <code>juniper_vjunosrouter</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Juniper vJunos-router nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-vjunosrouter/#how-to-obtain-the-image","title":"How to obtain the image","text":"<p>The qcow2 image can be freely downloaded from the Juniper support portal without a Juniper account and built with vrnetlab.</p>","boost":4},{"location":"manual/kinds/vr-vjunosrouter/#managing-juniper-vjunos-router-nodes","title":"Managing Juniper vJunos-router nodes","text":"<p>Note</p> <p>Containers with vJunos-router inside can take up to ~5-10min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vJunos-router node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONFConsole <p>to connect to a <code>bash</code> shell of a running Juniper vJunos-router container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vJunos-router CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>serial port (console) is exposed over telnet TCP port 5000: <pre><code>telnet &lt;node-name&gt; 5000\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vjunosrouter/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Juniper vJunos-router.</p> <p>The interface naming convention is: <code>et-0/0/X</code> (or <code>ge-0/0/X</code>, <code>xe-0/0/X</code>, all are accepted), where X denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>et-0/0/0</code> - first data port available</li> <li><code>et-0/0/1</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>0</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Juniper vJunos-router VM:</p> <p>Juniper vJunosEvolved container can have up to 17 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to a first data port of vJunosEvolved VM, which is <code>et-0/0/0</code> and not <code>et-0/0/1</code>.</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Juniper vJunos-router node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Juniper vJunos-router using containerlab's assigned IP.</p> <p>Data interfaces <code>et-0/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-vjunosrouter/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vjunosrouter/#node-configuration","title":"Node configuration","text":"<p>Juniper vJunos-router nodes come up with a basic configuration supplied by a mountable configuration disk to the main VM image. Users, management interfaces, and protocols such as SSH and NETCONF are configured.</p>","boost":4},{"location":"manual/kinds/vr-vjunosrouter/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vJunos-router nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: juniper_vjunosrouter\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vjunosrouter/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ul> <li>vJunos-router requires Linux kernel 4.17+</li> <li>To check the boot log, use <code>docker logs -f &lt;node-name&gt;</code>.</li> </ul>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/","title":"Juniper vJunos-switch","text":"<p>Juniper vJunos-switch is a virtualized EX9214 switch identified with <code>juniper_vjunosswitch</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Juniper vJunos-switch nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#how-to-obtain-the-image","title":"How to obtain the image","text":"<p>The qcow2 image can be freely downloaded from the Juniper support portal without a Juniper account and built with vrnetlab.</p>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#managing-juniper-vjunos-switch-nodes","title":"Managing Juniper vJunos-switch nodes","text":"<p>Note</p> <p>Containers with vJunos-switch inside will take ~15min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vJunos-switch node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONF <p>to connect to a <code>bash</code> shell of a running Juniper vJunos-switch container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vJunos-switch CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Juniper vJunos-switch.</p> <p>The interface naming convention is: <code>et-0/0/X</code> (or <code>ge-0/0/X</code>, <code>xe-0/0/X</code>, all are accepted), where X denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>et-0/0/0</code> - first data port available</li> <li><code>et-0/0/1</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>0</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Juniper vJunos-switch VM:</p> <p>Juniper vJunosEvolved container can have up to 17 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to a first data port of vJunosEvolved VM, which is <code>et-0/0/0</code> and not <code>et-0/0/1</code>.</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Juniper vJunos-switch node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Juniper vJunos-switch using containerlab's assigned IP.</p> <p>Data interfaces <code>et-0/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#node-configuration","title":"Node configuration","text":"<p>Juniper vJunos-switch nodes come up with a basic configuration supplied by a mountable configuration disk to the main VM image. Users, management interfaces, and protocols such as SSH and NETCONF are configured.</p>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vJunos-switch nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: juniper_vjunosswitch\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#lab-examples","title":"Lab examples","text":"<p>The following labs feature the Juniper vJunos-switch node:</p> <ul> <li>SR Linux and Juniper vJunos-switch</li> </ul>","boost":4},{"location":"manual/kinds/vr-vjunosswitch/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ul> <li>vJunos-switch requires Linux kernel 4.17+</li> <li>To check the boot log, use <code>docker logs -f &lt;node-name&gt;</code>.</li> </ul>","boost":4},{"location":"manual/kinds/vr-vmx/","title":"Juniper vMX","text":"<p>Juniper vMX virtualized router is identified with <code>juniper_vmx</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Juniper vMX nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#managing-juniper-vmx-nodes","title":"Managing Juniper vMX nodes","text":"<p>Note</p> <p>Containers with vMX inside will take ~7min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vMX node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONFgNMI <p>to connect to a <code>bash</code> shell of a running Juniper vMX container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vMX CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u admin -p admin@123 \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vmx/#interface-naming","title":"Interface naming","text":"<p>vMX nodes use the interface naming convention <code>ge-0/0/X</code> (or <code>et-0/0/X</code>, <code>xe-0/0/X</code>, all are accepted), where X denotes the port number.</p> <p>Info</p> <p>Data port numbering starts at <code>0</code>, like one would normally expect in the NOS.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#interfaces-mapping","title":"Interfaces mapping","text":"<p>You can use interfaces names in the topology file like they appear in Juniper vMX.</p> <p>The interface naming convention is: <code>et-0/0/X</code> (or <code>ge-0/0/X</code>, <code>xe-0/0/X</code>, all are accepted), where X denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>et-0/0/0</code> - first data port available</li> <li><code>et-0/0/1</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>0</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Juniper vMX VM:</p> <p>Juniper vJunosEvolved container can have up to 17 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to a first data port of vJunosEvolved VM, which is <code>et-0/0/0</code> and not <code>et-0/0/1</code>.</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Juniper vMX node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Juniper vMX using containerlab's assigned IP.</p> <p>Data interfaces <code>et-0/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vmx/#node-configuration","title":"Node configuration","text":"<p>Juniper vMX nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>admin</code> users and management interfaces such as NETCONF, SNMP, gNMI.</p> <p>Starting with hellt/vrnetlab v0.8.2 VMX will make use of the management VRF<sup>1</sup>.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vMX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: juniper_vmx\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#lab-examples","title":"Lab examples","text":"<p>The following labs feature Juniper vMX node:</p> <ul> <li>SR Linux and Juniper vMX</li> </ul>","boost":4},{"location":"manual/kinds/vr-vmx/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ul> <li>when listing docker containers, Juniper vMX containers will always report unhealthy status. Do not rely on this status.</li> <li>vMX requires Linux kernel 4.17+</li> <li>To check the boot log, use <code>docker logs -f &lt;node-name&gt;</code>.</li> </ul> <ol> <li> <p>https://github.com/hellt/vrnetlab/pull/98 \u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/vr-vqfx/","title":"Juniper vQFX","text":"<p>Juniper vQFX virtualized router is identified with <code>juniper_vqfx</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Warning</p> <p>The public vQFX image that is downloadable from the Juniper portal mentions version 20.2, but in fact, it is a 19.4 system. Until this issue is fixed (and it seems no one cares), rename the downloaded qcow2 file to mention the 19.4 version before building the container image.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#managing-juniper-vqfx-nodes","title":"Managing Juniper vQFX nodes","text":"<p>Note</p> <p>Containers with vQFX inside will take ~7min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vQFX node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONF <p>to connect to a <code>bash</code> shell of a running Juniper vQFX container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vQFX CLI (password <code>admin@123</code>) <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>Looking for contributions.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vqfx/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Juniper vQFX.</p> <p>The interface naming convention is: <code>et-0/0/X</code> (or <code>ge-0/0/X</code>, <code>xe-0/0/X</code>, all are accepted), where X denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>et-0/0/0</code> - first data port available</li> <li><code>et-0/0/1</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>0</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Juniper vQFX VM:</p> <p>Juniper vJunosEvolved container can have up to 17 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to a first data port of vJunosEvolved VM, which is <code>et-0/0/0</code> and not <code>et-0/0/1</code>.</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Juniper vQFX node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Juniper vQFX using containerlab's assigned IP.</p> <p>Data interfaces <code>et-0/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vqfx/#node-configuration","title":"Node configuration","text":"<p>Juniper vQFX nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>admin</code> user with the provided password.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vQFX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: juniper_vqfx\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#lab-examples","title":"Lab examples","text":"<p>Looking for contributions.</p>","boost":4},{"location":"manual/kinds/vr-vsrx/","title":"Juniper vSRX","text":"<p>Juniper vSRX virtualized firewall is identified with <code>juniper_vsrx</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/vr-vsrx/#managing-juniper-vsrx-nodes","title":"Managing Juniper vSRX nodes","text":"<p>Note</p> <p>Containers with vSRX inside will take ~7min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vSRX node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONF <p>to connect to a <code>bash</code> shell of a running Juniper vSRX container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vSRX CLI (password <code>admin@123</code>) <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>Coming soon</p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vsrx/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Juniper vSRX.</p> <p>The interface naming convention is: <code>et-0/0/X</code> (or <code>ge-0/0/X</code>, <code>xe-0/0/X</code>, all are accepted), where X denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>et-0/0/0</code> - first data port available</li> <li><code>et-0/0/1</code> - second data port, and so on...</li> </ul> <p>Note</p> <p>Data port numbering starts at <code>0</code>.</p> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Juniper vSRX VM:</p> <p>Juniper vJunosEvolved container can have up to 17 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to a first data port of vJunosEvolved VM, which is <code>et-0/0/0</code> and not <code>et-0/0/1</code>.</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Juniper vSRX node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Juniper vSRX using containerlab's assigned IP.</p> <p>Data interfaces <code>et-0/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-vsrx/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vsrx/#node-configuration","title":"Node configuration","text":"<p>Juniper vSRX nodes come up with a basic configuration where only the control plane and line cards are provisioned and the <code>admin</code> user with the provided password.</p>","boost":4},{"location":"manual/kinds/vr-vsrx/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vSRX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: juniper_vsrx\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob, containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started. Thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vsrx/#lab-examples","title":"Lab examples","text":"<p>The following simple lab consists of two Linux hosts connected via one vSRX:</p> <ul> <li>SR Linux and cRPD</li> </ul>","boost":4},{"location":"manual/kinds/vr-xrv/","title":"Cisco XRv","text":"<p>Cisco XRv virtualized router is identified with <code>cisco_xrv</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Cisco XRv nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled.</p> <p>Warning</p> <p>XRv image is discontinued by Cisco and superseded by XRv 9000 image. It was added to containerlab because the image is lightweight, compared to XRv9k. If recent features are needed, use Cisco XRv9k kind.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#managing-cisco-xrv-nodes","title":"Managing Cisco XRv nodes","text":"<p>Note</p> <p>Containers with XRv inside will take ~5min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Cisco XRv node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONFgNMI <p>to connect to a <code>bash</code> shell of a running Cisco XRv container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the XRv CLI <pre><code>ssh clab@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh clab@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u clab -p clab@123 \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>clab:clab@123</code></p>","boost":4},{"location":"manual/kinds/vr-xrv/#interface-naming","title":"Interface naming","text":"<p>Cisco XRv nodes use the interface naming convention <code>GigabitEthernet0/0/0/X</code> (or <code>Gi0/0/0/X</code>, both are accepted), where X denotes the port number.</p> <p>Info</p> <p>Data port numbering starts at <code>0</code>, like one would normally expect in the NOS.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#interfaces-mapping","title":"Interfaces mapping","text":"<p>Cisco XRv container can have up to 90 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of XRv line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Cisco XRv node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-xrv/#node-configuration","title":"Node configuration","text":"<p>Cisco XRv nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>clab</code> user and management interfaces such as NETCONF, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make XRv nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: cisco_xrv\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#lab-examples","title":"Lab examples","text":"<p>The following labs feature Cisco XRv node:</p> <ul> <li>SR Linux and Cisco XRv</li> </ul>","boost":4},{"location":"manual/kinds/vr-xrv/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ul> <li>LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.</li> </ul>","boost":4},{"location":"manual/kinds/vr-xrv9k/","title":"Cisco XRv9k","text":"<p>Cisco XRv9k virtualized router is identified with <code>cisco_xrv9k</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Cisco XRv9k nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled.</p> <p>Resource requirements</p> <p>XRv9k node is a resource hungry image. As of XRv9k 7.2.1 version the minimum resources should be set to 2vcpu/14GB. To be safe the defaults used in containerlab are 2vCPU/16G RAM. Image may take 25 minutes to fully boot, be patient. You can monitor the loading status with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>If you need to tune the allocated resources, you can do so with setting <code>VCPU</code> and <code>RAM</code> environment variables for the node. For example, to set 4vcpu/16GB for the node:</p> <pre><code>    iosxr:\n      kind: cisco_xrv9k\n      image: vr-xrv9k:7.10.1\n      env:\n        VCPU: 4\n        RAM: 16384\n</code></pre>","boost":4},{"location":"manual/kinds/vr-xrv9k/#managing-cisco-xrv9k-nodes","title":"Managing Cisco XRv9k nodes","text":"<p>Cisco XRv9k node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONFgNMI <p>to connect to a <code>bash</code> shell of a running Cisco XRv9k container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the XRv9kCLI <pre><code>ssh clab@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh clab@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u clab -p clab@123 \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>clab:clab@123</code></p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#interface-naming","title":"Interface naming","text":"<p>You can use interfaces names in the topology file like they appear in Cisco XRv9k.</p> <p>The interface naming convention is:</p> <ul> <li><code>GigabitEthernet0/0/0/X</code> or <code>Gi0/0/0/X</code></li> <li><code>TenGigabitEthernet0/0/0/X</code>, <code>TenGigE0/0/0X</code> or <code>Te0/0/0/X</code></li> </ul> <p>where X denotes the port number.</p> <p>With that naming convention in mind:</p> <ul> <li><code>Gi0/0/0/0</code> - first data port available</li> <li><code>Gi0/0/0/1</code> - second data port, and so on...</li> </ul> <p>Note</p> <ol> <li>Data port numbering starts at <code>0</code>.</li> <li>Data interfaces may take 10+ minutes to come up, please be patient.</li> <li>Cisco XRv9k can have up to 90 interfaces.</li> </ol> <p>The example ports above would be mapped to the following Linux interfaces inside the container running the Cisco XRv9k VM:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network.</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM (rendered as <code>Gi0/0/0/0</code>)</li> <li><code>eth2+</code> - second and subsequent data interfaces, mapped to the second and subsequent data ports of the VM (rendered as <code>Gi0/0/0/1</code> and so on)</li> </ul> <p>When containerlab launches Cisco XRv9k node the management interface of the VM gets assigned <code>10.0.0.15/24</code> address from the QEMU DHCP server. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cisco XRv9k using containerlab's assigned IP.</p> <p>Data interfaces <code>Gi0/0/0/0+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-xrv9k/#node-configuration","title":"Node configuration","text":"<p>Cisco XRv9k nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>clab</code> user and management interfaces such as NETCONF, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make XRv9k nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\n  nodes:\n    node:\n      kind: cisco_xrv9k\n      startup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#lab-examples","title":"Lab examples","text":"<p>The following labs feature Cisco XRv9k node:</p> <ul> <li>SR Linux and Cisco XRv9k</li> </ul>","boost":4},{"location":"manual/kinds/xrd/","title":"Cisco XRd","text":"<p>Cisco XRd Network OS is identified with <code>xrd</code> or <code>cisco_xrd</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a node.</p> <p>XRd comes in two variants:</p> <ul> <li>control-plane</li> <li>vrouter</li> </ul> <p>Containerlab supports only the control-plane flavor of XRd, as it allows to build topologies using virtual interfaces, whereas vrouter requires PCI interfaces to be attached to it.</p> <p>Tip</p> <p>Consult with XRd Tutorials series to get an in-depth understanding of XRd requirements and capabilities.</p>","boost":4},{"location":"manual/kinds/xrd/#getting-xrd","title":"Getting XRd","text":"<p>XRd image is available for download only for users who have an active service account<sup>1</sup>.</p>","boost":4},{"location":"manual/kinds/xrd/#host-server-requirements","title":"Host server requirements","text":"<p>Cisco xrdocs recommends to increase <code>inotify.max_user_instances</code> and <code>inotify.max_user_watches</code>.</p> <p>You can do this by executing the following:</p> <pre><code>sysctl -w fs.inotify.max_user_instances=64000\nsysctl -w fs.inotify.max_user_watches=64000\n</code></pre> <p>To make the settings persist reboots append <code>fs.inotify.max_user_instances=64000</code> and <code>fs.inotify.max_user_watches=64000</code> to <code>/etc/sysctl.conf</code>. You can use the following one-liner:</p> <pre><code>echo -e \"fs.inotify.max_user_instances=64000\\nfs.inotify.max_user_watches=64000\" | sudo tee -a /etc/sysctl.conf\n</code></pre> <p>Tip</p> <p>If using 10+ XRd nodes, you may need to increase the <code>fs.inotify.max_user_instances</code> and/or <code>fs.inotify.max_user_watches</code> even higher.</p>","boost":4},{"location":"manual/kinds/xrd/#managing-xrd-nodes","title":"Managing XRd nodes","text":"<p>There are several management interfaces supported by XRd nodes:</p> CLIbashSSHgNMINetconf <p>to connect to a XR CLI shell of a running XRd container: <pre><code>docker exec -it &lt;container-name/id&gt; /pkg/bin/xr_cli.sh\n</code></pre></p> <p>to connect to a <code>bash</code> shell of a running XRd container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p><code>ssh clab@&lt;container-name&gt;</code> Password: <code>clab@123</code></p> <p>gNMI server runs on <code>57400</code> port in the insecure mode (no TLS). Using gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt;:57400 --insecure \\\n  -u clab -p clab@123 \\\n  capabilities\n</code></pre></p> <p>Netconf server runs on <code>830</code> port: <pre><code>ssh clab@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default credentials: <code>clab:clab@123</code></p>","boost":4},{"location":"manual/kinds/xrd/#interfaces-mapping","title":"Interfaces mapping","text":"<p>XRd container uses the following mapping for its Linux interfaces<sup>2</sup>:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>Gi0-0-0-0</code> - first data interface mapped to <code>Gi0/0/0/0</code> internal interface.</li> <li><code>Gi0-0-0-N</code> - Nth data interface mapped to <code>Gi0/0/0/N</code> internal interface.</li> </ul> <p>When containerlab launches XRd node, it will set IPv4/6 addresses as assigned by docker to the <code>eth0</code> interface and XRd node will boot with these addresses configured for its <code>MgmtEth0</code>. Data interfaces <code>Gi0/0/0/N</code> need to be configured with IP addressing manually.</p> <pre><code>RP/0/RP0/CPU0:xrd#sh ip int br\nWed Dec 21 12:04:13.049 UTC\n\nInterface                      IP-Address      Status          Protocol Vrf-Name\nMgmtEth0/RP0/CPU0/0            172.20.20.5     Up              Up       default\n</code></pre>","boost":4},{"location":"manual/kinds/xrd/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/xrd/#node-configuration","title":"Node configuration","text":"<p>XRd nodes have a dedicated <code>config</code> directory that is used to persist the configuration of the node and expose internal directories of the NOS.</p> <p>For XRd nodes, containerlab exposes the following file layout of the node's lab directory:</p> <ul> <li><code>xr-storage</code> (dir): a directory that is mounted to <code>/xr-storage</code> path of the NOS and is used to persist changes made to the node as well as provides access to the logs and various runtime files.</li> <li><code>first-boot.cfg</code> - a configuration file in Cisco IOS-XR CLI format that the node boots with.</li> </ul>","boost":4},{"location":"manual/kinds/xrd/#default-node-configuration","title":"Default node configuration","text":"<p>It is possible to launch nodes of <code>cisco_xrd</code> kind with a basic config or to provide a custom config file that will be used as a startup config instead.</p> <p>When a node is defined without <code>startup-config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p> <pre><code># example of a topo file that does not define a custom config\n# as a result, the config will be generated from a template\n# and used by this node\nname: xrd\ntopology:\n  nodes:\n    xrd:\n      kind: cisco_xrd\n</code></pre>","boost":4},{"location":"manual/kinds/xrd/#user-defined-config","title":"User defined config","text":"<p>It is possible to make XRd nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>name: xrd\ntopology:\n  nodes:\n    xrd:\n      kind: cisco_xrd\n      startup-config: xrd.cfg\n</code></pre> <p>When a config file is passed via <code>startup-config</code> parameter it will be used during an initial lab deployment. However, a config file that might be in the lab directory of a node takes precedence over the startup-config<sup>3</sup>.</p> <p>With such topology file containerlab is instructed to take a file <code>xrd.cfg</code> from the current working directory and copy it to the lab directory for that specific node under the <code>/first-boot.cfg</code> name. This will result in this config acting as a startup-config for the node.</p> <p>To provide a user-defined config, take the default configuration template and add the necessary configuration commands without changing the rest of the file. This will result in proper automatic assignment of IP addresses to the management interface, as well as applying user-defined commands.</p> <p>Tip</p> <p>Check SR Linux and XRd lab example where startup configuration files are provided to both nodes to see it in action.</p>","boost":4},{"location":"manual/kinds/xrd/#configuration-persistency","title":"Configuration persistency","text":"<p>XRd nodes persist their configuration in <code>&lt;lab-directory&gt;/&lt;node-name&gt;/xr-storage</code> directory. When a user commits changes to XRd nodes using one of the management interfaces, they are kept in the configuration DB (but not exposed as a configuration file).</p> <p>This capability allows users to configure the XRd node, commit the changes, then destroy the lab (without using <code>--cleanup</code> flag to keep the lab dir intact) and on a subsequent deploy action, the node will boot with the previously saved configuration.</p>","boost":4},{"location":"manual/kinds/xrd/#known-issues-and-limitations","title":"Known issues and limitations","text":"<p>Note, that XRd requires elevated number of inotify resources. If you happen to see errors in the xrd bootlog about inotify resources, consult with this article on how to increase them.</p>","boost":4},{"location":"manual/kinds/xrd/#lab-examples","title":"Lab examples","text":"<p>The following labs feature XRd nodes:</p> <ul> <li>SR Linux and XRd</li> </ul> <ol> <li> <p>https://xrdocs.io/virtual-routing/tutorials/2022-08-22-xrd-images-where-can-one-get-them/ \u21a9</p> </li> <li> <p>It is not yet possible to manually assign interface mapping rules in containerlab for XRd nodes. PRs are welcome.\u00a0\u21a9</p> </li> <li> <p>if startup config needs to be enforced, either deploy a lab with <code>--reconfigure</code> flag, or use <code>enforce-startup-config</code> setting.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"rn/0.11.0/","title":"Release 0.11.0","text":"<p> 2021-03-09</p>"},{"location":"rn/0.11.0/#multi-node-labs-with-vxlan-tunneling","title":"Multi-node labs with VxLAN tunneling","text":"<p>Most containerlab users are fine with running labs within a single server/VM. Truthfully speaking, a VM with 32GB RAM can host a very decent lab with dozens of nodes<sup>1</sup>.</p> <p>But yet, it is not always the case, sometimes we have a shortage of resources or the workloads are quite heavyweighters themselves. In all these scenarios it would be really nice to scope containerlab out of a single server and make it reach far horizons.</p> <p>And there are several way of making containerlab nodes to reach nodes/systems outside of its container host:</p> <ul> <li>Exposing services/ports</li> <li>Exposing management network with routing</li> <li>Connecting remote nodes with bridging</li> <li>Creating VxLAN tunnels across the network</li> </ul> <p>To help you navigate these options we've created a multi-node labs documentation article that explains each approach in details.</p> <p>With 0.11.0 specifically, we add the last option in that list, which is most flexible and far-reaching - VxLAN Tunneling.</p> <p>To help containerlab users to provision VxLAN tunnels the helper commands have been created - vxlan create, vxlan delete.</p> <p>The multi-node lab provides a step-by-step coverage of how this all nicely play together and builds a playground to demonstrate the multi-node capabilities.</p>"},{"location":"rn/0.11.0/#openvswitch-ovs-bridges-support","title":"Openvswitch (ovs) bridges support","text":"<p>The versatility of containerlab is in its ability to run anywhere where linux runs and wire up anything that can be packaged in a docker container. But it is quite hard to package in a container a piece of a real hardware. Still, we need it more often than not to connect devices like traffic generators or physical chassis to containerlab labs.</p> <p>It was possible before with linux bridges, now we are adding support for ovs bridges that will allow for even more elaborated and flexible connectivity options.</p>"},{"location":"rn/0.11.0/#enhanced-topology-checks","title":"Enhanced topology checks","text":"<p>Gradually we add checks that will make containerlab fail early if anything in the topology file or the host system is not right.</p> <p>In this release containerlab will additionally perform these checks:</p> <ol> <li>if node name matches already existing container the deployment won't start</li> <li>if vrnetlab nodes are defined, but virtualization is not available the deployment won't start</li> <li>if &lt;2 vCPU is available a warning will be emitted. Some containerized NOSes demand 2vCPU, so running with 1vCPU is discouraged</li> <li>abrupt deployment if host link referenced in topo file already exists in the host</li> </ol>"},{"location":"rn/0.11.0/#shell-completions","title":"Shell completions","text":"<p>Thanks to our contributor @steiler we now have shell completions, so you can navigate containerlab CLI with guidance and confidence.</p>"},{"location":"rn/0.11.0/#manual-installation","title":"Manual installation","text":"<p>All these time we relied on package managers to handle containerlab installation. Having support for deb/rpm packages is enough for most of us, but those nerds on Arch and Gentoo .</p> <p>For minorities we added </p>"},{"location":"rn/0.11.0/#vrnetlab-changes","title":"Vrnetlab changes","text":"<p>Vrnetlab version <code>0.2.0</code> has been issued to freeze the code compatible with containerlab 0.11.0.</p> <p>If stopped working when you upgraded to 0.11.0, then re-build the images with vrnetlab 0.2.0</p>"},{"location":"rn/0.11.0/#boot-delay-for-vrnetlab-routers","title":"Boot delay for vrnetlab routers","text":"<p>To schedule the startup of vrnetlab routers the BOOT_DELAY env variable may be used. It takes a  </p>"},{"location":"rn/0.11.0/#added-arista-veos-support","title":"Added Arista vEOS support","text":"<p>Support for virtualized EOS from Arista has been added. Now it is possible to use containerized cEOS and virtualized vEOS.</p>"},{"location":"rn/0.11.0/#sr-os-route-to-management-network","title":"SR OS route to management network","text":"<p>SR OS routers will now have a static route in their Management routing instance to reach the docker management network. This enables SR OS initiated requests to reach services runnign on the management network.</p>"},{"location":"rn/0.11.0/#miscellaneous","title":"Miscellaneous","text":"<ol> <li>Default MTU on the management network/bridge is changed to 1500 from 1450. If you run containerlab in the environment where management network MTU is less than 1500, consider setting the needed MTU in the topo file.</li> <li>Fixed multiple cEOS links creation.</li> <li>Add bridge and linux kinds docs.</li> <li>To allow for multi-node labs and also to make it possible to connect nodes with the container host a new endpoint connection was created - host links.</li> <li>MTU on data links is set to 65000 from 1500. This will allow for jumbo frames safe passage.</li> <li>Additional explanations provided for vrnetlab integration and inter-dependency between projects.</li> </ol> <ol> <li> <p>especially if memory optimization techniques are enabled\u00a0\u21a9</p> </li> </ol>"},{"location":"rn/0.12.0/","title":"Release 0.12.0","text":"<p> 2021-03-28</p>"},{"location":"rn/0.12.0/#identity-aware-sockets","title":"Identity aware sockets","text":"<p>A major improvement to our \"published ports\" feature has landed in 0.12.0. Now it is possible to create Identity Aware sockets for any port of your lab.</p> <p>Identity Aware sockets is a feature of mysocketio service that allows you to let in only authorized users. Authorization is possible via multiple OAuth providers and can be configured to let in users with specific emails and/or with specific domains.</p> <p>Check out how easy it is to create identity aware tunnels with containerlab:</p> <p>With this enhancement it is now possible to create long-running secure tunnels which only the authorized users will be able to access.</p>"},{"location":"rn/0.12.0/#on-demand-veth-plumbing","title":"On-demand veth plumbing","text":"<p>Sometimes it is needed to add some additional connections after the lab has been deployed. Although the labs are quickly to re-spin, sometimes one find themselves in the middle of the use case configuration and there is a need to add another connection between the nodes.</p> <p>With <code>tools veth</code> command it is now possible to add veth pairs between container&lt;-&gt;container, containers&lt;-&gt;host and containers&lt;-&gt;bridge nodes. Now you can add modify your lab connections without redeploying the entire lab[^2].</p>"},{"location":"rn/0.12.0/#safer-ways-to-write-clab-files","title":"Safer ways to write clab files","text":"<p>Containerlab got its own JSON Schema that governs the structure of the topology definition files. If you name your topo file as <code>*.clab.yml</code> then some editors like VS Code will automatically provide auto-suggestions and linting for your clab files.</p> <p>Yes, from now on we will call our topo files as clab-files.</p>"},{"location":"rn/0.12.0/#create-tls-certificates-effortlessly","title":"Create TLS certificates effortlessly","text":"<p>With the new commands <code>tools cert ca create</code> and <code>tools cert sign</code> it is now possible to create CA and node certificates with just two commands embedded into containerlab. Start from here if you always wanted to be able to reduce the number of openssl commands.</p> <p>We also added a lab that pictures the net positive effect of having those commands when creating TLS certificates for secured gNMI connectivity.</p>"},{"location":"rn/0.12.0/#ansible-inventory","title":"Ansible inventory","text":"<p>It is imperative to create a nice automation flow that goes from infra deployment to the subsequent configuration automation. When containerlab finishes its deployment job we now create an ansible inventory file for a deployed lab.</p> <p>With this inventory file the users can start their configuration management playbooks and configure the lab for a use case in mind.</p>"},{"location":"rn/0.12.0/#smart-mtu-for-management-network","title":"Smart MTU for management network","text":"<p>Default MTU value for the management network will now be inherited from the MTU of the <code>docker0</code> bridge interface. Thus, if you configured your docker daemon for a custom MTU, it will be respected by containerlab.</p>"},{"location":"rn/0.12.0/#running-nodes-in-bridge-network","title":"Running nodes in <code>bridge</code> network","text":"<p>When management network name is set to <code>bridge</code>, containerlab nodes will be run in the default docker network named <code>bridge</code>. This is the network where containers end up connecting when you do <code>docker run</code>, so running the lab in the default docker network makes it easy to connect your lab with the workloads that have been started by someone else.</p>"},{"location":"rn/0.12.0/#releases-notification","title":"Releases notification","text":"<p>When a new release comes out we let you know next time you deploy a lab, a tiny message will pop up in the log saying that a new one is ready to make your labs even more efficient and easy.</p> <pre><code>INFO[0001] \ud83c\udf89 New containerlab version 0.12.0 is available! Release notes: https://containerlab.dev/rn/0.12.0\nRun 'containerlab version upgrade' to upgrade or go check other installation options at https://containerlab.dev/install/ \n</code></pre>"},{"location":"rn/0.12.0/#saving-sr-os-config","title":"Saving SR OS config","text":"<p>SR OS nodes which are launched with <code>vr-sros</code> kind now have support for saving their configuration with <code>containerlab save</code> command.</p> <p>This is implemented via Netconf's <code>&lt;copy-config&gt;</code> RPC that is executed against SR OS node.</p>"},{"location":"rn/0.12.0/#miscellaneous","title":"Miscellaneous","text":"<ol> <li>Added support for user-defined node labels which can convey metadata for a given node.</li> <li>Container node needs to support live interface attachment.</li> <li>New TLS certificate logic for SR Linux nodes. If the CA files and node cert exist, the re-deployment of a lab won't issue new certificates and will reuse the existing ones.</li> <li>Additional node property <code>network-mode</code> has been added which allows to deploy the node in the host networking mode.</li> <li>If the changes containerlab makes to LLDP/TX-offload on the management bridge fail, they won't prevent the lab from proceed deploying.</li> </ol>"},{"location":"rn/0.13.0/","title":"Release 0.13.0","text":"<p> 2021-04-13</p>"},{"location":"rn/0.13.0/#cisco-csr1000v-support","title":"Cisco CSR1000v support","text":"<p>Added support for Cisco CSR1000v system via <code>vr-csr</code> kind.</p>"},{"location":"rn/0.13.0/#routeros-support","title":"RouterOS support","text":"<p>With <code>vr-ros</code> kind added support for Mikrotik RouterOS system.</p>"},{"location":"rn/0.13.0/#arista-ceos-improvements","title":"Arista cEOS improvements","text":"<p>This patch release brings the following improvements to Arista cEOS:</p> <ol> <li>Arista Ma0 MAC address is now having Arista OUI, instead of docker generated local MAC. Additionally, System MAC address is now generated as the next MAC address from Ma0 interface.</li> <li>ETBA environment variable is set to <code>4</code>, from its original value of <code>1</code>.</li> <li>Default cEOS configuration now has the following config line to allow for enhanced BGP daemon to run on startup:     <pre><code>service routing protocols model multi-agent\n</code></pre></li> </ol>"},{"location":"rn/0.13.0/#attachments-to-management-network","title":"Attachments to management network","text":"<p>With a new reserved endpoint definition it is now possible to attach data interface of a node to the management network.</p>"},{"location":"rn/0.13.0/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>a warning message will be displayed if container host has less than 1GB of free memory</li> <li>a lab won't deploy if a user specified <code>eth0</code> interface in the links section as this is not a possible configuration</li> <li>Now it is possible to safely use <code>-</code> (dashes) in lab name.</li> </ul>"},{"location":"rn/0.13.0/#new-contributors","title":"New contributors","text":"<p>Thanks to @kellerza, @burnyd, @dharmbhai, @dpnetca for providing some of these enhancements and joining our contributors ranks!</p>"},{"location":"rn/0.14.0/","title":"Release 0.14.0","text":"<p> 2021-05-19</p>"},{"location":"rn/0.14.0/#container-runtime-support","title":"Container runtime support","text":"<p>Michael Kashin (@networkop) delivered a massive infrastructure improvement by adding the foundation that allows containerlab to run on multiple container runtimes such as <code>podman</code>.</p> <p>For the end users of containerlab that will give more flexibility on platforms selection where containerlab can run.</p>"},{"location":"rn/0.14.0/#arista-et-interfaces","title":"Arista <code>et</code> interfaces","text":"<p>Steve Ulrich (@sulrich) added support for synchronization the ENV vars passed to cEOS node and the respective container command. This makes it possible to set the cEOS specific env vars and be sure that they will be mirrored in the CMD instruction of the container.</p> <p>This allowed for users to, for example, overwrite the <code>INTFTYPE</code> env var to allow for using <code>et</code> interfaces with cEOS. This is documented in the ceos kind docs.</p>"},{"location":"rn/0.14.0/#nodedir-path-variable","title":"<code>nodeDir</code> path variable","text":"<p>Markus Vahlenkamp (@steiler) added support for <code>$nodeDir</code> variable that you can now use in the bind paths. This is useful to simplify the configuration artifacts mapping when they are kept in the node specific directories. Read more on this in the nodes/binds documentation section.</p>"},{"location":"rn/0.14.0/#improved-sr-os-vr-sros-boot-procedure","title":"Improved SR OS (<code>vr-sros</code>) boot procedure","text":"<p>With hellt/vrnetlab v0.3.1 we added a hardened process of SR OS boot sequence. Before that fix the vr-sros nodes might get problems in attaching container interfaces in time. Starting with v0.3.1 that issue is no more and vr-sros nodes will wait till the dataplane interfaces will show up in the container namespace.</p>"},{"location":"rn/0.14.0/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>fixed bridge attachment issue</li> <li>fixed docker repo naming resolution which prevented pulling certainly formatted repositories</li> <li>fixed Arista cEOS configuration regeneration and management interface addressing.</li> <li>@networkop added support for predefined mac addresses that containerlab uses for veth interfaces. The MAC OUI is <code>00:c1:ab</code> for all containerlab interfaces.</li> <li>@networkop added support for max-workers argument for <code>delete</code> command.</li> </ul>"},{"location":"rn/0.14.0/#new-contributors","title":"New contributors","text":"<p>Thanks to @sulrich, @blinklet and @networkop for providing some of these enhancements/fixes and joining our contributors ranks!</p>"},{"location":"rn/0.14.1/","title":"Release 0.14.1","text":"<p> 2021-05-20</p>"},{"location":"rn/0.14.1/#fix-lab-deletion-data-race","title":"fix lab deletion data race","text":"<p>This patch release fixes data race which could occur during the deletion of a lab with many nodes.</p>"},{"location":"rn/0.14.2/","title":"Release 0.14.2","text":"<p> 2021-05-27</p>"},{"location":"rn/0.14.2/#fix-additional-interfaces-to-management-network","title":"fix additional interfaces to management network","text":"<p>This patch release fixes issues with containerlab failing to add additional data interfaces to management network as described here.</p>"},{"location":"rn/0.14.3/","title":"Release 0.14.3","text":"<p> 2021-05-28</p>"},{"location":"rn/0.14.3/#fixes-and-improvements","title":"Fixes and improvements","text":"<ul> <li>fixed missing IPv4/6 information in <code>containerlab inspect --all</code> output</li> <li>prevented deployment of the lab if the same-named lab has already been deployed</li> <li>install script now checks if the released version set by the user actually exists</li> </ul>"},{"location":"rn/0.14.4/","title":"Release 0.14.4","text":"<p> 2021-06-06</p>"},{"location":"rn/0.14.4/#fixes-and-improvements","title":"Fixes and improvements","text":"<ul> <li>fixed generate command</li> <li>added \"Y/n\" prompt when doing <code>containerlab version upgrade</code>.</li> <li>version command will return a link to the relevant release notes</li> </ul>"},{"location":"rn/0.15/","title":"Release 0.15","text":"<p> 2021-07-16</p> <p>Warning</p> <p>This release adds a breaking change in the clab file schema. Starting from this release version, a path to a startup config file needs to be provided with <code>startup-config</code> key. Previously it was done with <code>config</code> key.  </p> <p>Use <code>sed -i s/config:/startup-config:/g &lt;topo-file&gt;</code> script to auto-substitute the values in the <code>&lt;topo-file&gt;</code> file.</p>"},{"location":"rn/0.15/#ignite-runtime-support-and-cumulus-vx","title":"Ignite runtime support and Cumulus VX","text":"<p>@networkop embarked on a journey of adding a Cumulus VX to containerlab, and he chose the doing-this-the-hard-way by running Cumulus VX as a container<sup>1</sup>. To make that happen, he added a new <code>ignite</code> runtime that allows us to launch containers with custom kernels.</p> <p>With that work done, containerlab is now supporting a new kind - <code>cvx</code> - that defines the Cumulus VX NOS and that can run in different modes<sup>2</sup>.</p>"},{"location":"rn/0.15/#containerd-runtime-support","title":"Containerd runtime support","text":"<p>@steiler added experimental support for <code>containerd</code> runtime. As the name implies, this makes it possible to run containerlab having only containerd installed and the necessary CNI plugins.</p> <p>Please bear in mind that not all containerlab features are available for containerd runtime, but the core feature set is there.</p> <p>To start containers with containerd runtime, the <code>--runtime | -r</code> global flag has been introduced.</p>"},{"location":"rn/0.15/#the-big-refactoring","title":"The BIG refactoring","text":"<p>With @karimra superpowers, containerlab has undergone a major refactoring of its code base. Now we have a strong and flexible foundation to build new features on top.</p> <p>This was the biggest architectural change since the beginning, so some rough edges might eventually show up, but we will iron them out.</p>"},{"location":"rn/0.15/#community","title":"Community","text":"<p>We strive to support the growing containerlab community to the best of our abilities. While github discussions is a nice place to ask formulated questions sometimes it is really nice to have a chat with the community members.</p> <p>With that in mind we launched the containerlab's own discord server that all are welcome to join!</p>"},{"location":"rn/0.15/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>kind base configuration templates are now embedded in the code. This means that you can download containerlab binary from Releases and launch the labs, without doing the installation from packages.     The reason you might want to do this is if you want to install containerlab on a system which doesn't have rpm/deb based package manager.     Note, that the package-based installation is still the recommended way.</li> <li>the <code>exec</code> command now has a <code>--format</code> flag that is capable of nicely handling the JSON outputs.</li> <li>deployment of SR Linux nodes will now proceed even if license file is not present<sup>3</sup></li> <li>a new <code>--keep-mgmt-net</code> flag has been added to <code>destroy</code> command to prevent the management network deletion attempt</li> <li>the MTU on the veth links created between the containers has been lowered to 9500 from its original 65000 value</li> </ul>"},{"location":"rn/0.15/#patches","title":"Patches","text":""},{"location":"rn/0.15/#0151","title":"0.15.1","text":"<ul> <li>Fixed directories creation for <code>ignite</code> runtimes</li> <li>Fixed release notes links in <code>containerlab version</code> command</li> </ul>"},{"location":"rn/0.15/#0152","title":"0.15.2","text":"<ul> <li>Fixed startup-config provisioning for ceos nodes #526</li> </ul>"},{"location":"rn/0.15/#0153","title":"0.15.3","text":"<ul> <li>Allow ceos startup config to receive ipv4/6 addresses  #528</li> <li>Added JSON-RPC server to SR Linux default configuration</li> </ul>"},{"location":"rn/0.15/#0154","title":"0.15.4","text":"<ul> <li>Fixed disabling tx checksum offload for linux nodes #543</li> <li>Fixed <code>tools disable-tx-offload</code> command #543</li> </ul> <ol> <li> <p>as opposed to a VM-way by running Cumulus with vrnetlab \u21a9</p> </li> <li> <p>both with <code>ignite</code> runtime or <code>docker</code> runtime\u00a0\u21a9</p> </li> <li> <p>starting with SR Linux 21.3, the license file is optional\u00a0\u21a9</p> </li> </ol>"},{"location":"rn/0.16/","title":"Release 0.16","text":"<p> 2021-08-03</p> <p>Warning</p> <p>This release adds a new way of writing containerlab hosts information into the <code>/etc/hosts</code> file.</p> <p>Before upgrading to this release destroy all labs and ensure no stale entries are present in the <code>/etc/hosts</code> file, then do the upgrade.</p>"},{"location":"rn/0.16/#improved-interfaces-detection-for-vr-based-nodes","title":"Improved interfaces detection for vr-based nodes","text":"<p>With the enhancements made by @carlmontanari in vrnetlab and @networkop in containerlab we improved the way vr-based nodes boot.</p> <p>The improvements are related to the way VMs inside the containers detect the presence of the dataplane interfaces. Starting with <code>hellt/vrnetlab v0.5.0</code> and <code>containerlab v0.16.0</code> the nodes will know exactly how many interfaces a user defined in the lab file, and therefore will wait for those interfaces to appear before booting the qemu VM.</p>"},{"location":"rn/0.16/#non-sequential-interfaces-for-vr-based-nodes","title":"Non-sequential interfaces for vr-based nodes","text":"<p>Thanks to @carlmontanari work in vrnetlab#55 it is now possible to define node's interfaces in a non-sequential way. This means, that a user now can define the links as follows:</p> <pre><code>topology:\n  nodes:\n    sr1:\n      kind: vr-sros\n      image: vr-sros:21.5.R2\n\n    sr2:\n      kind: vr-sros\n      image: vr-sros:21.5.R2\n\n  links:\n    # sr1 port 3 is connected to sr2 port 5\n    - endpoints: [\"sr1:eth3\", \"sr2:eth5\"]\n</code></pre>"},{"location":"rn/0.16/#new-dns-entries-format","title":"New DNS entries format","text":"<p>Containerlab creates nodes DNS entries when the lab is deployed. In this release we improved the visual layout of those entries so they can be introspected easier.</p>"},{"location":"rn/0.16/#ansible-inventory-improvements","title":"Ansible inventory improvements","text":"<p>A user can now specify an extra ansible group the node should be part of in addition to the default grouping by kind.</p> <p>Another change is that we now create an empty ansible inventory file when the deployment starts and write data to it once it is available. This approach makes it possible to create a clab file that mounts the <code>ansible-inventory.yml</code> file that will be created during the deploy command.</p> <pre><code># mounting the ansible inventory file to a node of a lab\nbinds:\n  - clab-demo/ansible-inventory.yml:/tmp/inv\n</code></pre>"},{"location":"rn/0.16/#new-node-kinds","title":"New node kinds","text":"<p>@carlmontanari added support for Cisco Nexus 9000v and Palo Alto PAN nodes.</p>"},{"location":"rn/0.16/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>IPv6 LLA is now enabled on veth interfaces #551</li> <li>SR Linux bind mounts were refined and now only config.json and topology.yml files are mounted #564</li> <li>SR Linux default type has changed from IXR-6 to IXR-D2 to support EVPN services by default</li> </ul>"},{"location":"rn/0.16/#new-contributors","title":"New contributors","text":"<p>Welcome @carlmontanari, @sc68cal and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.16/#patches","title":"Patches","text":""},{"location":"rn/0.16/#0161","title":"0.16.1","text":"<ul> <li>Fixed mysocketio integration #574</li> </ul>"},{"location":"rn/0.16/#0162","title":"0.16.2","text":"<ul> <li>@karimra added another cool addition to containerlab: it is possible to customize the lab prefix or remove it entirely. Read more about it here.</li> <li>We had to bring back directory based mounts for SR Linux nodes for the reasons explained in #579</li> </ul>"},{"location":"rn/0.17/","title":"Release 0.17","text":"<p> 2021-08-24</p>"},{"location":"rn/0.17/#environment-variables-expansion","title":"Environment variables expansion","text":"<p>A new feature contributed by @GrigoriyMikhalkin allows you to perform environment variables expansion throughout the whole clab topology file.</p> <p>This allows you to modify topology definition at runtime using the env vars you have defined in your environment. Awesome addition to make clab play nice in CI/CD systems!</p>"},{"location":"rn/0.17/#local-config-takes-precedence-over-startup-config","title":"Local config takes precedence over startup config","text":"<p>A major change has been made to the config load order. With this release the following order of configuration load is in place:</p> <ol> <li>config file found in lab directory</li> <li>startup config set with <code>startup-config</code> setting</li> <li>generated config from a template that comes within containerlab</li> </ol> <p>The change is with the local config to be preferred over a startup-config. The idea here is simple: when a user deploys a lab from with a <code>startup-config</code> pointing to a certain file the lab boots with this config.</p> <p>Later, users typically build on top of that startup config some additional use cases. And in the case they save their changes to the startup config the changes will be persisted in a lab directory of this node. Now, if a users destroys a lab and deploys it again, if the file in the lab dir is found, it will be used, and startup-config will be ignored.</p> <p>If it is needed to make a config file referenced in <code>startup-config</code> to be used, the deploy command should be augmented with <code>reconfigure</code> flag or a new parameter can be used in the topo file - enforce-startup-config.</p>"},{"location":"rn/0.17/#arista-ceos-persistency-improvements","title":"Arista cEOS persistency improvements","text":"<p>A big improvement in the way management interface is configured for ceos kinds.</p> <p>With a new scrapligo feature we were able to configure ceos nodes over <code>docker exec</code> command after the node finishes booting. That allowed us to always ensure that ceos nodes will have management interface addressed with the IP that docker assigned to it, regardless of which IP was already set in startup-config file.</p> <p>That made it possible to remove a lot of workarounds and restrictions that were in place before.</p>"},{"location":"rn/0.17/#mixed-mode-labs-with-static-and-dynamic-ip-addresses","title":"Mixed mode labs with Static and Dynamic IP addresses","text":"<p>Containerlab allows users to set static IP address for the node management interface. This is done with <code>mgmt_ipv4/6</code> setting of a node.</p> <p>It was possible to set static address for part of the nodes of a lab, in that case some nodes will get a static address, the rest would receive an address as assigned by Docker daemon. The issue that users might see with such labs is that docker assigns dynamically an IP that was set as a static IP for another node.</p> <p>To overcome this race condition the nodes with Static IP address will now be scheduled first, and the dynamic IP allocated nodes will follow.</p>"},{"location":"rn/0.17/#startup-delay-for-nodes","title":"Startup delay for nodes","text":"<p>Nodes can be artificially delayed if they have <code>startup-delay</code> field set to a value greater than zero. This indicates amount in seconds that this node will wait till it will be scheduled for creation by containerlab.</p>"},{"location":"rn/0.17/#new-contributors","title":"New contributors","text":"<p>Welcome @GrigoriyMikhalkin and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.18/","title":"Release 0.18","text":"<p> 2021-09-14</p>"},{"location":"rn/0.18/#exec-parameter","title":"exec parameter","text":"<p>The new <code>exec</code> node parameter allows users to specify a list of commands that will run once the nodes are created.</p> <p>A typical application of this parameter is to call some boot script that configures something on the node, for example its IP address.</p>"},{"location":"rn/0.18/#entrypoint","title":"Entrypoint","text":"<p>With the new <code>entrypoint</code> node parameter it is possible to change the entrypoint of the container.</p>"},{"location":"rn/0.18/#sr-linux-agents","title":"SR Linux agents","text":"<p>A new <code>srl-agents</code> parameter of the node's extra config will allow to copy SR Linux agent definition file to the container's <code>appmgr</code> dir.</p>"},{"location":"rn/0.18/#dell-ftos-support","title":"Dell FTOS support","text":"<p>Thanks to @log1cb0mb contribution, containerlab now knows how to start Dell FTOS systems.</p>"},{"location":"rn/0.18/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Better support for Mikrotik ROS system by @nlgotz</li> <li>SR Linux nodes will have their <code>/etc/hosts</code> file populated with name-IP pair of the other nodes of the lab</li> <li>Fixed panic for labs which used import of env vars #609</li> <li><code>save</code> command executed on Juniper cRPD will save config to the startup file</li> <li>fixed <code>save</code> command executed in vr-vmx nodes.</li> <li><code>srl</code> nodes additional config is now pushed once the nodes are started, instead of templating the config beforehand. This allows to follow the factory config of the release, without keeping the outdated template as it was before.</li> </ul>"},{"location":"rn/0.18/#new-contributors","title":"New contributors","text":"<p>Welcome @nlgotz, @log1cb0mb and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.19/","title":"Release 0.19","text":"<p> 2021-10-03</p>"},{"location":"rn/0.19/#containerlab-container","title":"Containerlab container","text":"<p>In this release our build pipeline finally started to produce container images with containerlab inside.</p> <p>This allows you to use containerlab on systems that have container runtime installed without requiring any installation whatsoever.</p> <p>Yes, a container with containerlab inside, so that you can launch containers from a container.</p>"},{"location":"rn/0.19/#experimental-vqfx-support","title":"Experimental vQFX support","text":"<p>With the help of @chriscummings-esnet we added experimental support for Juniper vQFX images built with vrnetlab. It may be rough around the edges, but should be a good start for future enhancements and improvements.</p>"},{"location":"rn/0.19/#mysocket-support-for-http-proxies","title":"Mysocket support for HTTP proxies","text":"<p>Our famous mysocketio integration that allows you to share labs effortlessly and secure has been enhanced with proxy support.</p> <p>With HTTP proxy support it is now possible to share lab access in the environments that have external SSH access blocked.</p>"},{"location":"rn/0.19/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>SR Linux variants have been expanded with support for IXR-H2 and IXR-H3 chassis.</li> <li>The 5 stage Clos lab example has been extended with a full blown configuration of the fabric using gnmic as a vehicle to push configs to all the nodes of this lab.</li> </ul>"},{"location":"rn/0.19/#new-contributors","title":"New contributors","text":"<p>Welcome @chriscummings-esnet, @sacckth, @siva19susi, @marcosfsch and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.19/#patches","title":"Patches","text":""},{"location":"rn/0.19/#0191","title":"0.19.1","text":"<ul> <li>fixed ovs-bridge kind name</li> <li>increased readiness timeout for SR Linux nodes to 120s to allow for slow boot on busy VMs</li> <li>increased external API timeouts (towards docker API) to 120s</li> <li>increased SSH allocated terminal width for config engine to deal with long cfg lines on SR Linux</li> <li>changed default license path on cRPD to make license to apply on boot</li> </ul>"},{"location":"rn/0.19/#0192","title":"0.19.2","text":"<ul> <li>fixed <code>tools veth create</code> command #667</li> <li>fixed <code>save</code> command for <code>vr-csr</code> nodes</li> <li>added config engine example (has been sunsetted in the course of 0.54.2+)</li> </ul>"},{"location":"rn/0.20/","title":"Release 0.20","text":"<p> 2021-11-17</p>"},{"location":"rn/0.20/#templated-topologies","title":"Templated topologies","text":"<p>In #658 @karimra added support for templated topologies. This feature allows users to generate topology files based on the variables defined in a file. With templates, users can create a template once and then deploy different flavors of the topologies by using variables files, which are naturally smaller and way more human-readable.</p> <p>Check the lab examples for addition details.</p>"},{"location":"rn/0.20/#limiting-cpu-and-memory-resources-for-nodes","title":"Limiting CPU and Memory resources for nodes","text":"<p>In #679 @karimra adds resource limitation capabilities for the nodes. It is possible to limit the amount of available CPU, Memory and allocate specific cores to the nodes.</p>"},{"location":"rn/0.20/#containerlab-in-the-media","title":"Containerlab in the media","text":"<p>Containerlab has recently been vocal in the network engineering community by participating in various podcasts and conferences. Don't worry if you missed some, now we have every notable material aggregated In The Media section.</p> <p>For example, here is our NANOG83 talk:</p>"},{"location":"rn/0.20/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Ignite runtime support for containerized clab execution (#670)</li> <li>Containerlab' config engine will not error on the nodes for which credentials were not found (#675)</li> <li>SONiC boot will not block in case of failures (#678)</li> </ul>"},{"location":"rn/0.20/#patches","title":"Patches","text":""},{"location":"rn/0.20/#0201","title":"0.20.1","text":"<ul> <li>Fix panic when copy files (#684)</li> </ul>"},{"location":"rn/0.20/#new-contributors","title":"New contributors","text":"<p>Welcome @LimeHat, and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.21/","title":"Release 0.21","text":"<p> 2021-12-02</p>"},{"location":"rn/0.21/#sr-linux-startup-config-in-cli-format","title":"SR Linux startup-config in CLI format","text":"<p>SR Linux nodes have had support for startup configuration for a very long time. The nuance was that users had to dump the whole running config in JSON format to use it as a startup.</p> <p>While this works, it is not the most straightforward way. Usually, users create some use case by dabbing commands over the CLI, and it would be great to offer them a chance to apply that extra config on top of the factory config when SR Linux node boots.</p> <p>And that is exactly what has been added in v0.21.0! Please meet startup-config in CLI format.</p> <p>Now, if you have a file with CLI commands, you can apply this short snippet to the factory config making the use case ready when SR Linux NOS starts.</p> CLI ConfigTopology <pre><code># contents of myconfig.cli\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp router-id 10.10.10.1\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp group ibgp export-policy export-lo\nset / network-instance default protocols bgp neighbor 192.168.1.2 admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-group ibgp\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-as 65001\n</code></pre> <pre><code>name: srl_lab\ntopology:\nnodes:\n    srl1:\n    kind: srl\n    type: ixrd3\n    image: ghcr.io/nokia/srlinux\n    # a path to the partial config in CLI format relative to the current working directory\n    startup-config: myconfig.cli\n</code></pre> <p>With that addition, you can ship your use cases without carrying the full SR Linux config files, just by checking in the small CLI-styled snippets.</p>"},{"location":"rn/0.21/#auto-enabled-sr-linux-interfaces","title":"Auto enabled SR Linux interfaces","text":"<p>Thanks to @jbemmel for suggesting enabling SR Linux interfaces referenced in the <code>links</code> section of a topology file.</p> <p>The interfaces will come up with Admin Up status when SR Linux finishes booting.</p>"},{"location":"rn/0.21/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Arista cEOS nodes will keep the same System MAC upon re-deployments. This is done to persist the same System MAC for systems like CVP.</li> <li>Fixes to containerlab JSON schema</li> <li>hellt/vrnetlab 0.6.2 added with support for Mikrotik RouterOS v7</li> </ul>"},{"location":"rn/0.22/","title":"Release 0.22","text":"<p> 2021-12-22</p>"},{"location":"rn/0.22/#sr-linux-password-less-login","title":"SR Linux password-less login","text":"<p>Containerlab now generates yet another file that will be stored in a lab directory - <code>authorized_keys</code>. This file will catenate all public keys found in <code>~/.ssh</code> directory.</p> <p>SR Linux nodes will mount this file for <code>admin</code>, <code>linuxadmin</code>, and <code>root</code> users; this will allow for password-less SSH access </p>"},{"location":"rn/0.22/#containerlab-schema","title":"Containerlab schema","text":"<p>To help users navigate in the sheer sea of configuration options containerlab has, we cleaned up the JSON schema.</p>"},{"location":"rn/0.22/#reworked-prefix-logic","title":"Reworked prefix logic","text":"<p>In 0.16, we added a new top-level field - <code>prefix</code> - to let users decide if they want to have containers prefixed with a string other than <code>clab</code>. Now we got a few requests to make containers ditch the prefixes altogether, such as if you named a node <code>mynode</code> it will be created as a <code>mynode</code> container.</p> <p>Your wish came true. Now, if you have an empty string <code>prefix</code>, the container name will be stripped of everything but name. The magic <code>__lab-name</code> prefix value will add lab name to the container name; leaving the prefix out in the topo file will set the container name to <code>clab-&lt;lab-name&gt;-&lt;node-name&gt;</code>. Read more here.</p>"},{"location":"rn/0.22/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>new SR Linux variants <code>ixrd2l</code> and <code>ixrd3l</code> have been added in #726</li> <li>containerlab assigned SR Linux MACs will have <code>1a:b0:</code> prefix and the node index will be the rightmost byte. This makes it easier to identify the macs in the outputs. #713</li> <li>our beloved users created some awesome blogs and streams about containerlab! We featured them on our community page.</li> <li>fixed doubled dot in the container's fqdn #724</li> <li><code>tools cert</code> command gained more checks #725</li> </ul>"},{"location":"rn/0.23/","title":"Release 0.23","text":"<p> 2022-01-25</p>"},{"location":"rn/0.23/#podman","title":"Podman \u03b2","text":"<p>Containerlab' ties to docker are quite substantial. Basically, containerlab assumes that users have Docker runtime installed to launch containerized labs.</p> <p>But docker is not the only runtime that can be used to launch containers. Containerd, Podman and a few other alternative high level runtimes have been available for quite a while. In containerlab 0.23.0 we add beta support for Podman runtime, thanks to @LimeHat contribution!</p> <p>Why podman? Well, a few reasons:</p> <ol> <li>Podman offers the most similar experience to Docker while not using docker at all; one can make an alias <code>podman-&gt;docker</code> and use the same docker-cli commands with the podman runtime.</li> <li>It is easier to install and sometimes comes as a default choice on redhat based distributions.</li> <li>Supporting an alternative runtime can make containerlab usable on systems which can't have docker installed for various reasons.</li> <li>And of course, it is fun to add a runtime which is not docker :D</li> </ol> <p>All in all, we encourage you to test podman runtime if it is of interest to you. Note that not every containerlab feature is supported with podman yet, but the basics are all there.</p> <p>When you install podman, make sure to enable the podman service for the API to work:</p> <pre><code>systemctl enable podman\nsystemctl start podman\n</code></pre> <p>and then you can use containerlab as per usual with just a flag enabling podman runtime:</p> <pre><code>containerlab --runtime podman deploy -t mytopo.clab.yml\n</code></pre>"},{"location":"rn/0.23/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>now whenever users decide to select an existing bridge as a backend for docker network, containerlab will check if the bridge has been already addressed and use the gateway information based on that. Read more about this new feature here.</li> <li>new community posts.</li> </ul>"},{"location":"rn/0.24/","title":"Release 0.24","text":"<p> 2022-02-22</p>"},{"location":"rn/0.24/#enabling-external-lab-access","title":"Enabling external lab access","text":"<p>As more users started playing with containerlab, more deployment use cases began to surface. One of the most common non-trivial deployment use cases was enabling external access from systems deployed outside of containerlab with the topology nodes.</p> <p>This use case was not that apparent because of the way Docker secures the host. By default, Docker doesn't allow external packets to reach containers, while containers can initiate connections to outside hosts. These security measures are carried out via iptables rules Docker maintains.</p> <p>In this release, containerlab will automatically create an allowing rule in the <code>DOCKER-USER</code> chain to allow external systems to reach containerlab nodes. Read more about this feature in our docs.</p>"},{"location":"rn/0.24/#docker-authentication","title":"Docker authentication","text":"<p>Containerlab pulls the images at deploy stage, if the images are not present. To support pulling the images from the repos which require authentication @lbaker-esnet in #755 added the logic to fetch the authentication data from local docker config store.</p> <p>Now if you have logged in to a certain repo with <code>docker login</code>, containerlab will be able to pull the images from this repo, using the credentials stored locally.</p>"},{"location":"rn/0.24/#binds-merge","title":"Binds merge","text":"<p>One of the most used containerlab options -- binds -- got even better. Now the bind paths will get merged should you define them on the defaults, kinds and nodes levels.</p> <p>This makes it possible to define some default binds and have node-specific binds to be added to them.</p>"},{"location":"rn/0.24/#restart-on-failure-for-linux-nodes","title":"Restart on failure for linux nodes","text":"<p>Now containerlab will automatically restart the nodes of <code>linux</code> kind if their main process exited with a non-0 return code. This will ensure longevity of the services such as Telemetry stacks or management software that may crash for various reasons.</p>"},{"location":"rn/0.24/#shared-network-namespaces","title":"Shared network namespaces","text":"<p>Our own @LimeHat added support for making containerlab nodes to join in Pods formation. That is when multiple nodes share the same network namespace.</p> <p>This is done with a new network-mode property value.</p>"},{"location":"rn/0.24/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>SR Linux breakout support has been fixed in #765</li> <li>It is now possible to create ansible inventory without auto-populated <code>ansible_host</code> variables. Thanks to @tobbbles for his work in #793</li> <li>A new flag has been added to deploy command to skip the post deploy actions. This may optimize the boot time if you have static configs provided. Thanks to @bjmeuer for adding this in #773</li> <li>podman \u03b2-support that we announced in <code>0.23.0</code> has fallen victim to a misconfigured build pipeline. Hopefully, in this release, you will be able to use it :D</li> <li>Linux nodes can now use ignite runtime #759</li> <li>SR Linux nodes now not only will get the authz keys from pub keys available at <code>~/.ssh</code> dir, but will also get everything from <code>~/.ssh/authorized_keys</code> file as well. Thanks to @hansthienpondt for adding this in #778</li> <li>destroy command will remove the lab dir even if no containers for that lab was found #753</li> <li>new community posts.</li> </ul>"},{"location":"rn/0.24/#patches","title":"Patches","text":""},{"location":"rn/0.24/#0241","title":"0.24.1","text":"<ul> <li>Do not stop containerlab deploy/destroy in case of a missing iptables DOCKER-USER chain. This is typical to docker installations older than 17.06 version. #797</li> <li>Fix directory removal flow during cleanup #799</li> </ul>"},{"location":"rn/0.25/","title":"Release 0.25","text":"<p> 2022-03-20</p>"},{"location":"rn/0.25/#graph-re-imagined","title":"Graph re-imagined","text":"<p>Since containerlab's inception, the focus has been on text files representing topology. While defining topologies in text format is a nice, git-friendly way of defining labs, we can't neglect the fact that having a nice visualisation of a topology is a compelling feature to have.</p> <p>With this release, we're adding a re-vamped graph engine based on the NeXt UI framework.</p> <p></p> <p>The new graph can be easily generated with the retrofitted <code>graph</code> command:</p> <pre><code>containerlab graph -t &lt;topology-file&gt;\n</code></pre>"},{"location":"rn/0.25/#ceos-interface-mapping-file","title":"cEOS interface mapping file","text":"<p>Starting with Arista cEOS 4.28 it is now possible to provide an interface mapping file that specifies which container interfaces are mapped to which ceos interfaces. This adds flexibility in interface mappings. #787</p>"},{"location":"rn/0.25/#ipinfusion-ocnos","title":"IPInfusion OcNOS","text":"<p>A new VM-based network OS has been added - IPInfusion OcNOS. An interesting fact about this image - it was built with boxen project.</p>"},{"location":"rn/0.25/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>numerous improvements to the resolve-path logic</li> <li>container's ulimits will be set to the host's values #825</li> <li>duplication of statically assigned management addresses will be detected</li> <li>added <code>ceos-copy-to-flash</code> extra node property to allow copying files to flash dir #819</li> <li>containerlab's topology backup file will now be created in the same dir as the original topo file. Before, it was created in the current working dir</li> <li>containerlab will stop if less than 2 vcpu has been detected and srlinux nodes were defined in a topology</li> <li>topology verification step has been enhanced to check if srlinux interface names comply with the expected structure of e1-1 or e1-1-1 pattern</li> <li>containerlab site moved to containerlab.dev domain</li> <li>new community posts</li> </ul>"},{"location":"rn/0.25/#patches","title":"Patches","text":""},{"location":"rn/0.25/#0251","title":"0.25.1","text":"<ul> <li>graph is now served on <code>0.0.0.0:50080</code> address instead of localhost. This enables reaching graphs on headless servers without any port forwarding.</li> </ul>"},{"location":"rn/0.26/","title":"Release 0.26","text":"<p> 2022-05-03</p>"},{"location":"rn/0.26/#nokia-sr-linux-banner","title":"Nokia SR Linux banner","text":"<p>Little things with big impact - this is what SR Linux banner addition is. Starting with 0.26 release, containerlab will add a banner message to all SR Linux nodes with all necessary pointers such as:</p> <ul> <li>docs site</li> <li>release notes PDF</li> <li>YANG browser URL</li> <li>repository where we keep container images</li> <li>links to our cozy discord server and coordinates of ISS our sales representatives</li> </ul> <pre><code>\u276f ssh admin@clab-srl-srl\nWarning: Permanently added 'clab-srl-srl,3fff:172:20:20::f' (ECDSA) to the list of known hosts.\n................................................................\n:                  Welcome to Nokia SR Linux!                  :\n:              Open Network OS for the NetOps era.             :\n:                                                              :\n:    This is a freely distributed official container image.    :\n:                      Use it - Share it                       :\n:                                                              :\n: Get started: https://learn.srlinux.dev                       :\n: Container:   https://go.srlinux.dev/container-image          :\n: Docs:        https://doc.srlinux.dev/21-11                   :\n: Rel. notes:  https://doc.srlinux.dev/rn21-11-2               :\n: YANG:        https://yang.srlinux.dev/v21.11.2               :\n: Discord:     https://go.srlinux.dev/discord                  :\n: Contact:     https://go.srlinux.dev/contact-sales            :\n................................................................\n\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                               \n</code></pre> <p>Now everything you need to get started is just one click away.</p>"},{"location":"rn/0.26/#topology-export","title":"Topology export","text":"<p>Our partner in crime @bortok added a nice feature titled \"topology data auto export\" for his graphite project in #850. What now happens is that on each lab deployment you will find a file called <code>topology-data.json</code> inside the lab directory that contains most important topology information that can be consumed by external tools.</p> <p>Read more about what data is dumped here.</p> <p>In addition to the auto-export functionality, users are now able to provide their own Go template file describing the desired output format for topology export. This is possible with the newly added <code>export-template</code> flag.</p> <p>And even that is not all :D To be able to easily refer to the files located in a lab directory<sup>1</sup> a new magic variable has been added - <code>__clabDir__</code>.</p>"},{"location":"rn/0.26/#keysight-ixia-c","title":"Keysight IXIA-C","text":"<p>Keysight folks added support for their open source traffic generator appliance - ixia-c - to containerlab. Meet <code>keysight_ixia-c-one</code> kind!</p> <p>This addition allows you to write and run traffic-based tests using open-source production-grade traffic generators all inside the container runtime setting. There is a lot to be said about this integration, and we plan to record a few videos with Keysight; for now you can hack it on your own.</p>"},{"location":"rn/0.26/#env-files","title":"Env files","text":"<p>Our own @steiler is back and added support (#847) for env files that you can use with <code>env-files</code> node container.</p>"},{"location":"rn/0.26/#labels-as-env-vars","title":"Labels as env vars","text":"<p>Now every label that you (or containerlab) defined for a node gets promoted to an env var. With this it is possible to let scripts running inside the container access to metadata information we attach via clab files #841.</p>"},{"location":"rn/0.26/#sysctls","title":"Sysctls","text":"<p>And if you wanted to configure sysctls for a node - it is now possible with <code>sysctls</code> container #856.</p>"},{"location":"rn/0.26/#bridge-forwarding","title":"Bridge forwarding","text":"<p>When clabbers used linux bridges in their topologies it was common to see traffic to be dropped when entering a bridge. This was in line with the default iptables rules which prevent forwarding over the bridge by default.</p> <p>Now containerlab will add the relevant iptables rule to allow forwarding over the bridges #871.</p>"},{"location":"rn/0.26/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>next-ui-based graph works better with long image names in tooltips #834</li> <li>added json-rpc example for nokia sr linux</li> <li>better runtime mtu detection #840</li> <li>Arista cEOS nodes now will wait for all veth interfaces to first be present inside the container, before starting the main startup scripts #854</li> <li>added PauseContainer runtime method #843</li> <li>containerlab will not attempt to deploy labs with sr linux nodes if SSE3 cpu instruction set is not found #846</li> <li>better cleanup procedure for veth between container and host namespaces #862</li> <li>new community posts</li> </ul>"},{"location":"rn/0.26/#patches","title":"Patches","text":""},{"location":"rn/0.26/#0261","title":"0.26.1","text":"<ul> <li>scrapligo updated to mitigate long ceos boot times #879</li> <li>better handling of file paths #877</li> <li>support for ceos <code>et</code> interfaces in wait script #881</li> </ul>"},{"location":"rn/0.26/#0262","title":"0.26.2","text":"<ul> <li>fix concurrent calls to iptables #884</li> </ul> <ol> <li> <p>that is the one that is named as <code>clab-&lt;lab-name&gt;</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"rn/0.27/","title":"Release 0.27","text":"<p> 2022-07-06</p>"},{"location":"rn/0.27/#multiple-kind-names","title":"Multiple kind names","text":"<p>When we started containerlab, using short kind names was easy. Everybody knew that <code>srl</code> is SR Linux, and, say, <code>ceos</code> is cEOS. As we grew, more people started to use containerlab and not everyone knew the short names of NOSes.</p> <p>Therefore we added an ability to use long names for kinds, for example, <code>nokia_srlinux</code> or <code>arista_ceos</code>. These new \"long\" names have been added to kind docs.</p>"},{"location":"rn/0.27/#juniper-vqfx-and-juniper-vmx-startup-config","title":"Juniper vQFX and Juniper vMX startup config","text":"<p>We finally added support to provision vMX and vQFX with startup configuration. The same <code>startup-config</code> node property allows users to point to a file that contains configuration lines that will get applied to a network element when it starts.</p>"},{"location":"rn/0.27/#misc","title":"Misc","text":"<ul> <li>SR Linux nodes will be able to use the names of the other nodes of the same #891</li> </ul>"},{"location":"rn/0.27/#patches","title":"Patches","text":""},{"location":"rn/0.27/#0271","title":"0.27.1","text":"<ul> <li>adding missing config dir mount for vMX</li> </ul>"},{"location":"rn/0.28/","title":"Release 0.28","text":"<p> 2022-06-27</p>"},{"location":"rn/0.28/#topology-file-auto-detect","title":"Topology file auto-detect","text":"<p>With embedded shorthands <code>containerlab</code>-&gt;<code>clab</code>, <code>deploy</code>-&gt;<code>dep</code> and so on, we made using containerlab over the CLI a fast and pleasant experience. But even then, typing <code>clab dep -t mytopo.clab.yml</code> is one argument too long.</p> <p>Please welcome the topology file auto-detect feature which turns <code>clab dep -t mytopo.clab.yml</code> to just <code>clab dep</code> </p> <p>Note</p> <p>A single file matching <code>*.clab.y*ml</code> pattern must be present in the current working directory for auto-detect feature to work.</p> <p>Should you have multiple topology files, use the <code>--topo</code> flag as before.</p>"},{"location":"rn/0.28/#ignite-based-linux-containers","title":"Ignite-based linux containers","text":"<p>Weaveworks/ignite runtime integration that was added by @networkop for Cumulus VX nodes got a new application. With #910 merged, users now can run Linux VMs in a container packaging and leverage VM-like experience while running lightweight and fast.</p>"},{"location":"rn/0.28/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>added <code>-c</code> shorthand for <code>--reconfigure</code> as it is very handy to use <code>clab dep -c</code> to cleanly redeploy the lab during the iterations of lab build-out.</li> </ul>"},{"location":"rn/0.28/#patches","title":"Patches","text":""},{"location":"rn/0.28/#0281","title":"0.28.1","text":"<ul> <li>fixed crpd kind name #922</li> <li>added missing kinds to clab.schema.json</li> <li>ceos nodes now have default route pointing to management gw address #920</li> <li>iptables has been added to clab container #921</li> <li>diagrams rendering engine update to latest version</li> </ul>"},{"location":"rn/0.29/","title":"Release 0.29","text":"<p> 2022-07-11</p>"},{"location":"rn/0.29/#checkpoint-cloudguard","title":"Checkpoint Cloudguard","text":"<p>Our firewall-focused camp got a new member - Checkpoint Cloudguard platform. This platform is built with boxen project instead of vrnetlab. We are slowly building confidence in boxen and new platforms will likely be solely powered by boxen. PR #934</p>"},{"location":"rn/0.29/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>added <code>-c</code> shorthand for <code>--cleanup</code> flag of the <code>destroy</code> command. Now to remove a lab and get rid of the lab directory simply call <code>clab des -c</code>.</li> <li><code>inspect</code> command has been fixed to display IP information even for nodes connected to external networks.</li> <li>added srlinux 6e and 10e platforms</li> </ul>"},{"location":"rn/0.30/","title":"Release 0.30","text":"<p> 2022-07-28</p>"},{"location":"rn/0.30/#podman-v4","title":"Podman v4","text":"<p>Podman v4 delivers a new API to schedule containers within the Podman runtime. Version 4 allows us to deliver better support for this runtime in the context of containerlab #919.</p>"},{"location":"rn/0.30/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>ipv6 gateway support in arista ceos startup-config #940.</li> <li>management vrf support in ceos with the use of a <code>CLAB_MGMT_VRF</code> env var #941</li> <li>SR Linux mac address generation function allows for multi-node setups with random seed #942</li> <li>fixed Netconf save operations #953</li> </ul>"},{"location":"rn/0.31/","title":"Release 0.31","text":"<p> 2022-08-11</p>"},{"location":"rn/0.31/#external-containers","title":"External containers","text":"<p>As people of cloud started to use containerlab to test features on the intersection of networking and compute domains, it became evident that containerlab lacked an option to schedule a container in the network namespace of an external container. For example, the need was to start a containerlab lab and stitch it nicely to the k8s kind cluster nodes which run some CNIs that are willing to talk to the network nodes launched in containerlab.</p> <p>With #969 being merged we are adding this capability and making it easy to bridge emulated network nodes with k8s nodes running with kind.</p>"},{"location":"rn/0.31/#san-support-for-sr-linux-certificates","title":"SAN support for SR Linux certificates","text":"<p>With <code>SANs</code> node property users can set subject alternative names for certificates that get generated by containerlab. Currently, this is only implemented for SR Linux #968.</p>"},{"location":"rn/0.31/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>startup config template files can leverage gomplate functions #955</li> <li>Logs are now printed to <code>stderr</code>. This is mainly done to make sure that when we print any json, you can parse it with <code>jq</code> without logging info getting in the way #958</li> <li>containerlab version check can now be skipped if CLAB_VERSION_CHECK env var is set to <code>disable</code> #959</li> <li>mysocketio information will now be printed in json output #887</li> <li>gnmi unix socket is now enabled by default for SR Linux nodes #965</li> </ul>"},{"location":"rn/0.31/#patches","title":"Patches","text":""},{"location":"rn/0.31/#0311","title":"0.31.1","text":"<ul> <li>added support for saving configuration for numerous vrnetlab-based nodes #973</li> <li>updated lab examples to be sourced from the main branch instead of master #978</li> <li>enhancements to config templates loader #974</li> </ul>"},{"location":"rn/0.32/","title":"Release 0.32","text":"<p> 2022-09-22</p>"},{"location":"rn/0.32/#dependency-manager","title":"Dependency Manager","text":"<p>Our own @steiler has volunteered to tackle a problem of inter-node dependency management and produced three mighty PRs #1013, #1022, #1026. Dependency Manager (or DM for short) gives containerlab superpowers to decide the scheduling order of the lab nodes.</p> <p>Scheduling the order of the nodes is important. Nodes with dynamic management IPs should start after the nodes with static IPs, nodes with shared network namespace should start after the donor node is ready, and so on...</p> <p>These implicit inter-node dependencies are handled by containerlab in the background, so you don't have to worry. Sometimes, though, you may want to influence the order of nodes scheduling yourself. For example, you may want to start a telemetry collector after the network nodes are running. For that reason, a new node property <code>wait-for</code> has been introduced.</p> <p>In <code>wait-for</code> section, you can specify node names that this node will wait for before being allowed to get scheduled.</p>"},{"location":"rn/0.32/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>iptables interactions with external bridges have been fixed #982</li> <li>dependabot integration will keep watch on the deps used in containerlab and update to latest once available #984</li> <li>crpd startup config handling has been fixed in a scenario when some nodes have used startup config and some don't</li> </ul>"},{"location":"rn/0.32/#patches","title":"Patches","text":""},{"location":"rn/0.32/#0321","title":"0.32.1","text":"<ul> <li>fixed SR Linux IXR-D5 topology specification</li> </ul>"},{"location":"rn/0.32/#0322","title":"0.32.2","text":"<ul> <li>support for multi line card SR OS deployments</li> <li>fix for cvx nodes sequential boot #1037</li> <li>return of the ignite runtime for cvx kind #1063</li> <li>added <code>auto-remove</code> option for nodes #1056</li> </ul>"},{"location":"rn/0.32/#0323","title":"0.32.3","text":"<ul> <li>fix default bridge misuse #1071</li> <li>improved auto-remove setting propagation #1065</li> <li>added docs for SR OS multi line card configuration</li> </ul>"},{"location":"rn/0.32/#0324","title":"0.32.4","text":"<ul> <li>cleanup veth pairs connected to bridges #1090</li> <li>fix SSSE3 instruction check #1083</li> <li>change sr linux readiness check to use a file-based approach #1084</li> <li>display errors which may occur during sr linux startup config apply #1089</li> </ul>"},{"location":"rn/0.33/","title":"Release 0.33","text":"<p> 2022-11-28</p>"},{"location":"rn/0.33/#external-container-kind","title":"External container kind","text":"<p>With a new <code>ext-container</code> kind it is possible to enrich clab topology with nodes scheduled by other tools (e.g. <code>k8s-kind</code>). Thanks @steiler.</p>"},{"location":"rn/0.33/#graph-enhancements","title":"Graph enhancements","text":"<p>Thanks to @gamerslouis our graph feature got some important fixes like properly displaying bridge properties in #1097 and renders bridges in offline mode in #1098.</p>"},{"location":"rn/0.33/#sr-os-and-bof-persistency","title":"SR OS and BOF persistency","text":"<p>Making SR OS BOF persistent was not a trivial task, but @mabra94 added a nice doc section explaining how bind mounts and boot-good-exec SR OS option combined can achieve that. #1107</p>"},{"location":"rn/0.33/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>fixed ssse3 check #1092</li> <li>housekeeping item by @steiler - default node embedding #1099</li> <li>startup-config support for PAN OS nodes #1109</li> <li>save CPU from spinning when waiting for nodes to boot #1108</li> </ul>"},{"location":"rn/0.34/","title":"Release 0.34","text":"<p> 2022-12-23</p>"},{"location":"rn/0.34/#cisco-xrd-support","title":"Cisco XRd support","text":"<p>Thanks to @trustywolf we finally landed support for Cisco XRd! No more dealing with a 16GB mem-hungry VM monster when all you need is a control plane. #1144</p>"},{"location":"rn/0.34/#major-codebase-refactoring","title":"Major codebase refactoring","text":"<p>@steiler went into the berzerk mode and refactored half of the containerlab's internal code base to have a cleaner separation of packages, internal APIs and increased extensibility.</p> <p>This change was carried over in multiple PRs and touched a lot of files; while we did quite some testing and maintained the same user experience, there might be things that work differently, do let us know if there is something out of order.</p>"},{"location":"rn/0.34/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Containerlab can now be installed on Core OS #1115</li> <li>Fixed pull image function to support pulling images without explicit tags #1123</li> <li>Memory calculation function has been fixed to report on available memory, not just free one #1133</li> </ul>"},{"location":"rn/0.35/","title":"Release 0.35","text":"<p> 2023-01-17</p>"},{"location":"rn/0.35/#dns-options","title":"DNS options","text":"<p>A new node property - DNS - allows users to provide DNS options to the nodes.</p>"},{"location":"rn/0.35/#containerlab-exec-fixes-and-improvements","title":"<code>containerlab exec</code> fixes and improvements","text":"<p>Thanks to efforts by @steiler we have refactored <code>exec</code> command and fixed a few bugs along the way. The command now supports multiple <code>--cmd</code> arguments. #1161</p>"},{"location":"rn/0.35/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Internal refactoring of the node registry #1156</li> <li>fixed ovs-bridge init #1172</li> <li>interface name checks enabled for all VM-based nodes #1191</li> </ul>"},{"location":"rn/0.35/#patches","title":"Patches","text":""},{"location":"rn/0.35/#0351","title":"0.35.1","text":"<ul> <li>fixed the regexp used in the interface name check function #1201</li> </ul>"},{"location":"rn/0.35/#0352","title":"0.35.2","text":"<ul> <li>removed interface name checks for linux kinds.</li> <li>added a note on <code>et_X_Y</code> nameing of ceos.</li> <li>fixed <code>containerlab config</code> command for SR OS nodes.</li> </ul>"},{"location":"rn/0.36/","title":"Release 0.36","text":"<p> 2023-01-27 \u00b7  Full Changelog</p>"},{"location":"rn/0.36/#image-pull-policy","title":"Image Pull Policy","text":"<p>Now we give you control over the image pulling behavior by introducing <code>image-pull-policy</code> node property. You want your image to be always fetched from the remote registry? Now it is possible. #1223</p>"},{"location":"rn/0.36/#cisco-c8000","title":"Cisco c8000","text":"<p>A new kind - <code>cisco_8000</code> - has been added to the list of supported platforms thanks to an awesome contribution from @rskorka in #1216.</p>"},{"location":"rn/0.36/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Fixed Cumulus interface name checks #1224</li> <li>Housekeeping items in #1208 #1209 and #1225 by @steiler</li> <li>Fixed credentials extraction for DockerHub images</li> </ul>"},{"location":"rn/0.36/#patches","title":"Patches","text":""},{"location":"rn/0.36/#0361","title":"0.36.1","text":"<ul> <li>fixed certificate creation for SR Linux nodes identified with <code>nokia_srlinux</code> kind.</li> </ul>"},{"location":"rn/0.37/","title":"Release 0.37","text":"<p> 2023-02-21 \u00b7  Full Changelog</p>"},{"location":"rn/0.37/#certificates-refactoring","title":"Certificates refactoring","text":"<p>Breaking change!</p> <p>Refactoring of the certificates handling in containerlab done in #1219 introduces a few breaking changes.</p> <p>First, the certificates are now stored in the <code>&lt;lab-dir&gt;/.tls</code> directory<sup>1</sup>.</p> <p>CA certificate and key are now placed under the <code>&lt;lab-dir&gt;/.tls/ca</code> directory<sup>2</sup>.</p> <p>Certificates are now generated for every node in the lab, but this may be configurable in the future.</p> <p>After upgrading to v0.37 make sure to deploy the labs using the <code>-c</code> flag to remove old lab directory and generate new certificates.</p>"},{"location":"rn/0.37/#custom-location-for-containerlab-lab-base-directory","title":"Custom location for containerlab lab base directory","text":"<p>By default, containerlab creates a lab directory (that is the one named <code>clab-xxx</code>) in the current working directory. This is not always convenient, especially when you want to run containerlab from a different directory than the one where you want to store the lab files. To address this issue, we have added a new environment variable <code>CLAB_LAB_DIR_BASE</code> that allows you to specify the location of the lab directory. #1248 #1250</p>"},{"location":"rn/0.37/#docker-v23-compatibility","title":"Docker v23 compatibility","text":"<p>In #1257 we have added support for Docker v23. This is a major release of Docker that brings some changes to the API specifically. We have tested containerlab with Docker v23 and it seems to work fine. However, we have not tested all the features of containerlab with Docker v23, so if you find any issues, please let us know.</p>"},{"location":"rn/0.37/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>SR Linux gNMI/gNOI server rate-limit increased to <code>65000</code> #1239</li> <li>Big refactoring of the path handling #1238</li> </ul>"},{"location":"rn/0.37/#patches","title":"Patches","text":""},{"location":"rn/0.37/#0371","title":"0.37.1","text":"<ul> <li>removed compression with <code>upx</code> as it caused selinux violations on Rocky Linux v9+ #1264</li> </ul> <ol> <li> <p>Previous path was <code>&lt;lab-dir&gt;/ca</code> \u21a9</p> </li> <li> <p>Previous path was <code>&lt;lab-dir&gt;/ca/root</code> \u21a9</p> </li> </ol>"},{"location":"rn/0.38/","title":"Release 0.38","text":"<p> 2023-03-13 \u00b7  Full Changelog</p>"},{"location":"rn/0.38/#remote-startup-configs","title":"Remote startup-configs","text":"<p>Node's <code>startup-config</code> can now point to a remote http/https location. Containerlab will download the remote file and use it as a startup-config. #1283</p>"},{"location":"rn/0.38/#link-mtu","title":"Link MTU","text":"<p>Now users can set the MTU of the link between two nodes. This is done by setting the <code>mtu</code> field in the link definition. Prior to that change, MTU was always 9500B. #1285</p>"},{"location":"rn/0.38/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Network mode <code>none</code> has been added to allow running nodes without docker networking #1284</li> <li>Fix extra newline chars added to files that already had one #1286</li> <li>Default interface name checks are no longer enforced, with only specific kinds using them #1276</li> <li>Updated Go to v1.20 #1287</li> </ul>"},{"location":"rn/0.39/","title":"Release 0.39","text":"<p> 2023-04-10 \u00b7  Full Changelog</p>"},{"location":"rn/0.39/#partial-configs-for-sr-os","title":"Partial configs for SR OS","text":"<p>Containerlab now supports partial configs for SR OS. This means you can now provide CLI snippets that will be added to the default config. A very useful feature when you want to add a few things to the config, and you don't want to copy the whole config to your lab. #1249</p> <p>We believe this will make your lab repos cleaner and easier to maintain.</p> Topology<code>myconfig.partial.txt</code> <pre><code>name: sros_lab\ntopology:\n  nodes:\n    sros:\n      kind: vr-sros\n      startup-config: myconfig.partial.txt\n</code></pre> <pre><code>configure {\n    router \"Base\" {\n        static-routes {\n            route 192.168.200.200/32 route-type unicast {\n                next-hop \"192.168.0.1\" {\n                    admin-state enable\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>The partial config can be provided as a local file, remote URL or even embedded, yay!</p> <p>Learn more about partial configs for SR OS here.</p>"},{"location":"rn/0.39/#embedded-startup-configs","title":"Embedded startup-configs","text":"<p>The changes made for allowing SR OS to have partial configs also allowed all other nodes to enjoy embedding startup-configs into the topology file.</p> <p>For example, this is a perfectly valid way to provide partial config for a Nokia SR Linux node:</p> <pre><code>name: srl\n\ntopology:\n  nodes:\n    srl:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux:22.11.2\n      startup-config: |\n        system information location \"I am embedded config\"\n</code></pre>"},{"location":"rn/0.39/#node-filtering","title":"Node filtering","text":"<p>One of the most requested features is finally there. Users wanted to be able to deploy only a subset of nodes defined in their topology file. Usually, this was driven by the need to control the resource usage of the lab and optimize the deployment time.</p> <p>Containerlab now allows users to provide a list of nodes to deploy, destroy, save, and graph commands to scope the operation to only those nodes. #1298</p> <p>Check out Node Filtering docs for details.</p>"},{"location":"rn/0.39/#cpu-and-memory-setting-for-sr-os-nodes","title":"CPU and Memory setting for SR OS nodes","text":"<p>With hellt/vrnetlab release v0.10.1 we support setting the CPU and Memory for SR OS nodes.</p> <p>Before that feature was available, the CPU and Memory were set to the default values for a particular SR OS variant. This was not ideal as requirements change over time and when resources are underprovisioned, the node may not be able to start.</p> <p>Now you can set the CPU and Memory for SR OS nodes directly in the topology file:</p> <pre><code># distrubuted node\n    sr:\n      kind: vr-sros\n      type: sr-1e\n      license: sros22.lic\n      env:\n        CP_MEMORY: 6 # CPM MEM\n        CP_CPU: 4    # CPM CPU\n        LC_MEMORY: 6 # Line card MEM\n        LC_CPU: 4    # Line card CPU\n\n# integrated\n    sr:\n      kind: vr-sros\n      type: sr-1\n      license: sros22.lic\n      env:\n        MEMORY: 6\n        CPU: 4\n</code></pre>"},{"location":"rn/0.39/#rare-joins-containerlab","title":"RARE joins Containerlab","text":"<p>We are happy to announce that RARE Network OS is now supported by Containerlab. RARE stands for Router for Academia, Research &amp; Education.</p>"},{"location":"rn/0.39/#securely-connecting-to-labs-with-border0-experimental","title":"Securely connecting to labs with Border0 (experimental)","text":"<p>With mysocketio rebranding to border0 we had to revisit APIs for creating secured remote access to labs. This is an experimental feature and is not yet fully supported. #1131</p>"},{"location":"rn/0.40/","title":"Release 0.40","text":"<p> 2023-04-28 \u00b7  Full Changelog</p>"},{"location":"rn/0.40/#ipv46-sans-for-sr-linux","title":"IPv4/6 SANs for SR Linux","text":"<p>In #1345 we added support for IPv4/6 SANs for SR Linux nodes. With this addition, the certificate will contain IPv4/6 addresses assigned by the container runtime as SANs, so TLS connections can be established using these addresses.</p>"},{"location":"rn/0.40/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>SR Linux variants D4 and H4 have been added #1339</li> <li>Deploy command honors Ctrl-C and cleans up interrupted deployment #1340</li> <li>SR Linux agent path can now be a remote HTTP(S) URL #1336</li> <li>Improvements to the license handling process #1330</li> <li>Doc examples updated to comply with changes in SR Linux 23.3.1 #1341</li> </ul>"},{"location":"rn/0.41/","title":"Release 0.41","text":"<p> 2023-05-11 \u00b7  Full Changelog</p>"},{"location":"rn/0.41/#parameters-renaming","title":"Parameters renaming","text":"<p>DEPRECATION NOTICE</p> <p>To unify the style of the topology definition file parameters, the following keys have been renamed:</p> <ul> <li><code>mgmt_ipv4</code> -&gt; <code>mgmt-ipv4</code></li> <li><code>mgmt_ipv6</code> -&gt; <code>mgmt-ipv6</code></li> <li><code>ipv4_subnet</code> -&gt; <code>ipv4-subnet</code></li> <li><code>ipv6_subnet</code> -&gt; <code>ipv6-subnet</code></li> </ul> <p>The old keys are deprecated and containerlab will emit a warning if they are used in the topology definition file. To quickly adapt the topology definition file to the new format use the following script that recursively replaces the old keys with the new ones for all yaml files in the current working directory:</p> <pre><code>#!/bin/bash\nfind . -name '*.yaml' -o -name '*.yml' | while read file; do\n\n    # Replace mgmt_ip\n    sed -i 's/mgmt_ipv4:/mgmt-ipv4:/g' \"$file\"\n    sed -i 's/mgmt_ipv6:/mgmt-ipv6:/g' \"$file\"\n\n    # Replace ip_subnet\n    sed -i 's/ipv4_subnet:/ipv4-subnet:/g' \"$file\"\n    sed -i 's/ipv6_subnet:/ipv6-subnet:/g' \"$file\"\n\ndone\n</code></pre>"},{"location":"rn/0.41/#juniper-vsrx-support","title":"Juniper vSRX support","text":"<p>In #1352 @Exhar added support for vSRX using vrnetlab. See https://github.com/hellt/vrnetlab/pull/112 for implementation details on the vrnetlab side. The new node kind is <code>vr-juniper_vsrx</code>.</p> <p>In #1345 we added support for IPv4/6 SANs for SR Linux nodes. With this addition, the certificate will contain IPv4/6 addresses assigned by the container runtime as SANs, so TLS connections can be established using these addresses.</p>"},{"location":"rn/0.41/#management-ip-range-for-docker-runtime","title":"Management IP range for Docker runtime","text":"<p>With #1365 @steiler added support for a new property of a management network - <code>ip-range</code> - that allows to specify a range of IP addresses that will be used by the Docker runtime to assign IP addresses from the specified subnet.</p> <p>This is particularly useful when you want to restrict docker from using a sub-block of the subnet that is used by the management network when it is, for example, used by another orchestrator.</p>"},{"location":"rn/0.41/#automatic-load-of-iptables-modules","title":"Automatic load of <code>iptables</code> modules","text":"<p>With #1363 merged, we hopefully crossed out one of the most annoying issues with containerlab - the need to manually load <code>ip_tables</code> and <code>ip6_tables</code> kernel modules on the host. These modules enable containerlab to provide external connectivity for the nodes and are crucial for VM-based nodes' datapath operation.</p> <p>Now containerlab will try to load these modules automatically if they are not loaded yet.</p>"},{"location":"rn/0.41/#environment-variables-in-topology-definition-file","title":"Environment variables in topology definition file","text":"<p>A hidden feature of containerlab is the ability to use environment variables in the topology definition file. This feature is helpful when you want to parametrize the topology file based on the environment where it is deployed. For example, you can use it to specify the image URI for the nodes, or a particular path to the config files.</p> <p>Now we enhanced the support for environment variables by adding bash-style variable expansion support. This means that you can use the following syntax to expand the environment variables in the topology definition file:</p> <pre><code>name: linux\n\ntopology:\n  nodes:\n    l1:\n      kind: linux\n      image: alpine:${ALPINE_VERSION:=3}\n</code></pre> <p>Where the syntax means that if the <code>ALPINE_VERSION</code> environment variable is not set, the default value of <code>3</code> will be used. With that, you can now use the same topology definition file for different environments by setting the environment variables accordingly.</p>"},{"location":"rn/0.41/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Fixed regression in external bridge support in #1361</li> <li>Fixed Ansible inventory generation to honor <code>--no-host-var</code> flag for user groups #1369</li> <li>We removed dependency on cloudflare ssl package in #1308 where @steiler used Go standard library to generate CA and node's certificates and keys.</li> </ul>"},{"location":"rn/0.41/#patches","title":"Patches","text":""},{"location":"rn/0.41/#0411","title":"0.41.1","text":"<p>We admit that we were too harsh with the removal of the old keys in <code>v0.41.0</code>. We decided to bring them back and deprecate them instead. This will allow users to migrate to the new keys at their own pace. #1391</p> <p>The deprecated fields will stay for a few months and a log message will be printed when they are used.</p> <ul> <li>Fixed public keys population for password-less SSH access for SR Linux nodes #1384</li> <li>Added support for extracting public keys from SSH agent #1388</li> <li>Added json_schema types for SR OS #1387</li> </ul>"},{"location":"rn/0.41/#0412","title":"0.41.2","text":"<ul> <li>fix an issue where deprecated <code>mgmt_ipv6</code> overwrites <code>mgmt_ipv4</code> value #1397</li> </ul>"},{"location":"rn/0.42/","title":"Release 0.42","text":"<p> 2023-06-17 \u00b7  Full Changelog</p>"},{"location":"rn/0.42/#home-dir-for-sudo-user","title":"Home dir for <code>sudo</code> user","text":"<p>Now when you run <code>sudo containerlab deploy</code>, the topology files will be searched in the home directory of the user that runs the command and not the root user. Before that change users had to use <code>sudo -E</code> to preserve the environment variables and run the command as the current user. Now we made it simpler, just in the spirit of containerlab. #1412</p>"},{"location":"rn/0.42/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Optimized link creation #1395</li> <li>Kernel version check is performed for topologies with SR Linux nodes which require kernel v4.10+ #1415</li> <li>SNMP has been added to the list of services that are enabled by default in SR Linux #1416</li> <li>Checkpoint named <code>clab-initial</code> is automatically generated on SR Linux nodes when the node is booted #1431</li> <li>Partial startup configs with no commands do not cause panic anymore #1423</li> </ul>"},{"location":"rn/0.43/","title":"Release 0.43","text":"<p> 2023-07-26 \u00b7  Full Changelog</p>"},{"location":"rn/0.43/#macvlan-interfaces","title":"MACVLAN interfaces","text":"<p>@steiler added support for MACVLAN interfaces in #1402. This is a great addition for those who want to run containerlab and have the nodes to be directly connected to the physical network. Here is an example of a topology that uses MACVLAN interfaces:</p> <pre><code>name: macvlan\n\ntopology:\n  nodes:\n    l1:\n      kind: linux\n      image: alpine:3\n\n  links:\n    - endpoints: [\"l1:eth1\", \"macvlan:enp0s3\"]\n</code></pre> <p>This results in <code>l1</code> node having a macvlan interface named <code>eth1</code> that uses a parent interface of <code>enp0s3</code>. The parent interface is the one that is connected to the physical network. The <code>enp0s3</code> interface is created on the host machine and is not visible to the container. The <code>eth1</code> interface is created inside the container and is visible to the container only. The <code>eth1</code> interface is assigned an IP address from the same subnet as the <code>enp0s3</code> interface. This allows the container to be directly connected to the physical network.</p>"},{"location":"rn/0.43/#mermaid-diagrams","title":"Mermaid diagrams","text":"<p>@YutaroHayakawa in #1433 added a new graph function that allows you to generate Mermaid-based diagrams which you can embed into, for example, markdown documents on Github.</p> <p>Check out the documentation for more details.</p> <p>Here is an example of embedding a mermaid graph diagram for the 5-tier Clos topology from the lab example. It was generated with the following command:</p> <pre><code>clab graph --mermaid -t /etc/containerlab/lab-examples/clos02/clos02.clab.yml \n</code></pre> <pre><code>---\ntitle: clos02\n---\ngraph BT\n  spine3---superspine1\n  spine4---superspine2\n  spine1---superspine1\n  leaf3---spine4\n  leaf2---spine1\n  leaf4---spine3\n  client2---leaf2\n  client3---leaf3\n  client4---leaf4\n  leaf1---spine1\n  leaf1---spine2\n  leaf3---spine3\n  leaf4---spine4\n  client1---leaf1\n  leaf2---spine2\n  spine2---superspine2</code></pre> <p>Compare it with the hand-drawn topology and you can see that it is pretty close to the real thing.</p>"},{"location":"rn/0.43/#overwriting-bind-mounts","title":"Overwriting bind mounts","text":"<p>With #1446 users can now overwrite bind mounts that are defined on a more granular levels of a topology. A good example for such case is where a user defines some bind mounts for a certain kind, but then wants to overwrite some of these binds on a per-node basis. Here is an example:</p> <pre><code>topology:\n  kinds:\n    linux:\n      binds:\n        - source1.json:/dst.json:ro\n  nodes:\n    linux1:\n      kind: linux\n      binds:\n        - source2.json:/dst.json:ro\n</code></pre> <p>Here the <code>linux1</code> node will have <code>source2.json</code> mounted to <code>/dst.json</code> instead of <code>source1.json</code> that is defined on the kind level.</p>"},{"location":"rn/0.43/#execute-on-host","title":"Execute on host","text":"<p>With #1481 users can now execute commands on the host machine defined in the topology file. Typically this is used to prepare host environment for the lab in question.</p> <p>Now by specifying the <code>host</code> node in the topology you can add <code>exec</code> section to it and this will tell containerlab to execute commands on the host machine.</p>"},{"location":"rn/0.43/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>when dynamic port bindings are used they are now printed in the <code>inspect -f json</code> and the <code>topology-data.json</code> file #1430</li> <li>users with authentication via AD can now properly enjoy containerlab's ability to use the home directory of the user that runs the command #1449</li> <li>kernel check has been fixed in #1460</li> </ul>"},{"location":"rn/0.44/","title":"Release 0.44","text":"<p> 2023-08-17 \u00b7  Full Changelog</p>"},{"location":"rn/0.44/#link-impairments","title":"Link Impairments","text":"<p>A long-awaited feature landed in this release. We are adding two new tools commands:</p> <ul> <li><code>tools netem set</code></li> <li><code>tools netem show</code></li> </ul> <p>These commands allow users to set link impairments (delay, jitter, packet loss) on any link that belongs to a container node and create labs simulating real-world network conditions.</p> setting packet loss at 10% rate on eth1 interface of clab-netem-r1 node<pre><code>containerlab tools netem set -n clab-netem-r1 -i eth1 --loss 10\n</code></pre>"},{"location":"rn/0.44/#external-ca","title":"External CA","text":"<p>Containerlab used to generate Certificate Authority certs and keys to create node certs. While this satisfies most lab deployments, some users wanted to bring their own CA to containerlab. And here it is.</p> <p>Now it is possible to provide a path to external CA cert and key files via a newly introduced <code>settings</code> section in the clab file which will instruct containerlab to create node certs using this \"external\" CA.</p> <p>Read more about this feature in the Certificates Management document.</p>"},{"location":"rn/0.44/#aruba-aos-cx","title":"Aruba AOS-CX","text":"<p>Aruba AOS-CX comes to containerlab! Thanks to @ssasso for his first and we hope not last contribution.</p>"},{"location":"rn/0.44/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>test harness powered by robot has been upgraded to rf 6.1.1 and CLAB_BIN env var is unified across all tests #1506</li> <li>instructions how to run Containerlab on Macs with ARM chips added in #1508</li> <li>ssh keys installation for srlinux has been adapted to support config-based ssh keys #1511</li> <li>big internal refactoring of links done by @steiler in #1475</li> </ul>"},{"location":"rn/0.44/#patches","title":"Patches","text":""},{"location":"rn/0.44/#0441","title":"0.44.1","text":"<ul> <li>fix: TCP sessions on srlinux were not establishing due to missing disabled tx offload #1537</li> <li>fix: links to ovs bridges were blocked #1534</li> </ul>"},{"location":"rn/0.44/#0442","title":"0.44.2","text":"<ul> <li>fixing concurrent attempt to link deployment #1541</li> </ul>"},{"location":"rn/0.44/#0443","title":"0.44.3","text":"<ul> <li>fixing CLAB_INTFS env var #1547</li> <li>fixing node filtering functionality #1549</li> <li>fixing ovs bridges and openflow 1.3 #1539</li> <li>remove ovs ports when the lab is destroyed #1545</li> <li>added doc entry for extended link format #1531</li> </ul>"},{"location":"rn/0.45/","title":"Release 0.45","text":"<p> 2023-09-20 \u00b7  Full Changelog</p>"},{"location":"rn/0.45/#vjunos-switch","title":"vJunos-switch","text":"<p>Thanks to contributions from @akielaries done in #1553 and accompanying vrnetlab PR containerlab gets support for Juniper vJunos-switch. This is a new VM-based virtual product from Juniper that has been integrated in containerlab by juniper folks, kudos to them!</p>"},{"location":"rn/0.45/#find-topology-file-in-a-directory","title":"Find topology file in a directory","text":"<p>For quite a while containerlab had two ways of finding which topology file to use when deploying/destroing/inspecting a lab:</p> <ol> <li>when users provide a path to a topology file via <code>-t</code> flag, containerlab will use that file</li> <li>when users do not provide a path to a topology file (no <code>-t</code> flag present), containerlab would search for a file named <code>*.clab.yml</code> in the current directory and use it.</li> </ol> <p>Now @steiler enhanced this functionality by making sure that if a user provides a path with <code>-t</code> flag that points to a directory, then we will apply topology file finding logic in this directory, instead of bailing out with an error.</p>"},{"location":"rn/0.45/#suppress-startup-config","title":"suppress-startup-config","text":"<p>@bewing added a new node parameter called <code>suppress-config</code> to give users ability to suppress startup config generation for a given node or a set of nodes. This is particularly useful in scenarios when you want to test ZTP and thus make sure containerlab doesn't add anything on its own accord.</p>"},{"location":"rn/0.45/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>ansible proxy variable is added to containerlab produced ansible inventory #1571</li> <li>ca cert and node certs now have a default country code set to US #1592</li> <li>default link MTU is now fixed to be 9500 again #1583</li> </ul>"},{"location":"rn/0.45/#patches","title":"Patches","text":""},{"location":"rn/0.45/#0451","title":"0.45.1","text":"<ul> <li>fix panic when no keys are present for srlinux node</li> </ul>"},{"location":"rn/0.46/","title":"Release 0.46","text":"<p> 2023-10-08 \u00b7  Full Changelog</p>"},{"location":"rn/0.46/#declarative-vxlan-links-handling","title":"Declarative VXLAN links handling","text":"<p>In #1532 @steiler added a couple of new link types - <code>vxlan</code> and <code>vxlan-stitch</code> that allow users to declaratively define VXLAN links between nodes.</p> <p>Our Clabernetes project will use the <code>vxlan-stitch</code> link type to declaratively define links between network elements on different worker nodes.</p> <p>Note</p> <p>The default vxlan port is <code>14789</code>.</p> vxlanvxlan-stitch <p>The <code>vxlan</code> link type creates a VXLAN link in the host network namespace and moves it to the node's netns as if this link belongs to the node itself.</p> <pre><code>name: vxlan-embed\n\ntopology:\nnodes:\n    srl1:\n    kind: nokia_srlinux\n    image: ghcr.io/nokia/srlinux\n\nlinks:\n    - type: vxlan\n    endpoint:\n        node: srl1\n        interface: e1-1\n        mac: 02:00:00:00:00:04\n    remote: 172.20.25.22 #(1)!\n    vni: 100\n    udp-port: 14788\n</code></pre> <ol> <li>The remote IP address must exist in the host network namespace.</li> </ol> <p>The <code>vxlan-stitch</code> mode creates vxlan link in the host netns, but contrary to <code>vxlan</code> link type it doesn't move the link to the node's netns; instead it creates a veth link between the node and the host, and uses tc redirect rules to stitch the veth link to the vxlan link.</p> <p>This is exactly the same approach as used by the <code>tools vxlan create</code> command.</p> <pre><code>name: vxlan-stitch\n\ntopology:\n  nodes:\n    srl1:\n    kind: nokia_srlinux\n    image: ghcr.io/nokia/srlinux\n\n  links:\n    - type: vxlan-stitch\n      endpoint:\n        node: srl1\n        interface: e1-1\n        mac: 02:00:00:00:00:04\n      remote: 172.20.25.22 #(1)!\n      vni: 100\n      udp-port: 14788\n</code></pre> <ol> <li>The remote IP address must exist in the host network namespace.</li> </ol>"},{"location":"rn/0.46/#containerlab-on-apple-m1m2","title":"Containerlab on Apple M1/M2","text":"<p>Systems that requires specific x86 CPU flags or nested virtualization may be launched with a UTM/Qemu VM as documented in the newly added Apple MacOS and ARM chapter.</p> <p>We prepared the UTM image with containerlab installed to make it easier to get started running CPU-picky systems on Apple M1/M2.</p>"},{"location":"rn/0.46/#stdin-topologies","title":"Stdin topologies","text":"<p>Now when #1621 is merged, you can pass the topology definition via stdin. This opens up a lot of possibilities for automation and integration with other tools. Most common use case is to use <code>curl</code> to fetch a remote topology and immediately pass it to containerlab.</p> <p>For example, this is how you can spin up a topology with a single SR Linux node whenver you need it:</p> <pre><code>curl -sL srlinux.dev/clab-srl | clab dep -c -t -\n</code></pre> <p>Couple that with containerlab' ability to use env variables in the topology file and you have a clever way to spin up parametrized topologies with a single command:</p> <pre><code>curl -sL srlinux.dev/clab-srl | \\\n  SRL_VERSION=23.3.3 SRL_TYPE=ixrd2l clab dep -c -t -\n</code></pre>"},{"location":"rn/0.46/#patches","title":"Patches","text":""},{"location":"rn/0.46/#0461","title":"0.46.1","text":"<ul> <li>fixed <code>tools vxlan create</code> #1625.</li> </ul>"},{"location":"rn/0.46/#0462","title":"0.46.2","text":"<ul> <li>fixed macvlan link provisioning #1635</li> <li>added learning flag for vxlan interface #1636</li> </ul>"},{"location":"rn/0.47/","title":"Release 0.47","text":"<p> 2023-10-20 \u00b7  Full Changelog</p>"},{"location":"rn/0.47/#remote-labs","title":"Remote labs","text":"<p>Making labs easily accessible is one of the main goals of Containerlab. With this release, we are introducing a new feature that allows you to deploy labs even faster and easier. You can now deploy labs by simply referring to a github repository!</p> <p>Containerlab will clone the repository on your behalf and deploy the topology. Hey, it can't be easier than that!</p> <p>Read more about this feature in the deploy command reference.</p> <p>Delivered in #1654.</p>"},{"location":"rn/0.47/#auto-provisioning-of-dns-servers-for-the-sr-linux-management-vrf","title":"Auto-provisioning of DNS servers for the SR Linux management VRF","text":"<p>Containerlab is now able to automatically detect which DNS servers are configured on the hosts and configure them in the SR Linux'es management VRF accordingly. This aligns SR Linux nodes behavior with the behavior of regular containers launched by Docker where the DNS requests are handled by Docker's DNS resolver #1650.</p>"},{"location":"rn/0.47/#ssh-config","title":"SSH Config","text":"<p>Even though auto accepting the changed host key is in the DNA of network engineers, the warning message is still annoying.</p> <p>Containerlab now generates an SSH config file for each lab that removes the host key check and sets the username to the one known to Containerlab. Making it even easier to quickly SSH into the nodes #1660.</p>"},{"location":"rn/0.47/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>fixes to mgmt0 interface config on SR Linux nodes #1641</li> <li>MAC address config for macvlan links #1642</li> <li>documentation fix for extended links format #1649</li> <li>podman exec command fix #1653</li> <li>APT/YUM repositories are now automatically added to all SR Linux nodes. They are used to install NDK apps and CLI plugins provided by Nokia #1657</li> <li>SR Linux's mgmt0.0 interface is now auto-configured with the correct IP-MTU value #1658</li> </ul>"},{"location":"rn/0.47/#patches","title":"Patches","text":""},{"location":"rn/0.47/#0471","title":"0.47.1","text":"<ul> <li>Do not extract more than 3 DNS servers from the host's resolv.conf #1671, #1672</li> </ul>"},{"location":"rn/0.47/#0472","title":"0.47.2","text":"<ul> <li>Do not populate DNS servers for nodes in <code>network-mode: container:&lt;some-container&gt;</code> as these options are mutually exclusive #1676</li> </ul>"},{"location":"rn/0.48/","title":"Release 0.48","text":"<p> 2023-11-13 \u00b7  Full Changelog</p>"},{"location":"rn/0.48/#improvements-to-lab-cloning","title":"Improvements to lab cloning","text":"<p>In #1694 @steiler made the lab cloning process really smart. Containerlab will check if the repo already cloned and will not clone it again. Instead it will pull the latest changes from the repo and will change the branch if one was used in the URL. Additionally, it is now also possible to copy the URL to a lab file that sits in a subdirectory of the repo. Containerlab will clone the repo and will then look for the lab file in the specified subdirectory.</p> <p>Another improvement added in #1704 allows you to specify the GitHub URL in its shortest form - <code>user/repo</code> - and Containerlab will automatically expand it to the full URL and will clone the repo. This is a very convenient way to quickly deploy labs from GitHub, less typing, less errors, more fun!</p> <p>Here is a rundown of the new ways you can deploy labs:</p>"},{"location":"rn/0.48/#ssh-key-provisioning-for-nokia-sr-os","title":"SSH key provisioning for Nokia SR OS","text":"<p>Nokia SR OS users get to enjoy a feature that was previously available only for SR Linux - the auto-provisioning of public SSH keys to enable password-less access.</p> <p>Containerlab retrieves public keys from local files at <code>~/.ssh</code> as well as extracts them from the <code>ssh-agent</code> if it is running. The keys are then provisioned to the SR OS nodes once they are up and running enabling password-less access to the nodes.</p> <p>Users will notice that with that change the lab deployment process will take a bit longer as Containerlab will wait for the SR OS nodes to boot up and will provision the keys before proceeding with the rest of the lab deployment.</p>"},{"location":"rn/0.48/#goodbye-vr-prefix","title":"Goodbye <code>vr-</code> prefix!","text":"<p>For a very long time we used <code>vr-&lt;NOS&gt;</code> kind names to denote the VM-based lab nodes - a naming prefix adopted from vrnetlab project. After we introduced a common naming scheme for kind names <code>&lt;vendor&gt;-&lt;NOS&gt;</code> we decided to deprecate the old naming scheme and to remove the <code>vr-</code> prefix from the VM-based nodes.</p> <p>Now <code>vr-nokia_sros</code> becomes <code>nokia_sros</code>, <code>vr-juniper_vqfx</code> becomes <code>juniper_vqfx</code> and so on. The old names with <code>vr</code> prefix are considered deprecated and will be removed later.</p>"},{"location":"rn/0.48/#directory-acls","title":"Directory ACLs","text":"<p>Now containerlab will add group ACLs to the lab directory to allow original user to access the lab files without requiring <code>sudo</code> #1701.</p>"},{"location":"rn/0.48/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Installer script now allows installing <code>apk</code> packages #1681</li> <li>TLS verification is now skipped when containerlab downloads files from HTTPS locations to allow downloading from self-signed HTTPS servers #1693</li> <li>Juniper vSRX lab example added in #1695</li> <li>Containerlab now errors early if the link is missing the <code>:</code> separator #1691</li> <li>Improvements to the devcontainer documentation #1689</li> </ul>"},{"location":"rn/0.48/#patches","title":"Patches","text":""},{"location":"rn/0.48/#0481","title":"0.48.1","text":"<ul> <li>Do not set <code>unbound</code> pubkey authentication option for older ssh clients #1721</li> </ul>"},{"location":"rn/0.48/#0482","title":"0.48.2","text":"<ul> <li>fixing error handling during SR OS config push #1723</li> <li>setting file ACLs for cloned repos #1720</li> <li>SR OS ssh keys to start from index 32 downwards #1724</li> <li>setting proper dir permissions for cloned repos #1726</li> </ul>"},{"location":"rn/0.48/#0483","title":"0.48.3","text":"<ul> <li>fix exec command failing to filter on the topology nodes #1729</li> </ul>"},{"location":"rn/0.48/#0484","title":"0.48.4","text":"<ul> <li>fixed <code>destroy --cleanup</code> command not removing lab directory located outside of the current working dir #1738</li> <li>fixed <code>mgmt-net</code> link provisioning #1741</li> <li>fixed setting up unix socket access for gnmi/gnoi and SR Linux 23.10.1 #1740</li> </ul>"},{"location":"rn/0.48/#0485","title":"0.48.5","text":"<ul> <li>fixed <code>destroy -t &lt;dir&gt;</code> case where topology file referred a directory #1747</li> <li>fixed SR OS getting stuck when partial config is used with ssh keys provisioning #1750</li> <li>introduced <code>CLAB_SKIP_SROS_SSH_KEY_CONFIG</code> env var to skip SSH key provisioning for SR OS #1756</li> </ul>"},{"location":"rn/0.48/#0486","title":"0.48.6","text":"<ul> <li>added <code>exit all</code> command to SR OS config push to fix prompt handling #1757</li> </ul>"},{"location":"rn/0.49/","title":"Release 0.49","text":"<p> 2023-12-21 \u00b7  Full Changelog</p>"},{"location":"rn/0.49/#healthchecks","title":"Healthchecks","text":"<p>@steiler added support for container healthchecks in #1426. Now you can specify healthcheck parameters for your containers in the lab topology file and Containerlab will configure the healthcheck for you.</p> <p>This allows you to monitor health of your containerlab nodes as well as serves as a prerequisite for the upcoming dependency manager improvements to create dependencies on healthiness of the lab nodes.</p>"},{"location":"rn/0.49/#openbsd-support","title":"OpenBSD support","text":"<p>Fearless @dteslya added support for the mighty OpenBSD in #1762. Not only you can deploy labs with OpenBSD, but also startup config support were baked in.</p>"},{"location":"rn/0.49/#juniper-evolved-evo-support","title":"Juniper Evolved (EVO) support","text":"<p>Thanks to @akielaries Containerlab got another platform under its belt! Welcome Juniper vJunos-Evolved #1775.</p>"},{"location":"rn/0.49/#cisco-ftdv-support","title":"Cisco FTDv support","text":"<p>Again thanks to @dteslya we now have support for Cisco FTDv platform #1783.</p>"},{"location":"rn/0.49/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Ctrl+C now does not remove the lab files #1769</li> <li>When using filters to execute commands with <code>exec</code>, the error is now returned if no nodes match the filter #1786</li> <li>ixia-c example lab has been updated by @bortok to feature latest developments done for the ixia-c kind #1795</li> </ul>"},{"location":"rn/0.50/","title":"Release 0.50","text":"<p> 2024-01-29 \u00b7  Full Changelog</p>"},{"location":"rn/0.50/#k8s-kind","title":"k8s-kind","text":"<p>Containerlab now natively supports kind clusters as part of its topology by introducing a new kind <code>k8s-kind</code>.</p> <p>Since more and more applications (including network management systems and network functions) are being deployed in the k8s clusters, it is important to be able to test the network connectivity between the k8s workloads and the underlay network.</p> <p>Kind is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. By integrating kind clusters via a new kind <code>k8s-kind</code> with containerlab, it is possible to spin-up kind clusters as part of the containerlab topology.</p> <p>This deployment model unlocks the possibility to integrate network underlay created by containerlab with the workloads running in the kind clusters in a single YAML file. The integration between kind clusters and containerlab topology makes it easy to deploy and interconnect k8s clusters and the underlay network.</p>"},{"location":"rn/0.50/#ansible-inventory-improvements","title":"Ansible inventory improvements","text":"<p>We've shipped containerlab with the automatic Ansible inventory generation for quite a while now. It is a very convenient way to get the inventory file for the Ansible playbooks that can be used to configure the nodes in the topology.</p> <p>Now we've improved the inventory generation by adding some additional Ansible variables for a few node kinds. If you use <code>nokia_srlinux</code> or <code>nokia_sros</code> kinds you will find the following variables in the inventory file:</p> <ul> <li><code>ansible_user</code> - set to the default username of the node</li> <li><code>ansible_password</code> - set to the default password of the node</li> <li><code>ansible_connection</code> - set to <code>network_cli</code> for the SR OS nodes and to <code>httpapi</code> for the SR Linux nodes</li> <li><code>ansible_network_os</code> - set to <code>nokia.sros.sros</code> for the SR OS nodes and to <code>nokia.srlinux.srlinux</code> for the SR Linux nodes</li> </ul> <p>With these variables set you can start using Ansible right away without setting these variables elsewhere.</p> <p>If you need the same functionality for other node kinds, please raise a PR or an issue.</p>"},{"location":"rn/0.50/#using-nftables-api","title":"Using nftables API","text":"<p>With #1362 containerlab now starts using nftables API backend when installing iptables rules to allow external access for the nodes.</p> <p>NFtables API is a new way to manage nftables/iptables rules and has been part of the kernel for quite a while. Still, some old kernels known to be used in Centos/RHEL distros up to and including version 7 might not have the nftables API available.</p> <p>In that case, containerlab will fail to setup external access rules and users would have to do it manually.</p>"},{"location":"rn/0.50/#sans-move-to-certificate-level","title":"SANs move to Certificate level","text":"<p>Warning</p> <p>This is a non backwards compatible change!</p> <p>We offered the users to set additional Subject Alternative Names (SANs) for the generated certificates via the <code>.san</code> node property. But then we introduced the <code>.certificate</code> node property to tune certificate parameters for a given node or a kind.</p> <p>Now we are moving the SANs to the certificate level to keep things under one roof.</p>"},{"location":"rn/0.50/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>TLS certificates that containerlab provides for the nodes now also include <code>KeyEncipherment</code> usage role #1822</li> <li>xrv9k default mem and cpu values have been increased to 16 GB and 2 vCPUs respectively #1806</li> <li>SR Linux <code>admin</code> user's rc files are now owned by <code>admin</code> user #1832</li> <li>Updated wireshark docs #1842</li> <li>Config apply timeout for the SR OS nodes has been increased to 15 minutes #1835</li> </ul>"},{"location":"rn/0.51/","title":"Release 0.51","text":"<p> 2024-02-14 \u00b7  Full Changelog</p>"},{"location":"rn/0.51/#stages","title":"Stages","text":"<p>The biggest change in this release is rightfully claimed by the introduction of the stages feature.</p> <p>Stages are a way to define stages a node goes through during its lifecycle and the interdependencies between the different stages of different nodes in the lab. This feature gives you superpowers in terms of ordering how different nodes \"wait\" on each other using different stages and their dependencies.</p> <p>With <code>stages</code> and the <code>wait-for</code> feature you can now make your VM nodes to boot in the staggered order to maintain the CPU utilization low.</p> <pre><code>name: bootdelay\ntopology:\n  nodes:\n    sr1:\n      kind: nokia_sros\n      image: nokia_sros:latest\n    sr2:\n      kind: nokia_sros\n      image: nokia_sros:latest\n      stages:\n        create:\n          wait-for:\n            - node: sr1\n              stage: healthy\n</code></pre> <p>Or you can have nodes that need to start after the other nodes are fully operational by combining <code>stages</code>, <code>wait-for</code> and <code>healthchecks</code>.</p> <p>Check out the stages documentation for more details.</p>"},{"location":"rn/0.51/#edgeshark-integration","title":"Edgeshark integration","text":"<p>We have seriously upped our packet capturing game by discovering, integrating and improving the Edgeshark project. Now you can capture packets from your containerlab nodes using a sleek web interface!</p>"},{"location":"rn/0.51/#iptables-are-back","title":"iptables are back","text":"<p>When we moved to using <code>nftables</code> as the default firewall backend in v0.50.0 release, we removed the support for <code>iptables</code> assuming that people ditched old kernels in 2024 for good. Buuut, you failed us.</p> <p>So we brought back the support for <code>iptables</code> as a firewall backend while keeping <code>nftables</code> as well. Now we will detect if nftables is available and use it, otherwise we will fall back to <code>iptables</code>. #1850</p>"},{"location":"rn/0.51/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Support for anonymous bind mounts #1853</li> <li>SR Linux config has been updated to support the coming 24.3 release #1845</li> <li>Ansible inventory file will not have an empty vars section if no vars are defined #1863</li> <li>Added a basic SR OS CI pipeline #1852</li> </ul>"},{"location":"rn/0.51/#patches","title":"Patches","text":""},{"location":"rn/0.51/#0511","title":"0.51.1","text":"<ul> <li>fixed erroneous error log message about the stages not found #1892</li> </ul>"},{"location":"rn/0.51/#0512","title":"0.51.2","text":"<ul> <li>improved nftables vs iptables backend detection for the rule installation #1900</li> <li>the log for k8s-kind cluster provisioning is now shown in the containerlab deployment log #1894</li> </ul>"},{"location":"rn/0.51/#0513","title":"0.51.3","text":"<ul> <li>fixed an interdependency issue between the nodes with a link between them and an associated exec #1907</li> </ul>"},{"location":"rn/0.52/","title":"Release 0.52","text":"<p> 2024-03-05 \u00b7  Full Changelog</p>"},{"location":"rn/0.52/#links-and-stages","title":"Links and Stages","text":"<p>In #1914 @steiler made a big leap forward by unblocking links creation between the nodes. Previously a link could only be created between the nodes if both node-backing containers were running.</p> <p>This interdependency between the links and nodes was a big blocker for the stages feature, since you couldn't, say, make one node to wait on another if they had a link between them. Effectively this reduced the usefullness of the stages particularly for VM-based nodes.</p> <p>Now, the links creation is handled on a per-node basis, which means that a node creates its side of the link as soon as it is ready, regardless of the state of the other node. The other node will pick its part of the link and attach it whenever it is ready.</p> <p>This change makes it possible for <code>nodeA</code> to depend on <code>nodeB</code> using stages, even if they have a link between them. Here is how it works:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant A as Node A\n    participant H as Host NS\n    participant B as Node B\n\n    Note over A: Reaches healthy stage\n    activate A\n    loop Healthcheck\n        B-&gt;&gt;B: waiting to reach the healthy stage\n    end\n    A-&gt;&gt;H: Creates a veth pair with remote end&lt;br/&gt; in the host namespace\n    Note over B: reaches healthy stage\n    activate B\n    B-&gt;&gt;H: takes the remote end from the host&lt;br/&gt;namespace and attaches it to the container\n    A-&gt;&gt;B: The veth link has been created\n    deactivate A\n    deactivate B</code></pre> <p>With links asynchronous creation, you can now define the stages for your nodes and make them wait on each other using the <code>wait-for</code> feature!</p>"},{"location":"rn/0.52/#per-stage-exec","title":"Per-stage exec","text":"<p>With Stages introduction in the previous release, we opened new possibilities for the nodes lifecycle management. Now, @steiler expanded it even further by adding the per-stage exec option to the stages.</p> <p>With per-stage command execution the user can define <code>exec</code> block under each stage; moreover, it is possible to specify when the commands should be run <code>on-enter</code> or <code>on-exit</code> of the stage.</p> <pre><code>nodes:\n  node1:\n    stages:\n      create-links:\n        exec:\n          on-enter:\n            - ls /sys/class/net/\n</code></pre> <p>In the example above, the <code>ls /sys/class/net/</code> command will be executed when <code>node1</code> is about to enter the <code>create-links</code> stage. As expected, the command will list only interfaces provisioned by docker (eth0 and lo), but none of the containerlab-provisioned interfaces, since the create-links stage has not been finished yet.</p> <p>Per-stage command execution gives you additional flexibility in terms of when the commands are executed, and what commands are executed at each stage.</p>"},{"location":"rn/0.52/#k8s-kind-extra-options","title":"k8s-kind extra options","text":"<p>In #1912 @YutaroHayakawa added <code>kind</code> options to containerlab definition file to add extra extensibility to the way you can deploy kind clusters.</p>"},{"location":"rn/0.52/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>long-named links will use interface's AltName instead of Aliases #1908</li> <li>fixed doubled log messages for SR OS nodes #1919</li> </ul>"},{"location":"rn/0.53/","title":"Release 0.53","text":"<p> 2024-03-25 \u00b7  Full Changelog</p>"},{"location":"rn/0.53/#fortinet-fortigate","title":"Fortinet Fortigate","text":"<p>@robotwalk added support for the Fortinet Fortigate firewall image. Refer to the <code>fortinet_fortigate</code> kind documentation to learn more about the supported options.</p>"},{"location":"rn/0.53/#freebsd","title":"FreeBSD","text":"<p>After clearing the OpenBSD, @dteslya added support for the FreeBSD kind as well!</p>"},{"location":"rn/0.53/#containerlab-to-drawio","title":"Containerlab to Drawio","text":"<p>@FloSch62 created a new tool - clab-io-draw - that allows containerlab users to generate drawio (aka diagrams.net) diagrams from their containerlab topologies. The tool is available as a standalone container image, and we packaged it under the <code>containerlab graph --drawio</code> command to make it easier to use.</p> <p></p> <p>Note</p> <p>This feature is in the \"preview\" status, we will likely change the command structure in the next release.</p>"},{"location":"rn/0.53/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Support for SR Linux release 24.3+</li> <li>fixed container network mode #1940</li> <li>first steps in making containerlab consumable as a package #1906</li> </ul>"},{"location":"rn/0.54/","title":"Release 0.54","text":"<p> 2024-04-04 \u00b7  Full Changelog</p>"},{"location":"rn/0.54/#cisco-c8000v","title":"Cisco c8000v","text":"<p>@mzagozen added support for the long-awaited Cisco c8000v router image. Cisco c8000v is a successor of Cisco CSR1000v and is a different product from Cisco 8000 platform emulator.</p> <p>Refer to the <code>cisco_c8000v</code> kind documentation to learn more about the supported options.</p>"},{"location":"rn/0.54/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>root login allowed for crpd #1967</li> <li>support for podman local images #1969</li> <li>fixes to SR Linux v24.3+ startup config #1968 #1971</li> </ul>"},{"location":"rn/0.54/#patches","title":"Patches","text":""},{"location":"rn/0.54/#0541","title":"0.54.1","text":"<ul> <li>add <code>insecure-mgmt</code> gRPC server to the default SR Linux config #1979</li> <li>use Docker v25 API and Podman v5 #1980</li> </ul>"},{"location":"rn/0.54/#0542","title":"0.54.2","text":"<ul> <li>fix inspect command behavior with respect to the named-based lab fetching #1984</li> </ul>"},{"location":"rn/0.55/","title":"Release 0.55","text":"<p> 2024-06-09 \u00b7  Full Changelog</p>"},{"location":"rn/0.55/#all-in-one-setup-script","title":"All-in-one setup script","text":"<p>We have been rocking quite some workshops and demos lately, and we realized that we can improve the user experience by providing a single script that installs containerlab and its prerequisites in one go.</p> <p>Since the ephemeral nature of containerlab labs started to attract more users, we decided to make the install script that would get you up and running in no time on a bare Apt/Yum-based Linux system.</p> <pre><code>curl -L http://containerlab.dev/setup \\\n| sudo bash -s \"all\"\n</code></pre> <p>This script will detect if apt or yum-based distribution is used and will install the following:</p> <ul> <li>latest Docker Community Edition - <code>docker-ce</code></li> <li>GitHub CLI - <code>gh</code></li> <li>Latest Containerlab release</li> </ul> <p>In addition to that, the script will setup sshd to allow 50 login attempts to accommodate for users with lengthy key chains.</p> <p>Do you want to install only docker or only containerlab? No problem, just run:</p> only dockeronly containerlab <pre><code>curl -L http://containerlab.dev/setup \\\n| sudo bash -s \"install-docker\"\n</code></pre> <pre><code>curl -L http://containerlab.dev/setup \\\n| sudo bash -s \"install-containerlab\"\n</code></pre>"},{"location":"rn/0.55/#generic-linux-vm-kind","title":"Generic Linux VM kind","text":"<p>We have added a new kind - <code>generic_vm</code> - that allows you to add a generic VM to your lab. An Ubuntu 20.04 image has been tested with this kind built with hellt/vrnetlab and a lab example has been provided. #1976</p>"},{"location":"rn/0.55/#auto-assigned-management-network-address","title":"Auto-assigned management network address","text":"<p>@mzagozen added support for the auto-assigned management network address to avoid collisions with the existing networks. Refer to the network documentation for more details.</p>"},{"location":"rn/0.55/#dummy-interface-support","title":"Dummy interface support","text":"<p>Whenever you need to add an interface to a node that does nothing but occupy a port, use the <code>dummy</code> interface type. #2055</p>"},{"location":"rn/0.55/#crpd-improvements","title":"cRPD improvements","text":"<p>License handling has been fixed for the recent cRPD versions. #2014</p> <p>Netconf has been enabled by default for cRPD. #2015</p>"},{"location":"rn/0.55/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>fixed NSPath handling for ignite runtime #2000</li> <li>SR OS interface check has been relaxed to allow specifying the interface name in any range, not limited by 32 ports #2012</li> <li>Fixed install on Alpine systems #2017</li> <li>added <code>--skip-labdir-acl</code> flag to the deploy command to allow skipping the lab directory ACL setup #2028</li> <li>cEOS examples have been updated to use 4.32.0F version #2034</li> <li>package repo links have been updated to enable v6 access #2032</li> <li>codespaces image have been added #2058 #2059 #2064 #2067</li> </ul>"},{"location":"rn/0.55/#patches","title":"Patches","text":""},{"location":"rn/0.55/#0551","title":"0.55.1","text":"<ul> <li>fixed ACL rules generation for SR Linux in cases where no ssh keys are present #2103</li> <li>switched to D2L platform as the default for SR Linux #2094</li> <li>added impairments docs section</li> </ul>"},{"location":"rn/0.56/","title":"Release 0.56","text":"<p> 2024-07-09 \u00b7  Full Changelog</p>"},{"location":"rn/0.56/#interface-aliases","title":"Interface aliases","text":"<p>We know, it has been a long time coming, but thanks to a brilliant contribution done by @vista- in #2124 we now have added support for interface aliases.</p> <p>What are interface aliases you ask? They are a way to use interface names as you see them in the NOS CLI, rather than the default <code>ethX</code> naming scheme. It is a convenience feature, but, oh boy, how much easier is it to define links when you don't have to keep doing the mental gymnastics of mapping <code>eth0</code> to <code>Gi0/1</code> and so on.</p> <p>Imagine we want to create a lab with four different Kinds: SR Linux, vEOS, CSR1000v and vSRX, cabled like this:</p> A side B side SR Linux ethernet-1/1 vEOS Ethernet1/1 vSRX ge-0/0/2 vEOS Ethernet1/2 CSR1000v Gi5 vSRX ge-0/0/5 vEOS Ethernet1/3 CSR1000v Gi3 Using Linux interfacesUsing interface aliases <p>Using the <code>ethX</code> interface naming convention, the topology would look like this:</p> <pre><code>links:\n  - endpoints: [\"srl:e1-1\", \"vEOS:eth1\"]\n  - endpoints: [\"vSRX:eth3\", \"vEOS:eth2\"]\n  - endpoints: [\"CSR1000v:eth4\", \"vSRX:eth6\"]\n  - endpoints: [\"vEOS:eth3\", \"CSR1000v:eth2\"]\n</code></pre> <p>Note the four different kinds of offset used here on the four different NOSes!</p> <p>Using aliased interface names, the topology definition becomes much more straightforward:</p> <pre><code>links:\n  - endpoints: [\"srl:ethernet-1/1\", \"vEOS:Ethernet1/1\"]\n  - endpoints: [\"vSRX:ge-0/0/2\", \"vEOS:Ethernet1/2\"]\n  - endpoints: [\"CSR1000v:Gi5\", \"vSRX:ge-0/0/5\"]\n  - endpoints: [\"vEOS:Ethernet1/3\", \"CSR1000v:Gi3\"]\n</code></pre> <p>Both topology definitions result in the same lab being deployed, but the latter is easier to write and to understand.</p> <p>Many Kinds (but not all) support interface aliases and the alias names are provided in the respective kind' documentation.</p> <p>Containerlab transparently maps from interface aliases to Linux interface names, and there's no additional syntax or configuration needed to specify either an interface alias or a Linux interface name in topologies.</p> <p>Interface aliases will be also displayed in the generated graph and even in the edgeshark web view.</p> GraphEdgeshark <p></p> <p></p>"},{"location":"rn/0.56/#ostinato-support","title":"Ostinato support","text":"<p>We have added support for Ostinato traffic generator in containerlab. You can now define Ostinato nodes in your topology file and use them to generate traffic between your nodes.</p> <p>See the Ostinato integration page for more details.</p>"},{"location":"rn/0.56/#sonic","title":"SONiC","text":"<p>We have added support for the following new SONiC kinds:</p> <ul> <li><code>sonic-vm</code> - is a VM-based upstream SONiC. You may want to pick sonic-vm over sonic-vs if you hit some issues with the original sonic container. Added by @adam-kulagowski in #2120</li> <li><code>dell_sonic</code> - is a SONiC distribution by Dell #2125</li> </ul>"},{"location":"rn/0.56/#multi-arch-clab-container-build","title":"Multi-arch <code>clab</code> container build","text":"<p>To support the imminent release of ARM64 friendly Network OSes we fine tuned the release pipeline to make sure the <code>clab</code> container is built for both <code>amd64</code> and <code>arm64</code> architectures. #2128</p>"},{"location":"rn/0.56/#cisco-catalyst-9000v","title":"Cisco Catalyst 9000v","text":"<p>Thanks to our strong community and @kaelemc in particular, we have added support for the Cisco Catalyst 9000v switch. #2133</p>"},{"location":"rn/0.56/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Additions to the Developers Guide done by @pstavirs go about how to contribute to the documentation and how to build the documentation locally. #2109</li> <li>Added more community posts in #2127</li> </ul>"},{"location":"rn/0.57/","title":"Release 0.57","text":"<p> 2024-08-22 \u00b7  Full Changelog</p>"},{"location":"rn/0.57/#dell-sonic-startup-config-handling","title":"Dell SONiC startup config handling","text":"<p>The Dell SONiC support has been improved by adding startup-config handling and saving #2175</p>"},{"location":"rn/0.57/#quick-setup-script-improvements","title":"Quick setup script improvements","text":"<p>The quick setup script has been improved to ensure even smoother installation experience.</p> <ul> <li>RHEL-based systems are now supported so you can install containerlab in one click on your RHELs and Rocky's. #2137</li> <li>a sudoers file is now created to allow SSH_AUTH_SOCK env var to persist when using sudo. This allows the user to use the ssh agent of the original user when deploying a lab with <code>sudo</code></li> <li> <p>it is no fun to use a cloud system with a lousy shell prompt. If you don't have time to setup your own custom prompt, you can now use one of the functions in the quick setup script to install a lightweight 2-line bash prompt #2174</p> <pre><code>curl -sL https://containerlab.dev/setup | sudo -E bash -s \"setup-bash-prompt\"\n</code></pre> </li> </ul>"},{"location":"rn/0.57/#lab-owner-information","title":"Lab owner information","text":"<p>When running in multi-user environments with several lab owners using the same containerlab host, it is important to know which lab belongs to which user. To address this, a new table column \"Owner\" was added to the output.</p> <p>This column is populated with the username of the user who started the lab and is displayed when <code>--wide</code> flag is provided to the <code>inspect</code> command. #2161</p>"},{"location":"rn/0.57/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>to decouple the containerlab CLI from the clab2drawio project, the <code>--drawio-args</code> flag was added to the <code>graph</code> command to allow customization of the drawio generation out of band #2143</li> <li>Ixia/Keysight OTG updates #2156</li> <li>improvements to docker user password handling #2167</li> <li>various template files have been embedded into the binary, instead of polluting the <code>/etc/containerlab</code> directory #2169</li> </ul>"},{"location":"rn/0.57/#patches","title":"Patches","text":""},{"location":"rn/0.57/#0571","title":"0.57.1","text":"<ul> <li>fixed handling of the docker user password containing special characters #2187</li> </ul>"},{"location":"rn/0.57/#0572","title":"0.57.2","text":"<ul> <li>fixed erroneous warn log when command execution succeeded</li> </ul>"},{"location":"rn/0.57/#0573","title":"0.57.3","text":"<ul> <li>fix YANG browser link in the SR Linux' banner #2201</li> <li>fix trailing whitespaces in the type parameter #2199</li> </ul>"},{"location":"rn/0.57/#0574","title":"0.57.4","text":"<ul> <li>set docker version to 26.1.4 in the devcontainer and quick setup scripts. This removes the drift when new docker version introduce breaking changes #2207</li> </ul>"},{"location":"rn/0.57/#0575","title":"0.57.5","text":"<ul> <li>fix <code>tools vxlan create</code> command #2213</li> <li>added more docs for xrd and cat9kv #2212</li> </ul>"},{"location":"rn/0.58/","title":"Release 0.58","text":"<p> 2024-10-14 \u00b7  Full Changelog</p>"},{"location":"rn/0.58/#cisco-iol","title":"Cisco IOL","text":"<p>Yes, \"IOS on Linux\" was a hot thing in early 2000s, but if you don't need datapath features and just want to have a lightweight Cisco IOS - Cisco IOL is as light as it gets.</p> <p>Thanks to @kaelemc and @DanPartelly and their work in #2211, #2235 and https://github.com/hellt/vrnetlab/pull/256 and https://github.com/hellt/vrnetlab/pull/257 you can now experience Cisco IOL in containerlab.</p> <p>Getting an IOS prompt was never easier, and faster!</p>"},{"location":"rn/0.58/#cisco-vios","title":"Cisco vIOS","text":"<p>Another good news for Cisco fanboys and fangirls, there is yet another NOS, apparently abbreviated as vIOS that is now supported in containerlab thanks to the effort from @jaro0149 in https://github.com/hellt/vrnetlab/pull/249.</p> <p>This variant doesn't have its own <code>kind</code> in containerlab, because you can get away with the regular <code>linux</code> kind, as explained in the docs.</p>"},{"location":"rn/0.58/#huawei-vrp","title":"Huawei VRP","text":"<p>And yet another contribution to vrnetlab/containerlab that adds a new system - this time from @rafabr and his work in https://github.com/hellt/vrnetlab/pull/250 that adds <code>huawei_vrp</code> system.</p> <p>Both N40E and CE12800 images were tested and they come with startup-config support. Great work @rafabr!</p>"},{"location":"rn/0.58/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>vrnetlab images are now tagged with <code>vrnetlab-version</code> label to help you identify when they were build and off of which vrnetlab commit</li> <li>updated N9Kv system requirements (cpu and mem bump)</li> <li>ssh config file is now explicitly created with <code>644</code> permissions</li> <li>documentation prefix <code>3fff</code> is used for v6 addresses for containerlab-driven management addresses. Thanks @hyposcaler-bot</li> </ul>"},{"location":"rn/0.59/","title":"Release 0.59","text":"<p> 2024-10-23 \u00b7  Full Changelog</p>"},{"location":"rn/0.59/#per-stage-exec-hooks","title":"Per-stage exec hooks","text":"<p>Back in 0.51 when we introduced the stages support every stage had a single exec hook that allowed you to execute a command in a container when it enters/exits a stage. It was a good start, but it had one hole in it - it was not possible to execute a command on a host machine when a container enters/exits a particular stage.</p> <p>Thanks to @steiler the stages support for execs became much more powerful in this release with allowing you to flexibly define where command is executed (host or container) and when (what stage and what event). Here is an example of a refactored exec hooks:</p> <pre><code>nodes:\n  node1:\n    stages:\n      create-links:\n        exec:\n          - command: ls /sys/class/net/\n            target: container #(1)!\n            phase: on-enter #(2)!\n</code></pre> <ol> <li><code>target</code> defaults to \"container\" and can be omitted. Possible values <code>container</code> or <code>host</code></li> <li><code>phase</code> defaults to \"on-enter\" and can be omitted. Possible values <code>on-enter</code> or <code>on-exit</code></li> </ol> <p>Note, this is a breaking change if you used execs in the stages before.</p>"},{"location":"rn/0.59/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>fixed loading custom export templates #2240</li> <li>iptables kernel load to support compressed modules #2251</li> <li>fixed the release retrieval routine #2250</li> <li>Cisco IOL to support custom PIDs and randomized MACs #2239</li> <li>SR Linux kind drops support for mounted authz_keys file #2254</li> </ul>"},{"location":"rn/0.60/","title":"Release 0.60","text":"<p> 2024-12-04 \u00b7  Full Changelog</p>"},{"location":"rn/0.60/#new-year-new-look-the-new-table-style","title":"New Year New Look: The new table style","text":"<p>Yes, the New Year is not quite here, but the gifts are. We have a new table style and layout that you will see whenever you run any of the containerlab commands that display tabular data.</p> <p>Before we used the classic table style which was not very pretty, but more importantly, it was quite W I D E. We've seen you struggling with it, shrinking the terminal font size to fit the table view. It was not a great UX.</p> <p>So we decided to change that, and in this release we introduce a new table style that looks nice(er) and is much more compact! Here is a 1:1 comparison of the table output for the SR Linux ACL lab that has three nodes in it:</p> <pre><code>Old table style:\n+---+------------+--------------+--------------------------------------------+---------------+---------+----------------+----------------------+\n| # |    Name    | Container ID |                   Image                    |     Kind      |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+------------+--------------+--------------------------------------------+---------------+---------+----------------+----------------------+\n| 1 | acl-client | ad01263e77f4 | ghcr.io/srl-labs/network-multitool         | linux         | running | 172.20.20.3/24 | 3fff:172:20:20::3/64 |\n| 2 | acl-server | 59623d96308b | public.ecr.aws/nginx/nginx:1.27-alpine3.19 | linux         | running | 172.20.20.2/24 | 3fff:172:20:20::2/64 |\n| 3 | acl-srl    | 9e3048d5e678 | ghcr.io/nokia/srlinux:24.10.1              | nokia_srlinux | running | 172.20.20.4/24 | 3fff:172:20:20::4/64 |\n+---+------------+--------------+--------------------------------------------+---------------+---------+----------------+----------------------+\n\nNew table style:\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502    Name    \u2502                 Kind/Image                 \u2502  State  \u2502   IPv4/6 Address  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 acl-client \u2502 linux                                      \u2502 running \u2502 172.20.20.4       \u2502\n\u2502            \u2502 ghcr.io/srl-labs/network-multitool         \u2502         \u2502 3fff:172:20:20::4 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 acl-server \u2502 linux                                      \u2502 running \u2502 172.20.20.2       \u2502\n\u2502            \u2502 public.ecr.aws/nginx/nginx:1.27-alpine3.19 \u2502         \u2502 3fff:172:20:20::2 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 acl-srl    \u2502 nokia_srlinux                              \u2502 running \u2502 172.20.20.3       \u2502\n\u2502            \u2502 ghcr.io/nokia/srlinux:24.10.1              \u2502         \u2502 3fff:172:20:20::3 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>As you can see, the table is now almost half the width of the old one, which means you are less likely to have to shrink the font size to fit the table. Simply lovely.</p> <p>Of course, it is not the style that made the difference, you may notice that we removed some columns like Container ID and node index. We also made each node to make use of the vertical space and combined Kind/Image and v4/v6 fields. This allowed us to narrow down the overall table width.</p> <p>Font matters</p> <p>There is a small price to pay for the new table style; it might be sensitive to the font family you use. In the terminal most fonts will work brilliantly, but when you dump the table to some UIs it might not be as pretty.</p> <p>For example, when dumping the tables to the beautiful chalk.ist, select the Nova font.</p> <p>We are curious to hear your feedback, negative or positive. If you feel that we should make the style configurable, please let us know in Discord.</p>"},{"location":"rn/0.60/#transparent-management-mode-for-vm-based-nodes-beta","title":"Transparent management mode for VM-based nodes (beta)","text":"<p>As predicted, we saw a growth in container-native network OSes over the past couple of years. Slowly, but surely we are moving to a better place, where we can run networking topologies fully in containers.</p> <p>But there is still a lot of legacy infrastructure out there, and we needed to support it. That was the prime motivation to integrate vrnetlab to containerlab and wrap these fatty VMs with a thin container layer.</p> <p>One particular feature of vrnetlab was that VMs were using the Qemu user-mode network stack, which is a bit of a pain to work with. It boils down to all VMs having the same management interface IP address, which is, of course, not ideal. It is quite critical to network management systems, who went crazy when they saw the same IP address on all VMs calling home. It was time to fix that.</p> <p>Thanks to @vista- and the work he started in hellt/vrnetlab#268 we started to chip away on what we call a \"transparent management mode\" for vrnetlab. In this mode, each VM will have a distinct IP address assigned to its management interface that matches the IP address you see in the containerlab table. With some <code>tc</code> magic we were able to achieve a functional management connectivity while keeping the telnet/console accesses intact.</p> <p></p> <p>We are looking for beta testers for this feature that is documented in https://github.com/hellt/vrnetlab/issues/286 with support added for</p> <ul> <li>Nokia SR OS</li> <li>Juniper vJunos and vSRX</li> </ul> <p>Other vrnetlab-based nodes will be supported as well, if you want to help -- please reach out to us on Discord or vrnetlab issue tracker.</p>"},{"location":"rn/0.60/#devcontainer-and-devpod","title":"Devcontainer and DevPod","text":"<p>The ultimate goal Containerlab pursues is to make networking labs a commodity. Doesn't matter what OS you are using, what platform you are on, or how skilled you are with containers.</p> <p>Over time we approached this lofty goal by making iterative improvements. Starting with making sure it is easy to install containerlab on any Linux distro using the quick setup script.</p> <p>Then making it easy to run containerlab on borrowed and free compute - that is how Codespaces integration story started and was picked up by the community.</p> <p>For this  release we are taking another step further and releasing two new integrations that will help you reduce the mean-time-to-lab even further.</p>"},{"location":"rn/0.60/#devcontainer","title":"Devcontainer","text":"<p>The devcontainer integration is a way to start a lab on a laptop, desktop, server or VM without installing anything on the host besides Docker. If you rememeber how easy it was to start a lab in Codespaces, you will be happy to get the same UX now with your local compute.</p> <p>We are documenting Devcontainer support</p> <ul> <li>for macOS</li> <li>and Windows</li> </ul> <p>And in this video we dive into the details of how to use it.</p>"},{"location":"rn/0.60/#devpod","title":"DevPod","text":"<p>DevPod takes the devcontainer experience and adds better UX on top of it </p> <p>An open-source project by Loft Labs, DevPod makes it possible to use the same devcontainer specification and create a \"workspace\" that uses almost any IDE known to men and deploys it on a wide range of providers.</p>"},{"location":"rn/0.60/#macos-documentation","title":"macOS documentation","text":"<p>It took us a while, but we finally refreshed the macOS documentation. The availability of Nokia SR Linux in a native arm64 architecture was definitely a catalyst for this, but not the only one.</p> <p>After @hellt did a video on running containerlabs on arm64 architecture where he featured OrbStack in the role of a virtual machine manager for macOS, we've been getting a lot of feedback from our users saying that they finally got to run labs on their Macs.</p>"},{"location":"rn/0.60/#windows-subsystem-for-linux-wsl-documentation","title":"Windows Subsystem for Linux (WSL) documentation","text":"<p>We also refreshed the Windows documentation that revolves around WSL. It was a bit outdated, and WSL is still improving quite a lot.</p> <p>With Win11 it became even better and the tireless team of our contributors - @kaelemc, @FloSch62, and @hyposcaler-bot - spent 900 messages in dicsord while delivering a custom WSL distro to elevate WSL experience to the sky.</p> <p>Please meet WSL-Containerlab.</p>"},{"location":"rn/0.60/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>network aliases option for nodes #2256 by @mzagozen</li> <li>added packet corruption capability for the tools netem command #2271</li> <li>support for the interactive mode of the drawio integration #2291 by @FloSch62</li> <li>fixes and improvements to the installation scripts #2273 #2273</li> <li>enabled Netconf on SR Linux #2322</li> </ul>"},{"location":"rn/0.60/#patches","title":"Patches","text":""},{"location":"rn/0.60/#0601","title":"0.60.1","text":"<ul> <li>do not check bind paths when performing <code>destroy</code> #2334 #2337</li> <li>setup docker/moby 26.1.5 #2336</li> </ul>"},{"location":"rn/0.61/","title":"Release 0.61","text":"<p> 2025-01-03 \u00b7  Full Changelog</p>"},{"location":"rn/0.61/#no-manual-no_proxy","title":"No manual <code>NO_PROXY</code>","text":"<p>Proxies are a pain that every corporate network user has to find a way painkiller for. What makes it even worse is that when adding a proxy to your environment, you have to add a <code>NO_PROXY</code> env var for local addresses that you want to reach without a proxy. Add to the mix the fact that the industry still can't agree if <code>NO_PROXY</code> or <code>no_proxy</code> env var name should be used so we have to deal with both.</p> <p>If you lab topology has nodes that use HTTP to communicate with other nodes of the lab you have to set up a NO_PROXY/no_proxy env var and exclude every single node IP and DNS name of the topology nodes. You can imagine what a time killer this is.</p> <p>Thanks to @toweber and his work in #2351, Containerlab will automatically inject the NO_PROXY env vars to each node in the lab - read more in the docs.</p>"},{"location":"rn/0.61/#startup-config-support-for-cisco-iol","title":"Startup-config support for Cisco IOL","text":"<p>@kaelemc continued to improve IOL support by adding support for startup-config #2347. Now you get a lightweight Cisco IOS-XE system with a decent feature set. Unbelievable!</p>"},{"location":"rn/0.61/#node-name-magic-variable","title":"Node Name magic variable","text":"<p>To further streamline the topology definition file, @hyposcaler-bot added support for the <code>__clabNodeName__</code> magic variable that allows you to dynamically set the node name in the startup-configuration path.</p> <pre><code>name: mylab\ntopology:\n  defaults:\n    kind: nokia_srlinux\n    startup-config: cfgs/__clabNodeName__.partial.cfg\n  nodes:\n    node1:\n    node2:\n</code></pre> <p>The above topology will instruct <code>node1</code> to use <code>cfgs/node1.partial.cfg</code> and <code>node2</code> to use <code>cfgs/node2.partial.cfg</code>. All with a single default setting provided. Read more about this neat feature in the nodes section.</p>"},{"location":"rn/0.61/#redeploy-command","title":"Redeploy command","text":"<p>Many of us redeploy labs dozens times a day. Before #2374 implemented by @axxyhtrx we all have been punching in <code>sudo clab dep -c -t &lt;topo&gt;</code> to redeploy a lab. While it worked for a clean redeployment, it wasn't possible to redeploy a lab while keeping the Lab Directory.</p> <p>Now the <code>redeploy</code>/<code>rdep</code> is a handy and syntactically more pleasant way to redeploy a lab and it supports both scenarios with keeping and without keeping the Lab Directory.</p>"},{"location":"rn/0.61/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>@jkary fixes the way link tags are displayed for LAGs in #2306</li> <li>SR Linux default config gets gRPC servers config blocks to support EDA onboarding #2368</li> <li>support for STDIN topologies in <code>destroy</code> command #2376</li> </ul>"},{"location":"rn/0.62/","title":"Release 0.62","text":"<p> 2025-01-17 \u00b7  Full Changelog</p>"},{"location":"rn/0.62/#lab-directory-path","title":"Lab Directory path","text":"<p>No one will miss the old behavior of creating the lab directory in the current working directory. Now we are changing the behavior so that the lab directory is created in the directory where the topology file is located.</p> <p>Why we did not do this from the beginning? Who knows...</p>"},{"location":"rn/0.62/#public-keys-export","title":"Public keys export","text":"<p>Containerlab has this super power of fetching your public keys and provisioning them to the supported nodes (Nokia SR Linux and SR OS at this point<sup>1</sup>).</p> <p>The extracted keys will now also be visible in the topology export file available in your Lab Directory so that external systems parsing this file could use the public keys if needed.</p>"},{"location":"rn/0.62/#command-line-completions","title":"Command line completions","text":"<p>A feature that is typically used by power users, but that blows the socks off of regular users. Command line completions. We had it for a very long time, but this time around we revisited the documentation to make sure the snippets still work and also support the shorthand <code>clab</code> alias.</p> <p>Go check it out.</p>"},{"location":"rn/0.62/#ipv6-allow-rules","title":"IPv6 allow rules","text":"<p>When containerlab creates the management docker network (<code>clab</code> by default) it sets up the iptables rules in the <code>DOCKER-USER</code> chain of the <code>filter</code> table to allow traffic destined to this network. This rule ensures that external systems can reach the lab nodes.</p> <p>Unfortunately, we only set up the rules in the ipv4 address family, and with #2397 we fix this for ipv6 as well.</p> <p>Besides the management network rules, containerlab also sets up rules in the <code>FORWARD</code> chain for the bridge nodes referenced in the topology file. But, again, we missed v6 rules, which are now being added as well (#2401).</p>"},{"location":"rn/0.62/#cisco-iol-save-support","title":"Cisco IOL <code>save</code> support","text":"<p>The Cisco IOL node type now supports the <code>save</code> command that saves the running configuration to the flash memory. #2403 by @kaelemc</p>"},{"location":"rn/0.62/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>an option to skip sshd settings when using the quick setup script #2381 #2383 by @jklare</li> <li>automatic CRLF-&gt;LF conversion for Nokia SR OS startup configs #2382 by @jcpvdm</li> <li>IOL to support <code>ethX</code> interface names #2400 by @kaelemc</li> <li><code>__clabNodeName__</code> magic var support for execs #2395</li> <li>As we continue with testing of the Transparent Management Interface feature, we are documenting the steps needed to set it up.</li> <li>removed <code>publish</code> nodes property as it was not working/used anymore #2404</li> </ul> <ol> <li> <p>You can add it for your platform of interest.\u00a0\u21a9</p> </li> </ol>"}]}